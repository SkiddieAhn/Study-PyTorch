{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, einsum\n",
    "from einops import rearrange\n",
    "from pytorch_model_summary import summary\n",
    "from monai.networks.layers.utils import get_norm_layer\n",
    "from unetr_plus_plus.unetr_pp.network_architecture.dynunet_block import get_conv_layer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기본 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=[32 * 32 * 32, 16 * 16 * 16, 8 * 8 * 8, 4 * 4 * 4]\n",
    "dims=[32, 64, 128, 256]\n",
    "proj_size =[64,64,64,32]\n",
    "depths=[3, 3, 3, 3]\n",
    "num_heads=4\n",
    "spatial_dims=3\n",
    "in_channels=1\n",
    "dropout=0.0\n",
    "transformer_dropout_rate=0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-1. Downsampling (기존 버전: Conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class downsampling(nn.Module):\n",
    "    def __init__(self,spatial_dims,in_channels,out_channels,kernel_size,stride,dropout,conv_only=True):\n",
    "        super().__init__()\n",
    "        self.downsample_layer=get_conv_layer(spatial_dims, in_channels, out_channels, kernel_size, stride,dropout, conv_only, )\n",
    "        self.get_norm_layer=get_norm_layer(name=(\"group\", {\"num_groups\": in_channels}), channels=out_channels)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.downsample_layer(x)\n",
    "        x=self.get_norm_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------\n",
      "      Layer (type)          Output Shape         Param #     Tr. Param #\n",
      "=========================================================================\n",
      "          Conv3d-1     [1, 128, 8, 8, 8]          65,536          65,536\n",
      "       GroupNorm-2     [1, 128, 8, 8, 8]             256             256\n",
      "=========================================================================\n",
      "Total params: 65,792\n",
      "Trainable params: 65,792\n",
      "Non-trainable params: 0\n",
      "-------------------------------------------------------------------------\n",
      "input: torch.Size([1, 64, 16, 16, 16])\n",
      "output: torch.Size([1, 128, 8, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "x=torch.zeros(1,64,16,16,16).cuda() # [B,C,D,H,W] input: 16 x 16 x 16 x 64\n",
    "model=downsampling(\n",
    "    spatial_dims=3, in_channels=x.shape[1], out_channels=x.shape[1]*2,kernel_size=[2,2,2],stride=[2,2,2],dropout=0.0\n",
    ").to(device)\n",
    "\n",
    "print(summary(model,x))\n",
    "print('input:',x.shape)\n",
    "print('output:',model(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1-2. Downsampling (새로운 버전: Patch Merging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchMerging(nn.Module):\n",
    "    def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.norm = norm_layer(8 * dim)\n",
    "        self.reduction = nn.Linear(8 * dim, 2 * dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: B,C,D,H,W\n",
    "        \"\"\"\n",
    "        x=x.permute(0,3,4,2,1) # [B,H,W,D,C]\n",
    "        B=x.shape[0];H=x.shape[1];W=x.shape[2];D=x.shape[3];C=x.shape[4]\n",
    "\n",
    "        y=None\n",
    "        for i in range(0,D,2):\n",
    "            # process 2 slice\n",
    "            x_=x[:, :, :, i:i+2, :] # B, H/2, W/2, 2, C\n",
    "            \n",
    "            x_0=x_[:, 0::2, 0::2, :, :] # B, H/2, W/2, 2, C\n",
    "            x_1=x_[:, 0::2, 1::2, :, :] # B, H/2, W/2, 2, C \n",
    "            x_2=x_[:, 1::2, 0::2, :, :]  # B, H/2, W/2, 2, C \n",
    "            x_3=x_[:, 1::2, 1::2, :, :] # B, H/2, W/2, 2, C\n",
    "\n",
    "            # width, height information -> channel information\n",
    "            rst=torch.cat([x_0,x_1,x_2,x_3],-1) # B, H/2, W/2, 2, 4*C\n",
    "\n",
    "            # dimension information -> channel information\n",
    "            rst=rst.view(B, H//2, W//2, 1, 8*C) # B, H/2, W/2, 1, 8*C\n",
    "\n",
    "            # concat \n",
    "            if i==0:\n",
    "                y=rst.clone() # B, H/2, W/2, 1, 8*C\n",
    "            else:\n",
    "                y=torch.cat([y,rst],-2) # final shape -> [B, H/2, W/2, D/2, 8*C]\n",
    "        \n",
    "        # normalization\n",
    "        y=self.norm(y) # B, H/2, W/2, D/2, 8*C\n",
    "        \n",
    "        # embedding\n",
    "        y=self.reduction(y) # B, H/2, W/2, D/2, 2*C\n",
    "\n",
    "        y=y.permute(0,4,3,1,2) # B, 2*C, D/2, H/2, W/2\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------\n",
      "      Layer (type)          Output Shape         Param #     Tr. Param #\n",
      "=========================================================================\n",
      "       LayerNorm-1     [1, 8, 8, 8, 512]           1,024           1,024\n",
      "          Linear-2     [1, 8, 8, 8, 128]          65,536          65,536\n",
      "=========================================================================\n",
      "Total params: 66,560\n",
      "Trainable params: 66,560\n",
      "Non-trainable params: 0\n",
      "-------------------------------------------------------------------------\n",
      "input: torch.Size([1, 64, 16, 16, 16])\n",
      "output: torch.Size([1, 128, 8, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "x=torch.zeros(1,64,16,16,16) # [B,C,D,H,W] input: 16 x 16 x 16 x 64\n",
    "model=PatchMerging(dim=x.shape[1])\n",
    "\n",
    "print(summary(model,x))\n",
    "print('input:',x.shape)\n",
    "print('output:',model(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-1.Upsampling (기존 버전: TrasposedConv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class upsampling(nn.Module):\n",
    "    def __init__(self,spatial_dims,in_channels,out_channels,upsample_kernel_size,upsample_stride,dropout,conv_only=True):\n",
    "        super().__init__()\n",
    "        self.transp_conv = get_conv_layer(\n",
    "            spatial_dims,\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=upsample_kernel_size,\n",
    "            stride=upsample_stride,\n",
    "            conv_only=True,\n",
    "            is_transposed=True,\n",
    "        )\n",
    "    \n",
    "    def forward(self,y):\n",
    "        y=self.transp_conv(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------\n",
      "        Layer (type)            Output Shape         Param #     Tr. Param #\n",
      "=============================================================================\n",
      "   ConvTranspose3d-1     [1, 64, 16, 16, 16]          65,536          65,536\n",
      "=============================================================================\n",
      "Total params: 65,536\n",
      "Trainable params: 65,536\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------------\n",
      "input: torch.Size([1, 128, 8, 8, 8])\n",
      "output: torch.Size([1, 64, 16, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "y=torch.zeros(1,128,8,8,8) # [B,C,D,H,W] input: 8 x 8 x 8 x 128\n",
    "model=upsampling(\n",
    "    spatial_dims=3, in_channels=y.shape[1], out_channels=y.shape[1]//2,upsample_kernel_size=[2,2,2],upsample_stride=[2,2,2],dropout=0.0\n",
    ")\n",
    "\n",
    "print(summary(model,y))\n",
    "print('input:',y.shape)\n",
    "print('output:',model(y).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2-2.Upsampling (새로운 버전: Patch Expanding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchExpanding(nn.Module):\n",
    "    def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.norm = norm_layer(dim//2)\n",
    "        self.expand = nn.Linear(dim, 4 * dim, bias=False)\n",
    "\n",
    "    def forward(self, y):\n",
    "        \"\"\"\n",
    "        y: B,C,D,H,W\n",
    "        \"\"\"\n",
    "        y=y.permute(0,3,4,2,1) # [B,H,W,D,C]\n",
    "        B=y.shape[0];H=y.shape[1];W=y.shape[2];D=y.shape[3];C=y.shape[4]\n",
    "\n",
    "        # channel expand\n",
    "        y=self.expand(y) # B, H, W, D, 4*C\n",
    "\n",
    "        x=None\n",
    "        for i in range(0,D):\n",
    "            y_=y[:,:,:,i,:] # B, H, W, 1, 4*C\n",
    "            y_=y_.view(B,H,W,1,4*C) \n",
    "\n",
    "            # channel information -> dimension information\n",
    "            y_=y_.view(B, H, W, 2, 2*C) # B, H, W, 2, 2*C\n",
    "\n",
    "            # channel informatinon -> width, height information\n",
    "            rst=rearrange(y_,'b h w d (p1 p2 c)-> b (h p1) (w p2) d c', p1=2, p2=2, c=C//2) # B, 2*H, 2*W, 2, C//2\n",
    "            \n",
    "            # concat\n",
    "            if i==0:\n",
    "                x=rst.clone() # B, 2*H, 2*W, 2, C//2\n",
    "            else:\n",
    "                x=torch.cat([x,rst],-2) # final shape -> [B, 2*H, 2*W, 2*D, C//2]\n",
    "                        \n",
    "        # normalization\n",
    "        x=self.norm(x) # B, 2*H, 2*W, 2*D, C//2\n",
    "\n",
    "        x=x.permute(0,4,3,1,2) # B, C//2, 2*D, 2*H, 2*W\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "      Layer (type)            Output Shape         Param #     Tr. Param #\n",
      "===========================================================================\n",
      "          Linear-1       [1, 8, 8, 8, 512]          65,536          65,536\n",
      "       LayerNorm-2     [1, 16, 16, 16, 64]             128             128\n",
      "===========================================================================\n",
      "Total params: 65,664\n",
      "Trainable params: 65,664\n",
      "Non-trainable params: 0\n",
      "---------------------------------------------------------------------------\n",
      "input: torch.Size([1, 128, 8, 8, 8])\n",
      "output: torch.Size([1, 64, 16, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "y=torch.zeros(1,128,8,8,8) # [B,C,D,H,W] input: 8 x 8 x 8 x 128\n",
    "model=PatchExpanding(dim=y.shape[1])\n",
    "\n",
    "print(summary(model,y))\n",
    "print('input:',y.shape)\n",
    "print('output:',model(y).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Check] x-> (Patch Merging) -> y -> (Patch Expanding) -> z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====stage2====\n",
      "x.shape: torch.Size([1, 32, 32, 32, 32])\n",
      "y.shape: torch.Size([1, 64, 16, 16, 16]) <-- downsampling(patch merging)\n",
      "z.shape: torch.Size([1, 32, 32, 32, 32]) <-- upsampling(patch expanding)\n",
      "====stage3====\n",
      "x.shape: torch.Size([1, 64, 16, 16, 16])\n",
      "y.shape: torch.Size([1, 128, 8, 8, 8]) <-- downsampling(patch merging)\n",
      "z.shape: torch.Size([1, 64, 16, 16, 16]) <-- upsampling(patch expanding)\n",
      "====stage4====\n",
      "x.shape: torch.Size([1, 128, 8, 8, 8])\n",
      "y.shape: torch.Size([1, 256, 4, 4, 4]) <-- downsampling(patch merging)\n",
      "z.shape: torch.Size([1, 128, 8, 8, 8]) <-- upsampling(patch expanding)\n"
     ]
    }
   ],
   "source": [
    "# stage1 input: x = 128 x 128 x 64 x 1 (H x W x D x 1)\n",
    "# stage2 input: x = 32 x 32 x 32 x 32 (H/4 x W/4 x D/2 x C)\n",
    "# stage3 input: x = 16 x 16 x 16 x 64 (H/8 x W/8 x D/4 x 2C)\n",
    "# stage4 input: x = 8 x 8 x 8 x 128 (H/16 x W/16 x D/8 x 4C)\n",
    "\n",
    "input=torch.zeros(1,1,64,128,128) # [B, C, D, H, W]\n",
    "# patch embedding은 생략 -> stage2부터 시작 \n",
    "C=32\n",
    "for i in range(0,3):\n",
    "    print(f'====stage{i+2}====')\n",
    "    x=torch.zeros(input.shape[0],C*2**i,input.shape[2]//(2*2**i),input.shape[3]//(4*2**i),input.shape[4]//(4*2**i))\n",
    "    print('x.shape:',x.shape)\n",
    "\n",
    "    down=PatchMerging(dim=x.shape[1])\n",
    "    y=down(x)\n",
    "    print('y.shape:',y.shape,'<-- downsampling(patch merging)')\n",
    "\n",
    "    up=PatchExpanding(dim=y.shape[1])\n",
    "    z=up(y)\n",
    "    print('z.shape:',z.shape,'<-- upsampling(patch expanding)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-1. EPA 모듈 (기존 버전: 병렬)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EPA(nn.Module):\n",
    "    \"\"\"\n",
    "        Efficient Paired Attention Block, based on: \"Shaker et al.,\n",
    "        UNETR++: Delving into Efficient and Accurate 3D Medical Image Segmentation\"\n",
    "        \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, proj_size, num_heads=4, qkv_bias=False,\n",
    "                 channel_attn_drop=0.1, spatial_attn_drop=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
    "        self.temperature2 = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
    "\n",
    "        # qkvv are 4 linear layers (query_shared, key_shared, value_spatial, value_channel)\n",
    "        self.qkvv = nn.Linear(hidden_size, hidden_size * 4, bias=qkv_bias)\n",
    "\n",
    "        # E and F are projection matrices with shared weights used in spatial attention module to project\n",
    "        # keys and values from HWD-dimension to P-dimension\n",
    "        self.E = self.F = nn.Linear(input_size, proj_size)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(channel_attn_drop)\n",
    "        self.attn_drop_2 = nn.Dropout(spatial_attn_drop)\n",
    "\n",
    "        self.out_proj = nn.Linear(hidden_size, int(hidden_size // 2))\n",
    "        self.out_proj2 = nn.Linear(hidden_size, int(hidden_size // 2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "\n",
    "        qkvv = self.qkvv(x).reshape(B, N, 4, self.num_heads, C // self.num_heads)\n",
    "\n",
    "        qkvv = qkvv.permute(2, 0, 3, 1, 4)\n",
    "\n",
    "        q_shared, k_shared, v_CA, v_SA = qkvv[0], qkvv[1], qkvv[2], qkvv[3]\n",
    "\n",
    "        q_shared = q_shared.transpose(-2, -1)\n",
    "        k_shared = k_shared.transpose(-2, -1)\n",
    "        v_CA = v_CA.transpose(-2, -1)\n",
    "        v_SA = v_SA.transpose(-2, -1)\n",
    "\n",
    "        k_shared_projected = self.E(k_shared)\n",
    "\n",
    "        v_SA_projected = self.F(v_SA)\n",
    "\n",
    "        q_shared = torch.nn.functional.normalize(q_shared, dim=-1)\n",
    "        k_shared = torch.nn.functional.normalize(k_shared, dim=-1)\n",
    "\n",
    "        attn_CA = (q_shared @ k_shared.transpose(-2, -1)) * self.temperature\n",
    "\n",
    "        attn_CA = attn_CA.softmax(dim=-1)\n",
    "        attn_CA = self.attn_drop(attn_CA)\n",
    "\n",
    "        x_CA = (attn_CA @ v_CA).permute(0, 3, 1, 2).reshape(B, N, C)\n",
    "\n",
    "        attn_SA = (q_shared.permute(0, 1, 3, 2) @ k_shared_projected) * self.temperature2\n",
    "\n",
    "        attn_SA = attn_SA.softmax(dim=-1)\n",
    "        attn_SA = self.attn_drop_2(attn_SA)\n",
    "\n",
    "        x_SA = (attn_SA @ v_SA_projected.transpose(-2, -1)).permute(0, 3, 1, 2).reshape(B, N, C)\n",
    "\n",
    "        # Concat fusion\n",
    "        x_SA = self.out_proj(x_SA)\n",
    "        x_CA = self.out_proj2(x_CA)\n",
    "        x = torch.cat((x_SA, x_CA), dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------\n",
      "      Layer (type)          Output Shape         Param #     Tr. Param #\n",
      "=========================================================================\n",
      "          Linear-1       [1, 32768, 128]           4,096           4,096\n",
      "          Linear-2         [1, 4, 8, 64]       2,097,216       2,097,216\n",
      "         Dropout-3          [1, 4, 8, 8]               0               0\n",
      "         Dropout-4     [1, 4, 32768, 64]               0               0\n",
      "          Linear-5        [1, 32768, 16]             528             528\n",
      "          Linear-6        [1, 32768, 16]             528             528\n",
      "=========================================================================\n",
      "Total params: 2,102,368\n",
      "Trainable params: 2,102,368\n",
      "Non-trainable params: 0\n",
      "-------------------------------------------------------------------------\n",
      "input: torch.Size([1, 32768, 32])\n",
      "output: torch.Size([1, 32768, 32])\n"
     ]
    }
   ],
   "source": [
    "x=torch.zeros(1,32,32,32,32) # B, C, H, W, D\n",
    "B, C, H, W, D = x.shape\n",
    "x = x.reshape(B, C, H * W * D).permute(0, 2, 1) # B, H*W*D, C\n",
    "\n",
    "input_size=x.shape[1]\n",
    "hidden_size=32\n",
    "proj_size=64\n",
    "\n",
    "model=EPA(input_size, hidden_size, proj_size, num_heads=4)\n",
    "print(summary(model,x))\n",
    "print('input:',x.shape)\n",
    "print('output:',model(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-2. EPA 모듈 (새로운 버전: 직렬)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_EPA(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, proj_size, num_heads=4, qkv_bias=False,\n",
    "                 channel_attn_drop=0.1, spatial_attn_drop=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1)) # for channel attention\n",
    "        self.temperature2 = nn.Parameter(torch.ones(num_heads, 1, 1)) # for spatial attention\n",
    "\n",
    "        # qkv are 3 linear layers (query, key, value)\n",
    "        self.qkv = nn.Linear(hidden_size, hidden_size * 3, bias=qkv_bias)\n",
    "        self.qkv2 = nn.Linear(hidden_size, hidden_size * 3, bias=qkv_bias)\n",
    "\n",
    "        # projection matrices with shared weights used in attention module to project\n",
    "        self.proj_k = self.proj_v = nn.Linear(input_size, proj_size)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(channel_attn_drop) \n",
    "        self.attn_drop_2 = nn.Dropout(spatial_attn_drop)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Channel Attention\n",
    "        : [ Q_T x K ]\n",
    "        '''\n",
    "        B, N, C = x.shape # N=HWD\n",
    "\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads) # B x N x 3 x h x C/h\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4) # 3 x B x h x N x C/h\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2] # B x h x N x C/h\n",
    "\n",
    "        q_t = q.transpose(-2, -1) # B x h x C/h x N\n",
    "        k_t = k.transpose(-2, -1) # B x h x C/h x N\n",
    "        v_t = v.transpose(-2, -1) # B x h x C/h x N\n",
    "\n",
    "        q_t = torch.nn.functional.normalize(q_t, dim=-1)\n",
    "        k_t = torch.nn.functional.normalize(k_t, dim=-1)\n",
    "        \n",
    "        k = k_t.transpose(-2, -1) # K : B x h x C/h x C/h\n",
    "        attn_CA = (q_t @ k) * self.temperature # [Q_T x K] B x h x C/h x C/h \n",
    "\n",
    "        attn_CA = attn_CA.softmax(dim=-1)\n",
    "        attn_CA = self.attn_drop(attn_CA) # [Channel Attn Map] B x h x C/h x C/h\n",
    "\n",
    "        v = v_t.permute(0,1,3,2) # V : B x h x N x C/h\n",
    "\n",
    "        # [V x Channel Attn Map] B x h x N x C/h -> B x C/h x h x N -> B x N x C\n",
    "        x_CA = (v @ attn_CA).permute(0, 3, 1, 2).reshape(B, N, C)\n",
    "\n",
    "        '''\n",
    "        Spatial Attention\n",
    "        : K -> K(p), V -> V(p) [ Q x K_T(p) ]\n",
    "        '''\n",
    "        qkv2 = self.qkv2(x_CA).reshape(B, N, 3, self.num_heads, C // self.num_heads) # B x N x 3 x h x C/h\n",
    "        qkv2 = qkv2.permute(2, 0, 3, 1, 4) # 3 x B x h x N x C/h\n",
    "        q2, k2, v2 = qkv2[0], qkv2[1], qkv2[2] # B x h x N x C/h\n",
    "\n",
    "        q2_t = q2.transpose(-2, -1) # B x h x C/h x N\n",
    "        k2_t = k2.transpose(-2, -1) # B x h x C/h x N\n",
    "        v2_t = v2.transpose(-2, -1) # B x h x C/h x N\n",
    "\n",
    "        k2_t_projected = self.proj_k(k2_t) # B x h x C/h x p\n",
    "        v2_t_projected = self.proj_v(v2_t) # B x h x C/h x p\n",
    "\n",
    "        q2_t = torch.nn.functional.normalize(q2_t, dim=-1)\n",
    "        k2_t = torch.nn.functional.normalize(k2_t, dim=-1)\n",
    "\n",
    "        q2 = q2_t.permute(0, 1, 3, 2) # Q : B x h x N x C/h\n",
    "        attn_SA = (q2 @ k2_t_projected) * self.temperature2  # [Q x K_T(p)] B x h x N x p\n",
    "        \n",
    "        attn_SA = attn_SA.softmax(dim=-1)\n",
    "        attn_SA = self.attn_drop_2(attn_SA) # [Spatial Attn Map] B x h x N x p\n",
    "        \n",
    "        v2_projected = v2_t_projected.transpose(-2, -1) # V(p) : B x h x p x C/h\n",
    "\n",
    "        # [Spatial Attn Map x V(p)] B x h x N x C/h -> B x C/h x h x N -> B x N x C\n",
    "        x_SA = (attn_SA @ v2_projected).permute(0, 3, 1, 2).reshape(B, N, C) \n",
    "        x = x_SA\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------\n",
      "      Layer (type)          Output Shape         Param #     Tr. Param #\n",
      "=========================================================================\n",
      "          Linear-1        [1, 32768, 96]           3,072           3,072\n",
      "         Dropout-2          [1, 4, 8, 8]               0               0\n",
      "          Linear-3        [1, 32768, 96]           3,072           3,072\n",
      "          Linear-4         [1, 4, 8, 64]       2,097,216       2,097,216\n",
      "         Dropout-5     [1, 4, 32768, 64]               0               0\n",
      "=========================================================================\n",
      "Total params: 2,103,360\n",
      "Trainable params: 2,103,360\n",
      "Non-trainable params: 0\n",
      "-------------------------------------------------------------------------\n",
      "input: torch.Size([1, 32768, 32])\n",
      "output: torch.Size([1, 32768, 32])\n"
     ]
    }
   ],
   "source": [
    "x=torch.zeros(1,32,32,32,32) # B, C, H, W, D\n",
    "B, C, H, W, D = x.shape\n",
    "x = x.reshape(B, C, H * W * D).permute(0, 2, 1) # B, H*W*D, C\n",
    "\n",
    "input_size=x.shape[1]\n",
    "hidden_size=32\n",
    "proj_size=64\n",
    "\n",
    "model=My_EPA(input_size, hidden_size, proj_size, num_heads=4)\n",
    "print(summary(model,x))\n",
    "print('input:',x.shape)\n",
    "print('output:',model(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-1. TIF 모듈 (기존 버전: 2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv_block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, groups):\n",
    "        super(Conv_block, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.GroupNorm(num_channels=out_ch,num_groups=groups),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.GroupNorm(num_channels=out_ch,num_groups=groups),\n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, dim_head = 64, dropout = 0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out =  self.to_out(out)\n",
    "        return out\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, Attention(dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class Cross_Att(nn.Module):\n",
    "    def __init__(self, dim_e, dim_r):\n",
    "        super().__init__()\n",
    "        self.transformer_e = Transformer(dim=dim_e, depth=1, heads=4, dim_head=dim_e//4, mlp_dim=128) # UNETR++와 head, dim_head 통일 <local>\n",
    "        self.transformer_r = Transformer(dim=dim_r, depth=1, heads=4, dim_head=dim_r//4, mlp_dim=256) # UNETR++와 head, dim_head 통일 <global>\n",
    "        self.norm_e = nn.LayerNorm(dim_e) \n",
    "        self.norm_r = nn.LayerNorm(dim_r) \n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.linear_e = nn.Linear(dim_e, dim_r)\n",
    "        self.linear_r = nn.Linear(dim_r, dim_e)\n",
    "\n",
    "    def forward(self, e, r):\n",
    "       b_e, c_e, h_e, w_e = e.shape\n",
    "       e = e.reshape(b_e, c_e, -1).permute(0, 2, 1)\n",
    "       b_r, c_r, h_r, w_r = r.shape\n",
    "       r = r.reshape(b_r, c_r, -1).permute(0, 2, 1)\n",
    "       e_t = torch.flatten(self.avgpool(self.norm_e(e).transpose(1,2)), 1)\n",
    "       r_t = torch.flatten(self.avgpool(self.norm_r(r).transpose(1,2)), 1)\n",
    "       e_t = self.linear_e(e_t).unsqueeze(1)\n",
    "       r_t = self.linear_r(r_t).unsqueeze(1)\n",
    "       r = self.transformer_r(torch.cat([e_t, r],dim=1))[:, 1:, :]\n",
    "       e = self.transformer_e(torch.cat([r_t, e],dim=1))[:, 1:, :]\n",
    "       e = e.permute(0, 2, 1).reshape(b_e, c_e, h_e, w_e) \n",
    "       r = r.permute(0, 2, 1).reshape(b_r, c_r, h_r, w_r) \n",
    "       return e, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TIF(nn.Module):\n",
    "    def __init__(self,dim_e,dim_r):\n",
    "        super().__init__()\n",
    "        self.dim_e=dim_e # local feature map channel (32,64,128,256)\n",
    "        self.dim_r=dim_r # global feature map channel (64,128,256,512)\n",
    "        self.cross_attn=Cross_Att(self.dim_e,self.dim_r)\n",
    "        self.up = nn.Upsample(scale_factor=2)\n",
    "        self.conv=Conv_block(in_ch=self.dim_e+self.dim_r, out_ch=self.dim_e, groups=32)\n",
    "        \n",
    "    def forward(self,e,r):\n",
    "        '''\n",
    "        e: local feature (H x W x C)\n",
    "        r: global feature (H/2 x W/2 x 2C)\n",
    "        '''\n",
    "        e,r=self.cross_attn(e,r) # [B,C,H,W], [B,C,H/2,W/2]\n",
    "        e = torch.cat([e,self.up(r)],1) # B,2C,H,W\n",
    "        e=self.conv(e) # B,C,H,W\n",
    "        \n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------\n",
      "      Layer (type)                         Output Shape         Param #     Tr. Param #\n",
      "========================================================================================\n",
      "       Cross_Att-1     [1, 32, 32, 32], [1, 64, 16, 16]          66,784          66,784\n",
      "        Upsample-2                      [1, 64, 32, 32]               0               0\n",
      "      Conv_block-3                      [1, 32, 32, 32]          37,056          37,056\n",
      "========================================================================================\n",
      "Total params: 103,840\n",
      "Trainable params: 103,840\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------------------------------\n",
      "input(e,r): torch.Size([1, 32, 32, 32]) torch.Size([1, 64, 16, 16])\n",
      "output: torch.Size([1, 32, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "e=torch.zeros(1,32,32,32) # [B,C,H,W] Local \n",
    "r=torch.zeros(1,64,16,16) # [B,C,H,W] Global\n",
    "\n",
    "model=TIF(e.shape[1],r.shape[1]) # dim=32\n",
    "\n",
    "print(summary(model,e,r))\n",
    "print('input(e,r):',e.shape, r.shape)\n",
    "print('output:',model(e,r).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-2. TIF 모듈 (새로운 버전: 3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Conv_block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, groups):\n",
    "        super(My_Conv_block, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv3d(in_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True), # 수정\n",
    "            nn.GroupNorm(num_channels=out_ch,num_groups=groups),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=True), # 수정\n",
    "            nn.GroupNorm(num_channels=out_ch,num_groups=groups),\n",
    "            nn.ReLU(inplace=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class My_Attention(nn.Module):\n",
    "    def __init__(self, input_size, proj_size, dim, heads, dim_head, dropout = 0.): # 수정\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head *  heads\n",
    "        project_out = not (heads == 1 and dim_head == dim)\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias = False) \n",
    "\n",
    "        self.E = self.F = nn.Linear(input_size+1, proj_size) # 수정\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        qkv = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h = h), qkv)\n",
    "        '''\n",
    "        k, v projection (차원 축소하지 않으면 연산량이 너무 커짐)\n",
    "        '''\n",
    "        k=self.E(k.transpose(-2, -1)).transpose(-2,-1) # 수정\n",
    "        v=self.F(v.transpose(-2, -1)).transpose(-2,-1) # 수정\n",
    "\n",
    "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
    "\n",
    "        attn = dots.softmax(dim=-1)\n",
    "\n",
    "        out = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out =  self.to_out(out)\n",
    "        return out\n",
    "\n",
    "class My_Transformer(nn.Module):\n",
    "    def __init__(self, input_size, proj_size, dim, depth, heads, dim_head, mlp_dim, dropout = 0.): # 수정\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(nn.ModuleList([\n",
    "                PreNorm(dim, My_Attention(input_size, proj_size, dim, heads = heads, dim_head = dim_head, dropout = dropout)),\n",
    "                PreNorm(dim, FeedForward(dim, mlp_dim, dropout = dropout))\n",
    "            ]))\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x\n",
    "\n",
    "class My_Cross_Att(nn.Module):\n",
    "    def __init__(self, HWD_e, HWD_r, proj_size, dim_e, dim_r): # 수정\n",
    "        super().__init__()\n",
    "        self.transformer_e = My_Transformer(HWD_e, proj_size, dim=dim_e, depth=1, heads=4, dim_head=dim_e//4, mlp_dim=128) # UNETR++와 head, dim_head 통일 <local>\n",
    "        self.transformer_r = My_Transformer(HWD_r, proj_size, dim=dim_r, depth=1, heads=4, dim_head=dim_r//4, mlp_dim=256) # UNETR++와 head, dim_head 통일 <global>\n",
    "        self.norm_e = nn.LayerNorm(dim_e) \n",
    "        self.norm_r = nn.LayerNorm(dim_r) \n",
    "        self.avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.linear_e = nn.Linear(dim_e, dim_r)\n",
    "        self.linear_r = nn.Linear(dim_r, dim_e)\n",
    "\n",
    "    def forward(self, e, r):\n",
    "       b_e, c_e, d_e, h_e, w_e = e.shape\n",
    "       e = e.reshape(b_e, c_e, -1).permute(0, 2, 1) # B, N, C\n",
    "       b_r, c_r, d_r, h_r, w_r = r.shape\n",
    "       r = r.reshape(b_r, c_r, -1).permute(0, 2, 1)\n",
    "       e_t = torch.flatten(self.avgpool(self.norm_e(e).transpose(1,2)), 1)\n",
    "       r_t = torch.flatten(self.avgpool(self.norm_r(r).transpose(1,2)), 1)\n",
    "       e_t = self.linear_e(e_t).unsqueeze(1)\n",
    "       r_t = self.linear_r(r_t).unsqueeze(1)\n",
    "       r = self.transformer_r(torch.cat([e_t, r],dim=1))[:, 1:, :]\n",
    "       e = self.transformer_e(torch.cat([r_t, e],dim=1))[:, 1:, :]\n",
    "       e = e.permute(0, 2, 1).reshape(b_e, c_e, d_e, h_e, w_e) \n",
    "       r = r.permute(0, 2, 1).reshape(b_r, c_r, d_r, h_r, w_r) \n",
    "       return e, r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_TIF(nn.Module):\n",
    "    def __init__(self,HWD_e,HWD_r,proj_size,dim_e,dim_r): \n",
    "        super().__init__()\n",
    "        # dim_e = local feature map channel (32,64,128,256)\n",
    "        # dim_r = global feature map channel (64,128,256,512)\n",
    "        self.cross_attn=My_Cross_Att(HWD_e,HWD_r,proj_size,dim_e,dim_r)\n",
    "        self.up = nn.Upsample(scale_factor=2)\n",
    "        self.conv=My_Conv_block(in_ch=dim_e+dim_r, out_ch=dim_e, groups=16)\n",
    "        \n",
    "    def forward(self,e,r):\n",
    "        '''\n",
    "        e: local feature (H x W x D x C)\n",
    "        r: global feature (H/2 x W/2 x D/2 x 2C)\n",
    "        '''\n",
    "        skip=e\n",
    "        e,r=self.cross_attn(e,r) # [B,C,D,H,W], [B,C,D/2,H/2,W/2]\n",
    "        e = torch.cat([e,self.up(r)],1) # B,3C,D,H,W\n",
    "        e=self.conv(e) # B,C,D,H,W\n",
    "        e=skip+e # skip connection\n",
    "        \n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------\n",
      "      Layer (type)                                 Output Shape         Param #     Tr. Param #\n",
      "================================================================================================\n",
      "    My_Cross_Att-1     [1, 32, 32, 32, 32], [1, 64, 16, 16, 16]       2,426,336       2,426,336\n",
      "        Upsample-2                          [1, 64, 32, 32, 32]               0               0\n",
      "   My_Conv_block-3                          [1, 32, 32, 32, 32]         110,784         110,784\n",
      "================================================================================================\n",
      "Total params: 2,537,120\n",
      "Trainable params: 2,537,120\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------------------------\n",
      "input(e,r): torch.Size([1, 32, 32, 32, 32]) torch.Size([1, 64, 16, 16, 16])\n",
      "output: torch.Size([1, 32, 32, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "e=torch.zeros(1,32,32,32,32) # [B,C,D,H,W] Local  \n",
    "r=torch.zeros(1,64,16,16,16) # [B,C,D,H,W] Global\n",
    "\n",
    "HWD_e=e.shape[2]*e.shape[3]*e.shape[4]\n",
    "HWD_r=r.shape[2]*r.shape[3]*r.shape[4]\n",
    "proj_size=64\n",
    "dim_e=e.shape[1]\n",
    "dim_r=r.shape[1]\n",
    "\n",
    "model=My_TIF(HWD_e,HWD_r,proj_size,dim_e,dim_r)\n",
    "\n",
    "print(summary(model,e,r))\n",
    "print('input(e,r):',e.shape, r.shape)\n",
    "print('output:',model(e,r).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Fusion 모듈  \n",
    "(channel reduction -> concat -> EPA -> Conv2d -> Skip Connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fusion(nn.Module):\n",
    "    def __init__(self,HWD_e,HWD_r,proj_size,dim_e,dim_r): \n",
    "        super().__init__()\n",
    "        # dim_e = local feature map channel (32,64,128)\n",
    "        # dim_r = global feature map channel (64,128,256)\n",
    "        self.HWD_e, self.HWD_r= HWD_e,HWD_r\n",
    "        self.HWD=self.HWD_e+self.HWD_r\n",
    "        self.reduction_r=nn.Linear(dim_r,dim_e) \n",
    "        self.dim=dim_e\n",
    "        self.EPA=My_EPA(input_size=self.HWD,hidden_size=self.dim,proj_size=proj_size)\n",
    "        # feature dimension reduction with conv2d\n",
    "        ks_h, ks_w=int(1+(1/8*HWD_e)),1\n",
    "        self.conv=nn.Conv2d(in_channels=1,out_channels=1,kernel_size=(ks_h,ks_w))\n",
    "        self.relu=nn.ReLU()\n",
    "\n",
    "    def forward(self,e,r):\n",
    "        '''\n",
    "        e: local feature (H x W x D x C)\n",
    "        r: global feature (H/2 x W/2 x D/2 x 2C)\n",
    "        '''\n",
    "        skip=e # e: [B, C, D, H, W]\n",
    "        b_e, c_e, d_e, h_e, w_e = e.shape[0],e.shape[1],e.shape[2],e.shape[3],e.shape[4]\n",
    "\n",
    "        e=e.reshape(e.shape[0],e.shape[1],e.shape[2]*e.shape[3]*e.shape[4]).permute(0,2,1) # e: [B, HWD, C]\n",
    "        r=r.reshape(r.shape[0],r.shape[1],r.shape[2]*r.shape[3]*r.shape[4]).permute(0,2,1) # r: [B, HWD/8, 2C]\n",
    "\n",
    "        r=self.reduction_r(r) # [B, HWD/8,C]\n",
    "        x=torch.cat([e,r],1) # [B, 9/8*HWD, C]\n",
    "        x=self.EPA(x) # [B, 9/8*HWD, C]\n",
    "        x=x.unsqueeze(1) # [B, 1, 9/8*HWD, C]\n",
    "        x=self.conv(x) # [B, 1, HWD, C]\n",
    "        x=x.squeeze(1).permute(0,2,1).reshape(b_e,c_e,d_e,h_e,w_e) # [B,C,D,H,W]\n",
    "        x=x+skip # skip connection\n",
    "        x=self.relu(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "      Layer (type)            Output Shape         Param #     Tr. Param #\n",
      "===========================================================================\n",
      "          Linear-1           [1, 4096, 32]           2,080           2,080\n",
      "          My_EPA-2          [1, 36864, 32]       2,362,440       2,362,440\n",
      "          Conv2d-3       [1, 1, 32768, 32]           4,098           4,098\n",
      "            ReLU-4     [1, 32, 32, 32, 32]               0               0\n",
      "===========================================================================\n",
      "Total params: 2,368,618\n",
      "Trainable params: 2,368,618\n",
      "Non-trainable params: 0\n",
      "---------------------------------------------------------------------------\n",
      "input(e,r): torch.Size([1, 32, 32, 32, 32]) torch.Size([1, 64, 16, 16, 16])\n",
      "output: torch.Size([1, 32, 32, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "e=torch.zeros(1,32,32,32,32) # [B,C,D,H,W] Local \n",
    "r=torch.zeros(1,64,16,16,16) # [B,C,D,H,W] Global\n",
    "\n",
    "HWD_e=e.shape[2]*e.shape[3]*e.shape[4]\n",
    "HWD_r=r.shape[2]*r.shape[3]*r.shape[4]\n",
    "proj_size=64\n",
    "dim_e=e.shape[1]\n",
    "dim_r=r.shape[1]\n",
    "\n",
    "model=Fusion(HWD_e,HWD_r,proj_size,dim_e,dim_r)\n",
    "\n",
    "print(summary(model,e,r))\n",
    "print('input(e,r):',e.shape, r.shape)\n",
    "print('output:',model(e,r).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. NFCE 모듈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_NFCE(nn.Module):\n",
    "    def __init__(self,in_dim): \n",
    "        super().__init__()\n",
    "        mid_dim=in_dim//4\n",
    "        self.conv1=nn.Conv3d(in_channels=in_dim, out_channels=mid_dim, kernel_size=1, bias=False) # Conv 1x1x1\n",
    "        self.norm1=nn.BatchNorm3d(mid_dim)\n",
    "\n",
    "        # depthwise seperable convolution\n",
    "        self.dsconv=nn.Sequential(\n",
    "            nn.Conv3d(in_channels=mid_dim, out_channels=mid_dim, kernel_size=3, padding=1, bias=False, groups=mid_dim), # Depth-wise Conv 3x3x3\n",
    "            nn.Conv3d(in_channels=mid_dim, out_channels=mid_dim, kernel_size=1, bias=False) # Point-wise Conv 1x1x1\n",
    "        )\n",
    "        self.norm2=nn.BatchNorm3d(mid_dim)\n",
    "\n",
    "        self.conv3=nn.Conv3d(in_channels=mid_dim, out_channels=in_dim, kernel_size=1, bias=False) # Conv 1x1x1\n",
    "        self.norm3=nn.BatchNorm3d(in_dim)\n",
    "\n",
    "        self.relu=nn.ReLU()\n",
    "\n",
    "    def forward(self,x):\n",
    "        '''\n",
    "        x: feature (H x W x D x C)\n",
    "        ex) 16 x 16 x 16 x 64\n",
    "        '''\n",
    "        save=x # [B, C, D, H, W]\n",
    "        \n",
    "        # 1x1x1 conv -> [B, C//4, D, H, W]\n",
    "        x=self.conv1(x) \n",
    "        x=self.norm1(x)\n",
    "        x=self.relu(x)\n",
    "\n",
    "        # depthwise seperable conv -> [B, C//4, D, H, W]\n",
    "        x=self.dsconv(x) \n",
    "        x=self.norm2(x)\n",
    "        x=self.relu(x)\n",
    "        \n",
    "        # 1x1x1 conv -> [B, C, D, H, W]\n",
    "        x=self.conv3(x)\n",
    "        x=self.norm3(x)\n",
    "        \n",
    "        # skip connection -> [B, C, D, H, W]\n",
    "        x=x+save \n",
    "        x=self.relu(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "      Layer (type)            Output Shape         Param #     Tr. Param #\n",
      "===========================================================================\n",
      "          Conv3d-1      [1, 8, 32, 32, 32]             256             256\n",
      "     BatchNorm3d-2      [1, 8, 32, 32, 32]              16              16\n",
      "            ReLU-3      [1, 8, 32, 32, 32]               0               0\n",
      "          Conv3d-4      [1, 8, 32, 32, 32]             216             216\n",
      "          Conv3d-5      [1, 8, 32, 32, 32]              64              64\n",
      "     BatchNorm3d-6      [1, 8, 32, 32, 32]              16              16\n",
      "          Conv3d-7     [1, 32, 32, 32, 32]             256             256\n",
      "     BatchNorm3d-8     [1, 32, 32, 32, 32]              64              64\n",
      "===========================================================================\n",
      "Total params: 888\n",
      "Trainable params: 888\n",
      "Non-trainable params: 0\n",
      "---------------------------------------------------------------------------\n",
      "Input: torch.Size([1, 32, 32, 32, 32])\n",
      "Output: torch.Size([1, 32, 32, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "x=torch.zeros(1, 32, 32, 32, 32)\n",
    "in_dim=x.shape[1]\n",
    "\n",
    "model=My_NFCE(in_dim)\n",
    "print(summary(model,x))\n",
    "print('Input:',x.shape)\n",
    "print('Output:',model(x).shape)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
