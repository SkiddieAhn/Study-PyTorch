{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch,sys\n",
    "import math\n",
    "from torch import nn, einsum\n",
    "from einops import rearrange\n",
    "from pytorch_model_summary import summary\n",
    "from monai.networks.layers.utils import get_norm_layer\n",
    "from unetr_plus_plus.unetr_pp.network_architecture.dynunet_block import get_conv_layer\n",
    "from my_unetr_plus_plus.unetr_pp.network_architecture.my_module import My_Transformer,My_EPA2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0. ResBlock (BN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        # Residual Block\n",
    "        self.residual_block = nn.Sequential(\n",
    "                nn.Conv3d(in_dim, out_dim, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm3d(num_features=out_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv3d(out_dim, out_dim, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm3d(num_features=out_dim),\n",
    "            )            \n",
    "        self.conv=nn.Conv3d(in_dim, out_dim, kernel_size=1)\n",
    "        self.relu = nn.ReLU()\n",
    "                  \n",
    "    def forward(self, x):\n",
    "        out = self.residual_block(x)  # (conv 3x3x3) *2\n",
    "        out = out + self.conv(x)  # residual connection\n",
    "        out = self.relu(out) # relu\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1x1x1 먼저 해준 버전 (GN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.conv=nn.Conv3d(in_dim, out_dim, kernel_size=1)\n",
    "        # Residual Block\n",
    "        self.residual_block = nn.Sequential(\n",
    "                nn.Conv3d(out_dim, out_dim, kernel_size=3, padding=1),\n",
    "                nn.GroupNorm(num_channels=out_dim,num_groups=32),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv3d(out_dim, out_dim, kernel_size=3, padding=1),\n",
    "                nn.GroupNorm(num_channels=out_dim,num_groups=32),\n",
    "            )            \n",
    "        self.relu = nn.ReLU()\n",
    "                  \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        out = self.residual_block(x)  # (conv 3x3x3) *2\n",
    "        out = out + x  # residual connection\n",
    "        out = self.relu(out) # relu\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------\n",
      "      Layer (type)          Output Shape         Param #     Tr. Param #\n",
      "=========================================================================\n",
      "          Conv3d-1     [1, 256, 4, 4, 4]         262,400         262,400\n",
      "          Conv3d-2     [1, 256, 4, 4, 4]       1,769,728       1,769,728\n",
      "       GroupNorm-3     [1, 256, 4, 4, 4]             512             512\n",
      "            ReLU-4     [1, 256, 4, 4, 4]               0               0\n",
      "          Conv3d-5     [1, 256, 4, 4, 4]       1,769,728       1,769,728\n",
      "       GroupNorm-6     [1, 256, 4, 4, 4]             512             512\n",
      "            ReLU-7     [1, 256, 4, 4, 4]               0               0\n",
      "=========================================================================\n",
      "Total params: 3,802,880\n",
      "Trainable params: 3,802,880\n",
      "Non-trainable params: 0\n",
      "-------------------------------------------------------------------------\n",
      "input: torch.Size([1, 1024, 4, 4, 4])\n",
      "output: torch.Size([1, 256, 4, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "x=torch.zeros(1,1024,4,4,4) # [B,C,D,H,W] input: 32 x 32 x 32 x 128\n",
    "model=ResBlock(in_dim=x.shape[1], out_dim=x.shape[1]//4)\n",
    "\n",
    "print(summary(model,x))\n",
    "print('input:',x.shape)\n",
    "print('output:',model(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ASTB (GN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASTB(nn.Module):\n",
    "    def __init__(self,proj_size): \n",
    "        '''\n",
    "        All Scale TIF Block\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        channels=[32, 64, 128, 256]\n",
    "        resolutions=[32*32*32, 16*16*16, 8*8*8, 4*4*4]\n",
    "\n",
    "        # 1. norm & pooling module\n",
    "        self.norm_set=nn.ModuleList([])\n",
    "        self.avgpool=nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        for crt_cnl in channels:\n",
    "            self.norm_set.append(nn.LayerNorm(crt_cnl))\n",
    "\n",
    "        # 2. channel control module\n",
    "        self.control_cnl_set=nn.ModuleList([])\n",
    "\n",
    "        for std_cnl in channels:\n",
    "            control_cnl=nn.ModuleList([])\n",
    "            for in_cnl in channels: \n",
    "                if in_cnl == std_cnl: # --> Identity\n",
    "                    control_cnl.append(nn.Identity())\n",
    "\n",
    "                else: # --> channel unify\n",
    "                    control_cnl.append(nn.Linear(in_features=in_cnl,out_features=std_cnl))\n",
    "            self.control_cnl_set.append(control_cnl)\n",
    "\n",
    "        # 3. Transformer module\n",
    "        self.transformer_set=nn.ModuleList([])\n",
    "        \n",
    "        for i,crt_cnl in enumerate(channels):\n",
    "            crt_rst = resolutions[i]+3\n",
    "            self.transformer_set.append(\n",
    "                My_Transformer(input_size=crt_rst, proj_size=proj_size, dim=crt_cnl, depth=1, heads=4, dim_head=crt_cnl//4, mlp_dim=crt_cnl*2)\n",
    "            )\n",
    "\n",
    "        # 4. channel & resolution control module\n",
    "        self.control_cnl_rst_set=nn.ModuleList([])\n",
    "\n",
    "        for std_cnl in channels:\n",
    "            control_cnl_rst=nn.ModuleList([])\n",
    "            for in_cnl in channels: \n",
    "                if in_cnl == std_cnl: # --> Identity\n",
    "                    control_cnl_rst.append(nn.Identity())\n",
    "\n",
    "                elif in_cnl > std_cnl: # --> Upsampling\n",
    "                    itr=int(math.log2(in_cnl//std_cnl))\n",
    "                    self.up_layer=nn.Sequential()\n",
    "                    cnl=in_cnl\n",
    "                    for i in range(itr):\n",
    "                        self.up_layer.add_module(f'upsample_{i+1}',nn.ConvTranspose3d(in_channels=cnl,out_channels=cnl//2,kernel_size=2,stride=2))\n",
    "                        cnl=cnl//2\n",
    "                    control_cnl_rst.append(self.up_layer)\n",
    "\n",
    "                else: # --> downsampling\n",
    "                    itr=int(math.log2(std_cnl//in_cnl))\n",
    "                    self.down_layer=nn.Sequential()\n",
    "                    cnl=in_cnl\n",
    "                    for i in range(itr):\n",
    "                        self.down_layer.add_module(f'downsample_{i+1}',nn.Conv3d(in_channels=cnl,out_channels=cnl*2,kernel_size=2,stride=2))\n",
    "                        self.down_layer.add_module(f'group_norm{i+1}',get_norm_layer(name=(\"group\", {\"num_groups\": cnl}), channels=cnl*2)) # <GN>\n",
    "                        cnl=cnl*2\n",
    "                    control_cnl_rst.append(self.down_layer)\n",
    "\n",
    "            self.control_cnl_rst_set.append(control_cnl_rst)\n",
    "\n",
    "        # 5. ResBlock module\n",
    "        self.resblock_set=nn.ModuleList([])\n",
    "\n",
    "        for crt_cnl in channels:\n",
    "            self.resblock_set.append(ResBlock(in_dim=crt_cnl*4, out_dim=crt_cnl))\n",
    "\n",
    "    def forward(self,x1,x2,x3,x4): \n",
    "        '''\n",
    "        x1: H x W x D x C\n",
    "        x2: H/2 x W/2 x D/2 x 2C\n",
    "        x3: H/4 x W/4 x D/4 x 4C\n",
    "        x4: H/8 x W/8 x D/8 x 8C\n",
    "        '''\n",
    "        save1, save2, save3, save4 = x1, x2, x3, x4       \n",
    "        b_x1, c_x1, d_x1, h_x1, w_x1 = x1.shape[0], x1.shape[1], x1.shape[2], x1.shape[3], x1.shape[4]\n",
    "        b_x2, c_x2, d_x2, h_x2, w_x2 = x2.shape[0], x2.shape[1], x2.shape[2], x2.shape[3], x2.shape[4]\n",
    "        b_x3, c_x3, d_x3, h_x3, w_x3 = x3.shape[0], x3.shape[1], x3.shape[2], x3.shape[3], x3.shape[4]\n",
    "        b_x4, c_x4, d_x4, h_x4, w_x4 = x4.shape[0], x4.shape[1], x4.shape[2], x4.shape[3], x4.shape[4]\n",
    "\n",
    "        # reshape\n",
    "        x1 = x1.reshape(b_x1, c_x1, -1).permute(0, 2, 1) # B, N, C (N=HWD)\n",
    "        x2 = x2.reshape(b_x2, c_x2, -1).permute(0, 2, 1) # B, N/8, 2C \n",
    "        x3 = x3.reshape(b_x3, c_x3, -1).permute(0, 2, 1) # B, N/64, 4C \n",
    "        x4 = x4.reshape(b_x4, c_x4, -1).permute(0, 2, 1) # B, N/512, 8C \n",
    "\n",
    "        # norm & pool\n",
    "        x1_p = torch.flatten(self.avgpool(self.norm_set[0](x1).transpose(1,2)), 1) # B, C\n",
    "        x2_p = torch.flatten(self.avgpool(self.norm_set[1](x2).transpose(1,2)), 1) # B, 2C\n",
    "        x3_p = torch.flatten(self.avgpool(self.norm_set[2](x3).transpose(1,2)), 1) # B, 4C\n",
    "        x4_p = torch.flatten(self.avgpool(self.norm_set[3](x4).transpose(1,2)), 1) # B, 8C\n",
    "\n",
    "        '''\n",
    "        Transformer (Stage1)\n",
    "        '''\n",
    "        # channel control\n",
    "        x2_p1 = self.control_cnl_set[0][1](x2_p).unsqueeze(1) # B, 1, C\n",
    "        x3_p1 = self.control_cnl_set[0][2](x3_p).unsqueeze(1) # B, 1, C\n",
    "        x4_p1 = self.control_cnl_set[0][3](x4_p).unsqueeze(1) # B, 1, C\n",
    "\n",
    "        # concat & transformer & reshape\n",
    "        X1 = self.transformer_set[0](torch.cat([x2_p1, x3_p1, x4_p1, x1],dim=1))[:, 3:, :] # B, N+3, C -> B, N, C\n",
    "        X1 = X1.reshape(b_x1, c_x1, d_x1, h_x1, w_x1) # B, C, D, H, W\n",
    "\n",
    "        '''\n",
    "        Transformer (Stage2)\n",
    "        '''\n",
    "        # channel control\n",
    "        x1_p2 = self.control_cnl_set[1][0](x1_p).unsqueeze(1) # B, 1, 2C\n",
    "        x3_p2 = self.control_cnl_set[1][2](x3_p).unsqueeze(1) # B, 1, 2C\n",
    "        x4_p2 = self.control_cnl_set[1][3](x4_p).unsqueeze(1) # B, 1, 2C\n",
    "\n",
    "        # concat & transformer & reshape\n",
    "        X2 = self.transformer_set[1](torch.cat([x1_p2, x3_p2, x4_p2, x2],dim=1))[:, 3:, :] # B, (N/8)+3, 2C -> B, N/8, 2C\n",
    "        X2 = X2.reshape(b_x2, c_x2, d_x2, h_x2, w_x2) # B, 2C, D/2, H/2, W/2\n",
    "\n",
    "        '''\n",
    "        Transformer (Stage3)\n",
    "        '''\n",
    "        # channel control\n",
    "        x1_p3 = self.control_cnl_set[2][0](x1_p).unsqueeze(1) # B, 1, 4C\n",
    "        x2_p3 = self.control_cnl_set[2][1](x2_p).unsqueeze(1) # B, 1, 4C\n",
    "        x4_p3 = self.control_cnl_set[2][3](x4_p).unsqueeze(1) # B, 1, 4C\n",
    "\n",
    "        # concat & transformer & reshape\n",
    "        X3 = self.transformer_set[2](torch.cat([x1_p3, x2_p3, x4_p3, x3],dim=1))[:, 3:, :] # B, (N/64)+3, 4C -> B, N/64, 4C\n",
    "        X3 = X3.reshape(b_x3, c_x3, d_x3, h_x3, w_x3) # B, 4C, D/4, H/4, W/4\n",
    "\n",
    "        '''\n",
    "        Transformer (Stage4)\n",
    "        '''\n",
    "        # channel control\n",
    "        x1_p4 = self.control_cnl_set[3][0](x1_p).unsqueeze(1) # B, 1, 8C\n",
    "        x2_p4 = self.control_cnl_set[3][1](x2_p).unsqueeze(1) # B, 1, 8C\n",
    "        x3_p4 = self.control_cnl_set[3][2](x3_p).unsqueeze(1) # B, 1, 8C\n",
    "\n",
    "        # concat & transformer & reshape\n",
    "        X4 = self.transformer_set[3](torch.cat([x1_p4, x2_p4, x3_p4, x4],dim=1))[:, 3:, :] # B, (N/512)+3, 8C -> B, N/512, 8C\n",
    "        X4 = X4.reshape(b_x4, c_x4, d_x4, h_x4, w_x4) # B, 8C, D/8, H/8, W/8\n",
    "\n",
    "        '''\n",
    "        ResBlock (Stage1)\n",
    "        '''\n",
    "        # channel & resolution control\n",
    "        X2_1 = self.control_cnl_rst_set[0][1](X2) # B, C, D, H, W\n",
    "        X3_1 = self.control_cnl_rst_set[0][2](X3) # B, C, D, H, W\n",
    "        X4_1 = self.control_cnl_rst_set[0][3](X4) # B, C, D, H, W\n",
    "\n",
    "        # concat & resblock\n",
    "        y1 = self.resblock_set[0](torch.cat([X2_1, X3_1, X4_1, X1], dim=1)) # B, C, D, H, W\n",
    "\n",
    "        '''\n",
    "        ResBlock (Stage2)\n",
    "        '''\n",
    "        # channel & resolution control\n",
    "        X1_2 = self.control_cnl_rst_set[1][0](X1) # B, 2C, D/2, H/2, W/2\n",
    "        X3_2 = self.control_cnl_rst_set[1][2](X3) # B, 2C, D/2, H/2, W/2\n",
    "        X4_2 = self.control_cnl_rst_set[1][3](X4) # B, 2C, D/2, H/2, W/2\n",
    "\n",
    "        # concat & resblock\n",
    "        y2 = self.resblock_set[1](torch.cat([X1_2, X3_2, X4_2, X2], dim=1)) # B, 2C, D/2, H/2, W/2\n",
    "\n",
    "        '''\n",
    "        ResBlock (Stage3)\n",
    "        '''\n",
    "        # channel & resolution control\n",
    "        X1_3 = self.control_cnl_rst_set[2][0](X1) # B, 4C, D/4, H/4, W/4\n",
    "        X2_3 = self.control_cnl_rst_set[2][1](X2) # B, 4C, D/4, H/4, W/4\n",
    "        X4_3 = self.control_cnl_rst_set[2][3](X4) # B, 4C, D/4, H/4, W/4\n",
    "\n",
    "        # concat & resblock\n",
    "        y3 = self.resblock_set[2](torch.cat([X1_3, X2_3, X4_3, X3], dim=1)) # B, 4C, D/4, H/4, W/4\n",
    "\n",
    "        '''\n",
    "        ResBlock (Stage4)\n",
    "        '''\n",
    "        # channel & resolution control\n",
    "        X1_4 = self.control_cnl_rst_set[3][0](X1) # B, 8C, D/8, H/8, W/8\n",
    "        X2_4 = self.control_cnl_rst_set[3][1](X2) # B, 8C, D/8, H/8, W/8\n",
    "        X3_4 = self.control_cnl_rst_set[3][2](X3) # B, 8C, D/8, H/8, W/8\n",
    "\n",
    "        # concat & resblock\n",
    "        y4 = self.resblock_set[3](torch.cat([X1_4, X2_4, X3_4, X4], dim=1)) # B, 8C, D/8, H/8, W/8\n",
    "\n",
    "        '''\n",
    "        Skip Connection\n",
    "        '''\n",
    "        y1 = y1 + save1\n",
    "        y2 = y2 + save2\n",
    "        y3 = y3 + save3\n",
    "        y4 = y4 + save4\n",
    "\n",
    "        return y1, y2, y3, y4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "          Layer (type)            Output Shape         Param #     Tr. Param #\n",
      "===============================================================================\n",
      "           LayerNorm-1          [1, 32768, 32]              64              64\n",
      "   AdaptiveAvgPool1d-2              [1, 32, 1]               0               0\n",
      "           LayerNorm-3           [1, 4096, 64]             128             128\n",
      "           LayerNorm-4           [1, 512, 128]             256             256\n",
      "           LayerNorm-5            [1, 64, 256]             512             512\n",
      "              Linear-6                 [1, 32]           2,080           2,080\n",
      "              Linear-7                 [1, 32]           4,128           4,128\n",
      "              Linear-8                 [1, 32]           8,224           8,224\n",
      "      My_Transformer-9          [1, 32771, 32]       2,105,856       2,105,856\n",
      "             Linear-10                 [1, 64]           2,112           2,112\n",
      "             Linear-11                 [1, 64]           8,256           8,256\n",
      "             Linear-12                 [1, 64]          16,448          16,448\n",
      "     My_Transformer-13           [1, 4099, 64]         295,680         295,680\n",
      "             Linear-14                [1, 128]           4,224           4,224\n",
      "             Linear-15                [1, 128]           8,320           8,320\n",
      "             Linear-16                [1, 128]          32,896          32,896\n",
      "     My_Transformer-17           [1, 515, 128]         165,120         165,120\n",
      "             Linear-18                [1, 256]           8,448           8,448\n",
      "             Linear-19                [1, 256]          16,640          16,640\n",
      "             Linear-20                [1, 256]          33,024          33,024\n",
      "     My_Transformer-21            [1, 67, 256]         530,688         530,688\n",
      "    ConvTranspose3d-22     [1, 32, 32, 32, 32]          16,416          16,416\n",
      "    ConvTranspose3d-23     [1, 64, 16, 16, 16]          65,600          65,600\n",
      "    ConvTranspose3d-24     [1, 32, 32, 32, 32]          16,416          16,416\n",
      "    ConvTranspose3d-25       [1, 128, 8, 8, 8]         262,272         262,272\n",
      "    ConvTranspose3d-26     [1, 64, 16, 16, 16]          65,600          65,600\n",
      "    ConvTranspose3d-27     [1, 32, 32, 32, 32]          16,416          16,416\n",
      "           ResBlock-28     [1, 32, 32, 32, 32]          59,616          59,616\n",
      "             Conv3d-29     [1, 64, 16, 16, 16]          16,448          16,448\n",
      "          GroupNorm-30     [1, 64, 16, 16, 16]             128             128\n",
      "    ConvTranspose3d-31     [1, 64, 16, 16, 16]          65,600          65,600\n",
      "    ConvTranspose3d-32       [1, 128, 8, 8, 8]         262,272         262,272\n",
      "    ConvTranspose3d-33     [1, 64, 16, 16, 16]          65,600          65,600\n",
      "           ResBlock-34     [1, 64, 16, 16, 16]         238,016         238,016\n",
      "             Conv3d-35     [1, 64, 16, 16, 16]          16,448          16,448\n",
      "          GroupNorm-36     [1, 64, 16, 16, 16]             128             128\n",
      "             Conv3d-37       [1, 128, 8, 8, 8]          65,664          65,664\n",
      "          GroupNorm-38       [1, 128, 8, 8, 8]             256             256\n",
      "             Conv3d-39       [1, 128, 8, 8, 8]          65,664          65,664\n",
      "          GroupNorm-40       [1, 128, 8, 8, 8]             256             256\n",
      "    ConvTranspose3d-41       [1, 128, 8, 8, 8]         262,272         262,272\n",
      "           ResBlock-42       [1, 128, 8, 8, 8]         951,168         951,168\n",
      "             Conv3d-43     [1, 64, 16, 16, 16]          16,448          16,448\n",
      "          GroupNorm-44     [1, 64, 16, 16, 16]             128             128\n",
      "             Conv3d-45       [1, 128, 8, 8, 8]          65,664          65,664\n",
      "          GroupNorm-46       [1, 128, 8, 8, 8]             256             256\n",
      "             Conv3d-47       [1, 256, 4, 4, 4]         262,400         262,400\n",
      "          GroupNorm-48       [1, 256, 4, 4, 4]             512             512\n",
      "             Conv3d-49       [1, 128, 8, 8, 8]          65,664          65,664\n",
      "          GroupNorm-50       [1, 128, 8, 8, 8]             256             256\n",
      "             Conv3d-51       [1, 256, 4, 4, 4]         262,400         262,400\n",
      "          GroupNorm-52       [1, 256, 4, 4, 4]             512             512\n",
      "             Conv3d-53       [1, 256, 4, 4, 4]         262,400         262,400\n",
      "          GroupNorm-54       [1, 256, 4, 4, 4]             512             512\n",
      "           ResBlock-55       [1, 256, 4, 4, 4]       3,802,880       3,802,880\n",
      "===============================================================================\n",
      "Total params: 10,495,392\n",
      "Trainable params: 10,495,392\n",
      "Non-trainable params: 0\n",
      "-------------------------------------------------------------------------------\n",
      "input: torch.Size([1, 32, 32, 32, 32]) torch.Size([1, 64, 16, 16, 16]) torch.Size([1, 128, 8, 8, 8]) torch.Size([1, 256, 4, 4, 4])\n",
      "output: torch.Size([1, 32, 32, 32, 32]) torch.Size([1, 64, 16, 16, 16]) torch.Size([1, 128, 8, 8, 8]) torch.Size([1, 256, 4, 4, 4])\n",
      "<bound method Module.parameters of ASTB(\n",
      "  (norm_set): ModuleList(\n",
      "    (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    (3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
      "  (control_cnl_set): ModuleList(\n",
      "    (0): ModuleList(\n",
      "      (0): Identity()\n",
      "      (1): Linear(in_features=64, out_features=32, bias=True)\n",
      "      (2): Linear(in_features=128, out_features=32, bias=True)\n",
      "      (3): Linear(in_features=256, out_features=32, bias=True)\n",
      "    )\n",
      "    (1): ModuleList(\n",
      "      (0): Linear(in_features=32, out_features=64, bias=True)\n",
      "      (1): Identity()\n",
      "      (2): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (3): Linear(in_features=256, out_features=64, bias=True)\n",
      "    )\n",
      "    (2): ModuleList(\n",
      "      (0): Linear(in_features=32, out_features=128, bias=True)\n",
      "      (1): Linear(in_features=64, out_features=128, bias=True)\n",
      "      (2): Identity()\n",
      "      (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    )\n",
      "    (3): ModuleList(\n",
      "      (0): Linear(in_features=32, out_features=256, bias=True)\n",
      "      (1): Linear(in_features=64, out_features=256, bias=True)\n",
      "      (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (3): Identity()\n",
      "    )\n",
      "  )\n",
      "  (transformer_set): ModuleList(\n",
      "    (0): My_Transformer(\n",
      "      (layers): ModuleList(\n",
      "        (0): ModuleList(\n",
      "          (0): PreNorm(\n",
      "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): My_Attention(\n",
      "              (to_qkv): Linear(in_features=32, out_features=96, bias=False)\n",
      "              (E): Linear(in_features=32771, out_features=64, bias=True)\n",
      "              (F): Linear(in_features=32771, out_features=64, bias=True)\n",
      "              (to_out): Sequential(\n",
      "                (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): PreNorm(\n",
      "            (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): FeedForward(\n",
      "              (net): Sequential(\n",
      "                (0): Linear(in_features=32, out_features=64, bias=True)\n",
      "                (1): GELU()\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "                (3): Linear(in_features=64, out_features=32, bias=True)\n",
      "                (4): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): My_Transformer(\n",
      "      (layers): ModuleList(\n",
      "        (0): ModuleList(\n",
      "          (0): PreNorm(\n",
      "            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): My_Attention(\n",
      "              (to_qkv): Linear(in_features=64, out_features=192, bias=False)\n",
      "              (E): Linear(in_features=4099, out_features=64, bias=True)\n",
      "              (F): Linear(in_features=4099, out_features=64, bias=True)\n",
      "              (to_out): Sequential(\n",
      "                (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): PreNorm(\n",
      "            (norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): FeedForward(\n",
      "              (net): Sequential(\n",
      "                (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "                (1): GELU()\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "                (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "                (4): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): My_Transformer(\n",
      "      (layers): ModuleList(\n",
      "        (0): ModuleList(\n",
      "          (0): PreNorm(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): My_Attention(\n",
      "              (to_qkv): Linear(in_features=128, out_features=384, bias=False)\n",
      "              (E): Linear(in_features=515, out_features=64, bias=True)\n",
      "              (F): Linear(in_features=515, out_features=64, bias=True)\n",
      "              (to_out): Sequential(\n",
      "                (0): Linear(in_features=128, out_features=128, bias=True)\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): PreNorm(\n",
      "            (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): FeedForward(\n",
      "              (net): Sequential(\n",
      "                (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "                (1): GELU()\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "                (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "                (4): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): My_Transformer(\n",
      "      (layers): ModuleList(\n",
      "        (0): ModuleList(\n",
      "          (0): PreNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): My_Attention(\n",
      "              (to_qkv): Linear(in_features=256, out_features=768, bias=False)\n",
      "              (E): Linear(in_features=67, out_features=64, bias=True)\n",
      "              (F): Linear(in_features=67, out_features=64, bias=True)\n",
      "              (to_out): Sequential(\n",
      "                (0): Linear(in_features=256, out_features=256, bias=True)\n",
      "                (1): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1): PreNorm(\n",
      "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "            (fn): FeedForward(\n",
      "              (net): Sequential(\n",
      "                (0): Linear(in_features=256, out_features=512, bias=True)\n",
      "                (1): GELU()\n",
      "                (2): Dropout(p=0.0, inplace=False)\n",
      "                (3): Linear(in_features=512, out_features=256, bias=True)\n",
      "                (4): Dropout(p=0.0, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (control_cnl_rst_set): ModuleList(\n",
      "    (0): ModuleList(\n",
      "      (0): Identity()\n",
      "      (1): Sequential(\n",
      "        (upsample_1): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (upsample_1): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "        (upsample_2): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (upsample_1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "        (upsample_2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "        (upsample_3): ConvTranspose3d(64, 32, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "      )\n",
      "    )\n",
      "    (1): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (downsample_1): Conv3d(32, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "        (group_norm1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (1): Identity()\n",
      "      (2): Sequential(\n",
      "        (upsample_1): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "      )\n",
      "      (3): Sequential(\n",
      "        (upsample_1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "        (upsample_2): ConvTranspose3d(128, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "      )\n",
      "    )\n",
      "    (2): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (downsample_1): Conv3d(32, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "        (group_norm1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "        (downsample_2): Conv3d(64, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "        (group_norm2): GroupNorm(64, 128, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (downsample_1): Conv3d(64, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "        (group_norm1): GroupNorm(64, 128, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (2): Identity()\n",
      "      (3): Sequential(\n",
      "        (upsample_1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "      )\n",
      "    )\n",
      "    (3): ModuleList(\n",
      "      (0): Sequential(\n",
      "        (downsample_1): Conv3d(32, 64, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "        (group_norm1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "        (downsample_2): Conv3d(64, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "        (group_norm2): GroupNorm(64, 128, eps=1e-05, affine=True)\n",
      "        (downsample_3): Conv3d(128, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "        (group_norm3): GroupNorm(128, 256, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (downsample_1): Conv3d(64, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "        (group_norm1): GroupNorm(64, 128, eps=1e-05, affine=True)\n",
      "        (downsample_2): Conv3d(128, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "        (group_norm2): GroupNorm(128, 256, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (2): Sequential(\n",
      "        (downsample_1): Conv3d(128, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "        (group_norm1): GroupNorm(128, 256, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (3): Identity()\n",
      "    )\n",
      "  )\n",
      "  (up_layer): Sequential(\n",
      "    (upsample_1): ConvTranspose3d(256, 128, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "  )\n",
      "  (down_layer): Sequential(\n",
      "    (downsample_1): Conv3d(128, 256, kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
      "    (group_norm1): GroupNorm(128, 256, eps=1e-05, affine=True)\n",
      "  )\n",
      "  (resblock_set): ModuleList(\n",
      "    (0): ResBlock(\n",
      "      (conv): Conv3d(128, 32, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (residual_block): Sequential(\n",
      "        (0): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (1): GroupNorm(32, 32, eps=1e-05, affine=True)\n",
      "        (2): ReLU()\n",
      "        (3): Conv3d(32, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (4): GroupNorm(32, 32, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (1): ResBlock(\n",
      "      (conv): Conv3d(256, 64, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (residual_block): Sequential(\n",
      "        (0): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (1): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "        (2): ReLU()\n",
      "        (3): Conv3d(64, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (4): GroupNorm(32, 64, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (2): ResBlock(\n",
      "      (conv): Conv3d(512, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (residual_block): Sequential(\n",
      "        (0): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "        (2): ReLU()\n",
      "        (3): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (4): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "    (3): ResBlock(\n",
      "      (conv): Conv3d(1024, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n",
      "      (residual_block): Sequential(\n",
      "        (0): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "        (2): ReLU()\n",
      "        (3): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      "        (4): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
      "      )\n",
      "      (relu): ReLU()\n",
      "    )\n",
      "  )\n",
      ")>\n"
     ]
    }
   ],
   "source": [
    "x1=torch.zeros(1,32,32,32,32)\n",
    "x2=torch.zeros(1,64,16,16,16)\n",
    "x3=torch.zeros(1,128,8,8,8)\n",
    "x4=torch.zeros(1,256,4,4,4)\n",
    "\n",
    "model=ASTB(proj_size=64)\n",
    "y1, y2, y3, y4=model(x1,x2,x3,x4)\n",
    "\n",
    "print(summary(model,x1,x2,x3,x4))\n",
    "print('input:',x1.shape,x2.shape,x3.shape,x4.shape)\n",
    "print('output:',y1.shape,y2.shape,y3.shape,y4.shape)\n",
    "print(model.parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. LSTB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTB(nn.Module):\n",
    "    '''\n",
    "    Large Scale TIF Block\n",
    "    '''\n",
    "    def __init__(self,proj_size): \n",
    "        super().__init__()\n",
    "\n",
    "        channels=[32, 64, 128, 256]\n",
    "        resolutions=[32*32*32, 16*16*16, 8*8*8, 4*4*4]\n",
    "\n",
    "        # 1. norm & pooling module\n",
    "        self.norm_set=nn.ModuleList([])\n",
    "        self.avgpool=nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        for crt_cnl in channels:\n",
    "            self.norm_set.append(nn.LayerNorm(crt_cnl))\n",
    "\n",
    "        # 2. channel control module\n",
    "        self.control_cnl_set=nn.ModuleList([])\n",
    "\n",
    "        for std_cnl in channels:\n",
    "            control_cnl=nn.ModuleList([])\n",
    "            for in_cnl in channels: \n",
    "                if in_cnl == std_cnl: # --> Identity\n",
    "                    control_cnl.append(nn.Identity())\n",
    "\n",
    "                else: # --> channel unify\n",
    "                    control_cnl.append(nn.Linear(in_features=in_cnl,out_features=std_cnl))\n",
    "            self.control_cnl_set.append(control_cnl)\n",
    "\n",
    "        # 3. Transformer module\n",
    "        self.transformer_set=nn.ModuleList([])\n",
    "        \n",
    "        for i,crt_cnl in enumerate(channels):\n",
    "            crt_rst = resolutions[i]+i # stage1 = NxC, stage2 = ((N/8)+1)x2C, stage3 = ((N/64)+2)x4C, stage4 = ((N/512)+3)x8C\n",
    "            self.transformer_set.append(\n",
    "                My_Transformer(input_size=crt_rst, proj_size=proj_size, dim=crt_cnl, depth=1, heads=4, dim_head=crt_cnl//4, mlp_dim=crt_cnl*2)\n",
    "            )\n",
    "\n",
    "        # 4. channel & resolution control module\n",
    "        self.control_cnl_rst_set=nn.ModuleList([])\n",
    "\n",
    "        for std_cnl in channels:\n",
    "            control_cnl_rst=nn.ModuleList([])\n",
    "            for in_cnl in channels: \n",
    "                if in_cnl == std_cnl: # --> Identity\n",
    "                    control_cnl_rst.append(nn.Identity())\n",
    "\n",
    "                elif in_cnl > std_cnl: # --> Upsampling\n",
    "                    itr=int(math.log2(in_cnl//std_cnl))\n",
    "                    self.up_layer=nn.Sequential()\n",
    "                    cnl=in_cnl\n",
    "                    for i in range(itr):\n",
    "                        self.up_layer.add_module(f'upsample_{i+1}',nn.ConvTranspose3d(in_channels=cnl,out_channels=cnl//2,kernel_size=2,stride=2))\n",
    "                        cnl=cnl//2\n",
    "                    control_cnl_rst.append(self.up_layer)\n",
    "\n",
    "                else: # --> downsampling\n",
    "                    itr=int(math.log2(std_cnl//in_cnl))\n",
    "                    self.down_layer=nn.Sequential()\n",
    "                    cnl=in_cnl\n",
    "                    for i in range(itr):\n",
    "                        self.down_layer.add_module(f'downsample_{i+1}',nn.Conv3d(in_channels=cnl,out_channels=cnl*2,kernel_size=2,stride=2))\n",
    "                        cnl=cnl*2\n",
    "                    control_cnl_rst.append(self.down_layer)\n",
    "\n",
    "            self.control_cnl_rst_set.append(control_cnl_rst)\n",
    "\n",
    "        # 5. ResBlock module\n",
    "        self.resblock_set=nn.ModuleList([])\n",
    "        self.resblock_set.append(ResBlock(in_dim=32, out_dim=32)) # C -> C\n",
    "        self.resblock_set.append(ResBlock(in_dim=128, out_dim=64)) # 4C -> 2C\n",
    "        self.resblock_set.append(ResBlock(in_dim=384, out_dim= 128)) # 12C -> 4C\n",
    "        self.resblock_set.append(ResBlock(in_dim=1024, out_dim=256)) # 32C -> 8C\n",
    "\n",
    "\n",
    "    def forward(self,x1,x2,x3,x4): \n",
    "        '''\n",
    "        x1: H x W x D x C\n",
    "        x2: H/2 x W/2 x D/2 x 2C\n",
    "        x3: H/4 x W/4 x D/4 x 4C\n",
    "        x4: H/8 x W/8 x D/8 x 8C\n",
    "        '''\n",
    "        save1, save2, save3, save4 = x1, x2, x3, x4\n",
    "        b_x1, c_x1, d_x1, h_x1, w_x1 = x1.shape[0], x1.shape[1], x1.shape[2], x1.shape[3], x1.shape[4]\n",
    "        b_x2, c_x2, d_x2, h_x2, w_x2 = x2.shape[0], x2.shape[1], x2.shape[2], x2.shape[3], x2.shape[4]\n",
    "        b_x3, c_x3, d_x3, h_x3, w_x3 = x3.shape[0], x3.shape[1], x3.shape[2], x3.shape[3], x3.shape[4]\n",
    "        b_x4, c_x4, d_x4, h_x4, w_x4 = x4.shape[0], x4.shape[1], x4.shape[2], x4.shape[3], x4.shape[4]\n",
    "\n",
    "        # reshape\n",
    "        x1 = x1.reshape(b_x1, c_x1, -1).permute(0, 2, 1) # B, N, C (N=HWD)\n",
    "        x2 = x2.reshape(b_x2, c_x2, -1).permute(0, 2, 1) # B, N/8, 2C \n",
    "        x3 = x3.reshape(b_x3, c_x3, -1).permute(0, 2, 1) # B, N/64, 4C \n",
    "        x4 = x4.reshape(b_x4, c_x4, -1).permute(0, 2, 1) # B, N/512, 8C \n",
    "\n",
    "        # norm & pool\n",
    "        x1_p = torch.flatten(self.avgpool(self.norm_set[0](x1).transpose(1,2)), 1) # B, C\n",
    "        x2_p = torch.flatten(self.avgpool(self.norm_set[1](x2).transpose(1,2)), 1) # B, 2C\n",
    "        x3_p = torch.flatten(self.avgpool(self.norm_set[2](x3).transpose(1,2)), 1) # B, 4C\n",
    "        x4_p = torch.flatten(self.avgpool(self.norm_set[3](x4).transpose(1,2)), 1) # B, 8C\n",
    "\n",
    "        '''\n",
    "        Transformer (Stage1)\n",
    "        '''\n",
    "        # transformer & reshape\n",
    "        X1 = self.transformer_set[0](x1) # B, N, C\n",
    "        X1 = X1.reshape(b_x1, c_x1, d_x1, h_x1, w_x1) # B, C, D, H, W\n",
    "\n",
    "        '''\n",
    "        Transformer (Stage2)\n",
    "        '''\n",
    "        # channel control\n",
    "        x1_p2 = self.control_cnl_set[1][0](x1_p).unsqueeze(1) # B, 1, 2C\n",
    "\n",
    "        # concat & transformer & reshape\n",
    "        X2 = self.transformer_set[1](torch.cat([x1_p2, x2],dim=1))[:, 1:, :] # B, (N/8)+1, 2C -> B, N/8, 2C\n",
    "        X2 = X2.reshape(b_x2, c_x2, d_x2, h_x2, w_x2) # B, 2C, D/2, H/2, W/2\n",
    "\n",
    "        '''\n",
    "        Transformer (Stage3)\n",
    "        '''\n",
    "        # channel control\n",
    "        x1_p3 = self.control_cnl_set[2][0](x1_p).unsqueeze(1) # B, 1, 4C\n",
    "        x2_p3 = self.control_cnl_set[2][1](x2_p).unsqueeze(1) # B, 1, 4C\n",
    "\n",
    "        # concat & transformer & reshape\n",
    "        X3 = self.transformer_set[2](torch.cat([x1_p3, x2_p3, x3],dim=1))[:, 2:, :] # B, (N/64)+2, 4C -> B, N/64, 4C\n",
    "        X3 = X3.reshape(b_x3, c_x3, d_x3, h_x3, w_x3) # B, 4C, D/4, H/4, W/4\n",
    "\n",
    "        '''\n",
    "        Transformer (Stage4)\n",
    "        '''\n",
    "        # channel control\n",
    "        x1_p4 = self.control_cnl_set[3][0](x1_p).unsqueeze(1) # B, 1, 8C\n",
    "        x2_p4 = self.control_cnl_set[3][1](x2_p).unsqueeze(1) # B, 1, 8C\n",
    "        x3_p4 = self.control_cnl_set[3][2](x3_p).unsqueeze(1) # B, 1, 8C\n",
    "\n",
    "        # concat & transformer & reshape\n",
    "        X4 = self.transformer_set[3](torch.cat([x1_p4, x2_p4, x3_p4, x4],dim=1))[:, 3:, :] # B, (N/512)+3, 8C -> B, N/512, 8C\n",
    "        X4 = X4.reshape(b_x4, c_x4, d_x4, h_x4, w_x4) # B, 8C, D/8, H/8, W/8\n",
    "\n",
    "        '''\n",
    "        ResBlock (Stage1)\n",
    "        '''\n",
    "        # resblock\n",
    "        y1 = self.resblock_set[0](X1) # B, C, D, H, W\n",
    "\n",
    "        '''\n",
    "        ResBlock (Stage2)\n",
    "        '''\n",
    "        # channel & resolution control\n",
    "        X1_2 = self.control_cnl_rst_set[1][0](X1) # B, 2C, D/2, H/2, W/2\n",
    "\n",
    "        # concat & resblock\n",
    "        y2 = self.resblock_set[1](torch.cat([X1_2, X2], dim=1)) # B, 2C, D/2, H/2, W/2\n",
    "\n",
    "        '''\n",
    "        ResBlock (Stage3)\n",
    "        '''\n",
    "        # channel & resolution control\n",
    "        X1_3 = self.control_cnl_rst_set[2][0](X1) # B, 4C, D/4, H/4, W/4\n",
    "        X2_3 = self.control_cnl_rst_set[2][1](X2) # B, 4C, D/4, H/4, W/4\n",
    "\n",
    "        # concat & resblock\n",
    "        y3 = self.resblock_set[2](torch.cat([X1_3, X2_3, X3], dim=1)) # B, 4C, D/4, H/4, W/4\n",
    "\n",
    "        '''\n",
    "        ResBlock (Stage4)\n",
    "        '''\n",
    "        # channel & resolution control\n",
    "        X1_4 = self.control_cnl_rst_set[3][0](X1) # B, 8C, D/8, H/8, W/8\n",
    "        X2_4 = self.control_cnl_rst_set[3][1](X2) # B, 8C, D/8, H/8, W/8\n",
    "        X3_4 = self.control_cnl_rst_set[3][2](X3) # B, 8C, D/8, H/8, W/8\n",
    "\n",
    "        # concat & resblock\n",
    "        y4 = self.resblock_set[3](torch.cat([X1_4, X2_4, X3_4, X4], dim=1)) # B, 8C, D/8, H/8, W/8\n",
    "\n",
    "        '''\n",
    "        Skip Connection\n",
    "        '''\n",
    "        y1 = y1 + save1\n",
    "        y2 = y2 + save2\n",
    "        y3 = y3 + save3\n",
    "        y4 = y4 + save4\n",
    "\n",
    "        return y1, y2, y3, y4\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "          Layer (type)            Output Shape         Param #     Tr. Param #\n",
      "===============================================================================\n",
      "           LayerNorm-1          [1, 32768, 32]              64              64\n",
      "   AdaptiveAvgPool1d-2              [1, 32, 1]               0               0\n",
      "           LayerNorm-3           [1, 4096, 64]             128             128\n",
      "           LayerNorm-4           [1, 512, 128]             256             256\n",
      "           LayerNorm-5            [1, 64, 256]             512             512\n",
      "      My_Transformer-6          [1, 32768, 32]       2,105,664       2,105,664\n",
      "              Linear-7                 [1, 64]           2,112           2,112\n",
      "      My_Transformer-8           [1, 4097, 64]         295,552         295,552\n",
      "              Linear-9                [1, 128]           4,224           4,224\n",
      "             Linear-10                [1, 128]           8,320           8,320\n",
      "     My_Transformer-11           [1, 514, 128]         165,056         165,056\n",
      "             Linear-12                [1, 256]           8,448           8,448\n",
      "             Linear-13                [1, 256]          16,640          16,640\n",
      "             Linear-14                [1, 256]          33,024          33,024\n",
      "     My_Transformer-15            [1, 67, 256]         530,688         530,688\n",
      "           ResBlock-16     [1, 32, 32, 32, 32]          56,544          56,544\n",
      "             Conv3d-17     [1, 64, 16, 16, 16]          16,448          16,448\n",
      "           ResBlock-18     [1, 64, 16, 16, 16]         340,416         340,416\n",
      "             Conv3d-19     [1, 64, 16, 16, 16]          16,448          16,448\n",
      "             Conv3d-20       [1, 128, 8, 8, 8]          65,664          65,664\n",
      "             Conv3d-21       [1, 128, 8, 8, 8]          65,664          65,664\n",
      "           ResBlock-22       [1, 128, 8, 8, 8]       1,819,520       1,819,520\n",
      "             Conv3d-23     [1, 64, 16, 16, 16]          16,448          16,448\n",
      "             Conv3d-24       [1, 128, 8, 8, 8]          65,664          65,664\n",
      "             Conv3d-25       [1, 256, 4, 4, 4]         262,400         262,400\n",
      "             Conv3d-26       [1, 128, 8, 8, 8]          65,664          65,664\n",
      "             Conv3d-27       [1, 256, 4, 4, 4]         262,400         262,400\n",
      "             Conv3d-28       [1, 256, 4, 4, 4]         262,400         262,400\n",
      "           ResBlock-29       [1, 256, 4, 4, 4]       9,111,296       9,111,296\n",
      "===============================================================================\n",
      "Total params: 15,597,664\n",
      "Trainable params: 15,597,664\n",
      "Non-trainable params: 0\n",
      "-------------------------------------------------------------------------------\n",
      "input: torch.Size([1, 32, 32, 32, 32]) torch.Size([1, 64, 16, 16, 16]) torch.Size([1, 128, 8, 8, 8]) torch.Size([1, 256, 4, 4, 4])\n",
      "output: torch.Size([1, 32, 32, 32, 32]) torch.Size([1, 64, 16, 16, 16]) torch.Size([1, 128, 8, 8, 8]) torch.Size([1, 256, 4, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "x1=torch.zeros(1,32,32,32,32)\n",
    "x2=torch.zeros(1,64,16,16,16)\n",
    "x3=torch.zeros(1,128,8,8,8)\n",
    "x4=torch.zeros(1,256,4,4,4)\n",
    "\n",
    "model=LSTB(proj_size=64)\n",
    "y1, y2, y3, y4=model(x1,x2,x3,x4)\n",
    "\n",
    "print(summary(model,x1,x2,x3,x4))\n",
    "print('input:',x1.shape,x2.shape,x3.shape,x4.shape)\n",
    "print('output:',y1.shape,y2.shape,y3.shape,y4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. SSTB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSTB(nn.Module):\n",
    "    def __init__(self,proj_size): \n",
    "        '''\n",
    "        Small Scale TIF Block\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        channels=[32, 64, 128, 256]\n",
    "        resolutions=[32*32*32, 16*16*16, 8*8*8, 4*4*4]\n",
    "\n",
    "        # 1. norm & pooling module\n",
    "        self.norm_set=nn.ModuleList([])\n",
    "        self.avgpool=nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        for crt_cnl in channels:\n",
    "            self.norm_set.append(nn.LayerNorm(crt_cnl))\n",
    "\n",
    "        # 2. channel control module\n",
    "        self.control_cnl_set=nn.ModuleList([])\n",
    "\n",
    "        for std_cnl in channels:\n",
    "            control_cnl=nn.ModuleList([])\n",
    "            for in_cnl in channels: \n",
    "                if in_cnl == std_cnl: # --> Identity\n",
    "                    control_cnl.append(nn.Identity())\n",
    "\n",
    "                else: # --> channel unify\n",
    "                    control_cnl.append(nn.Linear(in_features=in_cnl,out_features=std_cnl))\n",
    "            self.control_cnl_set.append(control_cnl)\n",
    "\n",
    "        # 3. Transformer module\n",
    "        self.transformer_set=nn.ModuleList([])\n",
    "        \n",
    "        for i,crt_cnl in enumerate(channels):\n",
    "            crt_rst = resolutions[i]+(3-i) # stage1 = (N+3)xC, stage2 = ((N/8)+2)x2C, stage3 = ((N/64)+1)x4C, stage4 = (N/512)x8C\n",
    "            self.transformer_set.append(\n",
    "                My_Transformer(input_size=crt_rst, proj_size=proj_size, dim=crt_cnl, depth=1, heads=4, dim_head=crt_cnl//4, mlp_dim=crt_cnl*2)\n",
    "            )\n",
    "\n",
    "        # 4. channel & resolution control module\n",
    "        self.control_cnl_rst_set=nn.ModuleList([])\n",
    "\n",
    "        for std_cnl in channels:\n",
    "            control_cnl_rst=nn.ModuleList([])\n",
    "            for in_cnl in channels: \n",
    "                if in_cnl == std_cnl: # --> Identity\n",
    "                    control_cnl_rst.append(nn.Identity())\n",
    "\n",
    "                elif in_cnl > std_cnl: # --> Upsampling\n",
    "                    itr=int(math.log2(in_cnl//std_cnl))\n",
    "                    self.up_layer=nn.Sequential()\n",
    "                    cnl=in_cnl\n",
    "                    for i in range(itr):\n",
    "                        self.up_layer.add_module(f'upsample_{i+1}',nn.ConvTranspose3d(in_channels=cnl,out_channels=cnl//2,kernel_size=2,stride=2))\n",
    "                        cnl=cnl//2\n",
    "                    control_cnl_rst.append(self.up_layer)\n",
    "\n",
    "                else: # --> downsampling\n",
    "                    itr=int(math.log2(std_cnl//in_cnl))\n",
    "                    self.down_layer=nn.Sequential()\n",
    "                    cnl=in_cnl\n",
    "                    for i in range(itr):\n",
    "                        self.down_layer.add_module(f'downsample_{i+1}',nn.Conv3d(in_channels=cnl,out_channels=cnl*2,kernel_size=2,stride=2))\n",
    "                        cnl=cnl*2\n",
    "                    control_cnl_rst.append(self.down_layer)\n",
    "\n",
    "            self.control_cnl_rst_set.append(control_cnl_rst)\n",
    "\n",
    "        # 5. ResBlock module\n",
    "        self.resblock_set=nn.ModuleList([])\n",
    "        self.resblock_set.append(ResBlock(in_dim=128, out_dim=32)) # 4C -> C\n",
    "        self.resblock_set.append(ResBlock(in_dim=192, out_dim=64)) # 6C -> 2C\n",
    "        self.resblock_set.append(ResBlock(in_dim=256, out_dim=128)) # 8C -> 4C\n",
    "        self.resblock_set.append(ResBlock(in_dim=256, out_dim=256)) # 8C -> 8C\n",
    "\n",
    "    def forward(self,x1,x2,x3,x4): \n",
    "        '''\n",
    "        x1: H x W x D x C\n",
    "        x2: H/2 x W/2 x D/2 x 2C\n",
    "        x3: H/4 x W/4 x D/4 x 4C\n",
    "        x4: H/8 x W/8 x D/8 x 8C\n",
    "        '''\n",
    "        save1, save2, save3, save4 = x1, x2, x3, x4\n",
    "        b_x1, c_x1, d_x1, h_x1, w_x1 = x1.shape[0], x1.shape[1], x1.shape[2], x1.shape[3], x1.shape[4]\n",
    "        b_x2, c_x2, d_x2, h_x2, w_x2 = x2.shape[0], x2.shape[1], x2.shape[2], x2.shape[3], x2.shape[4]\n",
    "        b_x3, c_x3, d_x3, h_x3, w_x3 = x3.shape[0], x3.shape[1], x3.shape[2], x3.shape[3], x3.shape[4]\n",
    "        b_x4, c_x4, d_x4, h_x4, w_x4 = x4.shape[0], x4.shape[1], x4.shape[2], x4.shape[3], x4.shape[4]\n",
    "\n",
    "        # reshape\n",
    "        x1 = x1.reshape(b_x1, c_x1, -1).permute(0, 2, 1) # B, N, C (N=HWD)\n",
    "        x2 = x2.reshape(b_x2, c_x2, -1).permute(0, 2, 1) # B, N/8, 2C \n",
    "        x3 = x3.reshape(b_x3, c_x3, -1).permute(0, 2, 1) # B, N/64, 4C \n",
    "        x4 = x4.reshape(b_x4, c_x4, -1).permute(0, 2, 1) # B, N/512, 8C \n",
    "\n",
    "        # norm & pool\n",
    "        x1_p = torch.flatten(self.avgpool(self.norm_set[0](x1).transpose(1,2)), 1) # B, C\n",
    "        x2_p = torch.flatten(self.avgpool(self.norm_set[1](x2).transpose(1,2)), 1) # B, 2C\n",
    "        x3_p = torch.flatten(self.avgpool(self.norm_set[2](x3).transpose(1,2)), 1) # B, 4C\n",
    "        x4_p = torch.flatten(self.avgpool(self.norm_set[3](x4).transpose(1,2)), 1) # B, 8C\n",
    "\n",
    "        '''\n",
    "        Transformer (Stage1)\n",
    "        '''\n",
    "        # channel control\n",
    "        x2_p1 = self.control_cnl_set[0][1](x2_p).unsqueeze(1) # B, 1, C\n",
    "        x3_p1 = self.control_cnl_set[0][2](x3_p).unsqueeze(1) # B, 1, C\n",
    "        x4_p1 = self.control_cnl_set[0][3](x4_p).unsqueeze(1) # B, 1, C\n",
    "\n",
    "        # concat & transformer & reshape\n",
    "        X1 = self.transformer_set[0](torch.cat([x2_p1, x3_p1, x4_p1, x1],dim=1))[:, 3:, :] # B, N+3, C -> B, N, C\n",
    "        X1 = X1.reshape(b_x1, c_x1, d_x1, h_x1, w_x1) # B, C, D, H, W\n",
    "\n",
    "        '''\n",
    "        Transformer (Stage2)\n",
    "        '''\n",
    "        # channel control\n",
    "        x3_p2 = self.control_cnl_set[1][2](x3_p).unsqueeze(1) # B, 1, 2C\n",
    "        x4_p2 = self.control_cnl_set[1][3](x4_p).unsqueeze(1) # B, 1, 2C\n",
    "\n",
    "        # concat & transformer & reshape\n",
    "        X2 = self.transformer_set[1](torch.cat([x3_p2, x4_p2, x2],dim=1))[:, 2:, :] # B, (N/8)+2, 2C -> B, N/8, 2C\n",
    "        X2 = X2.reshape(b_x2, c_x2, d_x2, h_x2, w_x2) # B, 2C, D/2, H/2, W/2\n",
    "\n",
    "        '''\n",
    "        Transformer (Stage3)\n",
    "        '''\n",
    "        # channel control\n",
    "        x4_p3 = self.control_cnl_set[2][3](x4_p).unsqueeze(1) # B, 1, 4C\n",
    "\n",
    "        # concat & transformer & reshape\n",
    "        X3 = self.transformer_set[2](torch.cat([x4_p3, x3],dim=1))[:, 1:, :] # B, (N/64)+1, 4C -> B, N/64, 4C\n",
    "        X3 = X3.reshape(b_x3, c_x3, d_x3, h_x3, w_x3) # B, 4C, D/4, H/4, W/4\n",
    "\n",
    "        '''\n",
    "        Transformer (Stage4)\n",
    "        '''\n",
    "        # transformer & reshape\n",
    "        X4 = self.transformer_set[3](x4) # B, N/512, 8C\n",
    "        X4 = X4.reshape(b_x4, c_x4, d_x4, h_x4, w_x4) # B, 8C, D/8, H/8, W/8\n",
    "\n",
    "        '''\n",
    "        ResBlock (Stage1)\n",
    "        '''\n",
    "        # channel & resolution control\n",
    "        X2_1 = self.control_cnl_rst_set[0][1](X2) # B, C, D, H, W\n",
    "        X3_1 = self.control_cnl_rst_set[0][2](X3) # B, C, D, H, W\n",
    "        X4_1 = self.control_cnl_rst_set[0][3](X4) # B, C, D, H, W\n",
    "\n",
    "        # concat & resblock\n",
    "        y1 = self.resblock_set[0](torch.cat([X2_1, X3_1, X4_1, X1], dim=1)) # B, C, D, H, W\n",
    "\n",
    "        '''\n",
    "        ResBlock (Stage2)\n",
    "        '''\n",
    "        # channel & resolution control\n",
    "        X3_2 = self.control_cnl_rst_set[1][2](X3) # B, 2C, D/2, H/2, W/2\n",
    "        X4_2 = self.control_cnl_rst_set[1][3](X4) # B, 2C, D/2, H/2, W/2\n",
    "\n",
    "        # concat & resblock\n",
    "        y2 = self.resblock_set[1](torch.cat([X3_2, X4_2, X2], dim=1)) # B, 2C, D/2, H/2, W/2\n",
    "\n",
    "        '''\n",
    "        ResBlock (Stage3)\n",
    "        '''\n",
    "        # channel & resolution control\n",
    "        X4_3 = self.control_cnl_rst_set[2][3](X4) # B, 4C, D/4, H/4, W/4\n",
    "\n",
    "        # concat & resblock\n",
    "        y3 = self.resblock_set[2](torch.cat([X4_3, X3], dim=1)) # B, 4C, D/4, H/4, W/4\n",
    "\n",
    "        '''\n",
    "        ResBlock (Stage4)\n",
    "        '''\n",
    "        # resblock\n",
    "        y4 = self.resblock_set[3](X4) # B, 8C, D/8, H/8, W/8\n",
    "\n",
    "        '''\n",
    "        Skip Connection\n",
    "        '''\n",
    "        y1 = y1 + save1\n",
    "        y2 = y2 + save2\n",
    "        y3 = y3 + save3\n",
    "        y4 = y4 + save4\n",
    "\n",
    "        return y1, y2, y3, y4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "          Layer (type)            Output Shape         Param #     Tr. Param #\n",
      "===============================================================================\n",
      "           LayerNorm-1          [1, 32768, 32]              64              64\n",
      "   AdaptiveAvgPool1d-2              [1, 32, 1]               0               0\n",
      "           LayerNorm-3           [1, 4096, 64]             128             128\n",
      "           LayerNorm-4           [1, 512, 128]             256             256\n",
      "           LayerNorm-5            [1, 64, 256]             512             512\n",
      "              Linear-6                 [1, 32]           2,080           2,080\n",
      "              Linear-7                 [1, 32]           4,128           4,128\n",
      "              Linear-8                 [1, 32]           8,224           8,224\n",
      "      My_Transformer-9          [1, 32771, 32]       2,105,856       2,105,856\n",
      "             Linear-10                 [1, 64]           8,256           8,256\n",
      "             Linear-11                 [1, 64]          16,448          16,448\n",
      "     My_Transformer-12           [1, 4098, 64]         295,616         295,616\n",
      "             Linear-13                [1, 128]          32,896          32,896\n",
      "     My_Transformer-14           [1, 513, 128]         164,992         164,992\n",
      "     My_Transformer-15            [1, 64, 256]         530,496         530,496\n",
      "    ConvTranspose3d-16     [1, 32, 32, 32, 32]          16,416          16,416\n",
      "    ConvTranspose3d-17     [1, 64, 16, 16, 16]          65,600          65,600\n",
      "    ConvTranspose3d-18     [1, 32, 32, 32, 32]          16,416          16,416\n",
      "    ConvTranspose3d-19       [1, 128, 8, 8, 8]         262,272         262,272\n",
      "    ConvTranspose3d-20     [1, 64, 16, 16, 16]          65,600          65,600\n",
      "    ConvTranspose3d-21     [1, 32, 32, 32, 32]          16,416          16,416\n",
      "           ResBlock-22     [1, 32, 32, 32, 32]         142,560         142,560\n",
      "    ConvTranspose3d-23     [1, 64, 16, 16, 16]          65,600          65,600\n",
      "    ConvTranspose3d-24       [1, 128, 8, 8, 8]         262,272         262,272\n",
      "    ConvTranspose3d-25     [1, 64, 16, 16, 16]          65,600          65,600\n",
      "           ResBlock-26     [1, 64, 16, 16, 16]         455,104         455,104\n",
      "    ConvTranspose3d-27       [1, 128, 8, 8, 8]         262,272         262,272\n",
      "           ResBlock-28       [1, 128, 8, 8, 8]       1,360,768       1,360,768\n",
      "           ResBlock-29       [1, 256, 4, 4, 4]       3,606,272       3,606,272\n",
      "===============================================================================\n",
      "Total params: 9,833,120\n",
      "Trainable params: 9,833,120\n",
      "Non-trainable params: 0\n",
      "-------------------------------------------------------------------------------\n",
      "input: torch.Size([1, 32, 32, 32, 32]) torch.Size([1, 64, 16, 16, 16]) torch.Size([1, 128, 8, 8, 8]) torch.Size([1, 256, 4, 4, 4])\n",
      "output: torch.Size([1, 32, 32, 32, 32]) torch.Size([1, 64, 16, 16, 16]) torch.Size([1, 128, 8, 8, 8]) torch.Size([1, 256, 4, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "x1=torch.zeros(1,32,32,32,32)\n",
    "x2=torch.zeros(1,64,16,16,16)\n",
    "x3=torch.zeros(1,128,8,8,8)\n",
    "x4=torch.zeros(1,256,4,4,4)\n",
    "\n",
    "model=SSTB(proj_size=64)\n",
    "y1, y2, y3, y4=model(x1,x2,x3,x4)\n",
    "\n",
    "print(summary(model,x1,x2,x3,x4))\n",
    "print('input:',x1.shape,x2.shape,x3.shape,x4.shape)\n",
    "print('output:',y1.shape,y2.shape,y3.shape,y4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. ASTB with ESA (Spatial->Channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASTB_ESA(nn.Module):\n",
    "    def __init__(self,proj_size): \n",
    "        '''\n",
    "        All Scale TIF Block\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        channels=[32, 64, 128, 256]\n",
    "        resolutions=[32*32*32, 16*16*16, 8*8*8, 4*4*4]\n",
    "\n",
    "        # 1. norm & pooling module\n",
    "        self.norm_set=nn.ModuleList([])\n",
    "        self.avgpool=nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        for crt_cnl in channels:\n",
    "            self.norm_set.append(nn.LayerNorm(crt_cnl))\n",
    "\n",
    "        # 2. channel control module\n",
    "        self.control_cnl_set=nn.ModuleList([])\n",
    "\n",
    "        for std_cnl in channels:\n",
    "            control_cnl=nn.ModuleList([])\n",
    "            for in_cnl in channels: \n",
    "                if in_cnl == std_cnl: # --> Identity\n",
    "                    control_cnl.append(nn.Identity())\n",
    "\n",
    "                else: # --> channel unify\n",
    "                    control_cnl.append(nn.Linear(in_features=in_cnl,out_features=std_cnl))\n",
    "            self.control_cnl_set.append(control_cnl)\n",
    "\n",
    "        # 3. Transformer module\n",
    "        self.transformer_set=nn.ModuleList([])\n",
    "        \n",
    "        for i,crt_cnl in enumerate(channels):\n",
    "            crt_rst = resolutions[i]+3\n",
    "            self.transformer_set.append(\n",
    "                My_EPA2(input_size=crt_rst, hidden_size=crt_cnl, proj_size=proj_size)\n",
    "            )\n",
    "\n",
    "        # 4. channel & resolution control module\n",
    "        self.control_cnl_rst_set=nn.ModuleList([])\n",
    "\n",
    "        for std_cnl in channels:\n",
    "            control_cnl_rst=nn.ModuleList([])\n",
    "            for in_cnl in channels: \n",
    "                if in_cnl == std_cnl: # --> Identity\n",
    "                    control_cnl_rst.append(nn.Identity())\n",
    "\n",
    "                elif in_cnl > std_cnl: # --> Upsampling\n",
    "                    itr=int(math.log2(in_cnl//std_cnl))\n",
    "                    self.up_layer=nn.Sequential()\n",
    "                    cnl=in_cnl\n",
    "                    for i in range(itr):\n",
    "                        self.up_layer.add_module(f'upsample_{i+1}',nn.ConvTranspose3d(in_channels=cnl,out_channels=cnl//2,kernel_size=2,stride=2))\n",
    "                        cnl=cnl//2\n",
    "                    control_cnl_rst.append(self.up_layer)\n",
    "\n",
    "                else: # --> downsampling\n",
    "                    itr=int(math.log2(std_cnl//in_cnl))\n",
    "                    self.down_layer=nn.Sequential()\n",
    "                    cnl=in_cnl\n",
    "                    for i in range(itr):\n",
    "                        self.down_layer.add_module(f'downsample_{i+1}',nn.Conv3d(in_channels=cnl,out_channels=cnl*2,kernel_size=2,stride=2))\n",
    "                        cnl=cnl*2\n",
    "                    control_cnl_rst.append(self.down_layer)\n",
    "\n",
    "            self.control_cnl_rst_set.append(control_cnl_rst)\n",
    "\n",
    "        # 5. ResBlock module\n",
    "        self.resblock_set=nn.ModuleList([])\n",
    "\n",
    "        for crt_cnl in channels:\n",
    "            self.resblock_set.append(ResBlock(in_dim=crt_cnl*4, out_dim=crt_cnl))\n",
    "\n",
    "    def forward(self,x1,x2,x3,x4): \n",
    "        '''\n",
    "        x1: H x W x D x C\n",
    "        x2: H/2 x W/2 x D/2 x 2C\n",
    "        x3: H/4 x W/4 x D/4 x 4C\n",
    "        x4: H/8 x W/8 x D/8 x 8C\n",
    "        '''\n",
    "        save1, save2, save3, save4 = x1, x2, x3, x4\n",
    "        b_x1, c_x1, d_x1, h_x1, w_x1 = x1.shape[0], x1.shape[1], x1.shape[2], x1.shape[3], x1.shape[4]\n",
    "        b_x2, c_x2, d_x2, h_x2, w_x2 = x2.shape[0], x2.shape[1], x2.shape[2], x2.shape[3], x2.shape[4]\n",
    "        b_x3, c_x3, d_x3, h_x3, w_x3 = x3.shape[0], x3.shape[1], x3.shape[2], x3.shape[3], x3.shape[4]\n",
    "        b_x4, c_x4, d_x4, h_x4, w_x4 = x4.shape[0], x4.shape[1], x4.shape[2], x4.shape[3], x4.shape[4]\n",
    "\n",
    "        # reshape\n",
    "        x1 = x1.reshape(b_x1, c_x1, -1).permute(0, 2, 1) # B, N, C (N=HWD)\n",
    "        x2 = x2.reshape(b_x2, c_x2, -1).permute(0, 2, 1) # B, N/8, 2C \n",
    "        x3 = x3.reshape(b_x3, c_x3, -1).permute(0, 2, 1) # B, N/64, 4C \n",
    "        x4 = x4.reshape(b_x4, c_x4, -1).permute(0, 2, 1) # B, N/512, 8C \n",
    "\n",
    "        # norm & pool\n",
    "        x1_p = torch.flatten(self.avgpool(self.norm_set[0](x1).transpose(1,2)), 1) # B, C\n",
    "        x2_p = torch.flatten(self.avgpool(self.norm_set[1](x2).transpose(1,2)), 1) # B, 2C\n",
    "        x3_p = torch.flatten(self.avgpool(self.norm_set[2](x3).transpose(1,2)), 1) # B, 4C\n",
    "        x4_p = torch.flatten(self.avgpool(self.norm_set[3](x4).transpose(1,2)), 1) # B, 8C\n",
    "\n",
    "        '''\n",
    "        Transformer (Stage1)\n",
    "        '''\n",
    "        # channel control\n",
    "        x2_p1 = self.control_cnl_set[0][1](x2_p).unsqueeze(1) # B, 1, C\n",
    "        x3_p1 = self.control_cnl_set[0][2](x3_p).unsqueeze(1) # B, 1, C\n",
    "        x4_p1 = self.control_cnl_set[0][3](x4_p).unsqueeze(1) # B, 1, C\n",
    "\n",
    "        # concat & transformer & reshape\n",
    "        X1 = self.transformer_set[0](torch.cat([x2_p1, x3_p1, x4_p1, x1],dim=1))[:, 3:, :] # B, N+3, C -> B, N, C\n",
    "        X1 = X1.reshape(b_x1, c_x1, d_x1, h_x1, w_x1) # B, C, D, H, W\n",
    "\n",
    "        '''\n",
    "        Transformer (Stage2)\n",
    "        '''\n",
    "        # channel control\n",
    "        x1_p2 = self.control_cnl_set[1][0](x1_p).unsqueeze(1) # B, 1, 2C\n",
    "        x3_p2 = self.control_cnl_set[1][2](x3_p).unsqueeze(1) # B, 1, 2C\n",
    "        x4_p2 = self.control_cnl_set[1][3](x4_p).unsqueeze(1) # B, 1, 2C\n",
    "\n",
    "        # concat & transformer & reshape\n",
    "        X2 = self.transformer_set[1](torch.cat([x1_p2, x3_p2, x4_p2, x2],dim=1))[:, 3:, :] # B, (N/8)+3, 2C -> B, N/8, 2C\n",
    "        X2 = X2.reshape(b_x2, c_x2, d_x2, h_x2, w_x2) # B, 2C, D/2, H/2, W/2\n",
    "\n",
    "        '''\n",
    "        Transformer (Stage3)\n",
    "        '''\n",
    "        # channel control\n",
    "        x1_p3 = self.control_cnl_set[2][0](x1_p).unsqueeze(1) # B, 1, 4C\n",
    "        x2_p3 = self.control_cnl_set[2][1](x2_p).unsqueeze(1) # B, 1, 4C\n",
    "        x4_p3 = self.control_cnl_set[2][3](x4_p).unsqueeze(1) # B, 1, 4C\n",
    "\n",
    "        # concat & transformer & reshape\n",
    "        X3 = self.transformer_set[2](torch.cat([x1_p3, x2_p3, x4_p3, x3],dim=1))[:, 3:, :] # B, (N/64)+3, 4C -> B, N/64, 4C\n",
    "        X3 = X3.reshape(b_x3, c_x3, d_x3, h_x3, w_x3) # B, 4C, D/4, H/4, W/4\n",
    "\n",
    "        '''\n",
    "        Transformer (Stage4)\n",
    "        '''\n",
    "        # channel control\n",
    "        x1_p4 = self.control_cnl_set[3][0](x1_p).unsqueeze(1) # B, 1, 8C\n",
    "        x2_p4 = self.control_cnl_set[3][1](x2_p).unsqueeze(1) # B, 1, 8C\n",
    "        x3_p4 = self.control_cnl_set[3][2](x3_p).unsqueeze(1) # B, 1, 8C\n",
    "\n",
    "        # concat & transformer & reshape\n",
    "        X4 = self.transformer_set[3](torch.cat([x1_p4, x2_p4, x3_p4, x4],dim=1))[:, 3:, :] # B, (N/512)+3, 8C -> B, N/512, 8C\n",
    "        X4 = X4.reshape(b_x4, c_x4, d_x4, h_x4, w_x4) # B, 8C, D/8, H/8, W/8\n",
    "\n",
    "        '''\n",
    "        ResBlock (Stage1)\n",
    "        '''\n",
    "        # channel & resolution control\n",
    "        X2_1 = self.control_cnl_rst_set[0][1](X2) # B, C, D, H, W\n",
    "        X3_1 = self.control_cnl_rst_set[0][2](X3) # B, C, D, H, W\n",
    "        X4_1 = self.control_cnl_rst_set[0][3](X4) # B, C, D, H, W\n",
    "\n",
    "        # concat & resblock\n",
    "        y1 = self.resblock_set[0](torch.cat([X2_1, X3_1, X4_1, X1], dim=1)) # B, C, D, H, W\n",
    "\n",
    "        '''\n",
    "        ResBlock (Stage2)\n",
    "        '''\n",
    "        # channel & resolution control\n",
    "        X1_2 = self.control_cnl_rst_set[1][0](X1) # B, 2C, D/2, H/2, W/2\n",
    "        X3_2 = self.control_cnl_rst_set[1][2](X3) # B, 2C, D/2, H/2, W/2\n",
    "        X4_2 = self.control_cnl_rst_set[1][3](X4) # B, 2C, D/2, H/2, W/2\n",
    "\n",
    "        # concat & resblock\n",
    "        y2 = self.resblock_set[1](torch.cat([X1_2, X3_2, X4_2, X2], dim=1)) # B, 2C, D/2, H/2, W/2\n",
    "\n",
    "        '''\n",
    "        ResBlock (Stage3)\n",
    "        '''\n",
    "        # channel & resolution control\n",
    "        X1_3 = self.control_cnl_rst_set[2][0](X1) # B, 4C, D/4, H/4, W/4\n",
    "        X2_3 = self.control_cnl_rst_set[2][1](X2) # B, 4C, D/4, H/4, W/4\n",
    "        X4_3 = self.control_cnl_rst_set[2][3](X4) # B, 4C, D/4, H/4, W/4\n",
    "\n",
    "        # concat & resblock\n",
    "        y3 = self.resblock_set[2](torch.cat([X1_3, X2_3, X4_3, X3], dim=1)) # B, 4C, D/4, H/4, W/4\n",
    "\n",
    "        '''\n",
    "        ResBlock (Stage4)\n",
    "        '''\n",
    "        # channel & resolution control\n",
    "        X1_4 = self.control_cnl_rst_set[3][0](X1) # B, 8C, D/8, H/8, W/8\n",
    "        X2_4 = self.control_cnl_rst_set[3][1](X2) # B, 8C, D/8, H/8, W/8\n",
    "        X3_4 = self.control_cnl_rst_set[3][2](X3) # B, 8C, D/8, H/8, W/8\n",
    "\n",
    "        # concat & resblock\n",
    "        y4 = self.resblock_set[3](torch.cat([X1_4, X2_4, X3_4, X4], dim=1)) # B, 8C, D/8, H/8, W/8\n",
    "\n",
    "        '''\n",
    "        Skip Connection\n",
    "        '''\n",
    "        y1 = y1 + save1\n",
    "        y2 = y2 + save2\n",
    "        y3 = y3 + save3\n",
    "        y4 = y4 + save4\n",
    "\n",
    "        return y1, y2, y3, y4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "          Layer (type)            Output Shape         Param #     Tr. Param #\n",
      "===============================================================================\n",
      "           LayerNorm-1          [1, 32768, 32]              64              64\n",
      "   AdaptiveAvgPool1d-2              [1, 32, 1]               0               0\n",
      "           LayerNorm-3           [1, 4096, 64]             128             128\n",
      "           LayerNorm-4           [1, 512, 128]             256             256\n",
      "           LayerNorm-5            [1, 64, 256]             512             512\n",
      "              Linear-6                 [1, 32]           2,080           2,080\n",
      "              Linear-7                 [1, 32]           4,128           4,128\n",
      "              Linear-8                 [1, 32]           8,224           8,224\n",
      "             My_EPA2-9          [1, 32771, 32]       2,103,560       2,103,560\n",
      "             Linear-10                 [1, 64]           2,112           2,112\n",
      "             Linear-11                 [1, 64]           8,256           8,256\n",
      "             Linear-12                 [1, 64]          16,448          16,448\n",
      "            My_EPA2-13           [1, 4099, 64]         286,984         286,984\n",
      "             Linear-14                [1, 128]           4,224           4,224\n",
      "             Linear-15                [1, 128]           8,320           8,320\n",
      "             Linear-16                [1, 128]          32,896          32,896\n",
      "            My_EPA2-17           [1, 515, 128]         131,336         131,336\n",
      "             Linear-18                [1, 256]           8,448           8,448\n",
      "             Linear-19                [1, 256]          16,640          16,640\n",
      "             Linear-20                [1, 256]          33,024          33,024\n",
      "            My_EPA2-21            [1, 67, 256]         397,576         397,576\n",
      "    ConvTranspose3d-22     [1, 32, 32, 32, 32]          16,416          16,416\n",
      "    ConvTranspose3d-23     [1, 64, 16, 16, 16]          65,600          65,600\n",
      "    ConvTranspose3d-24     [1, 32, 32, 32, 32]          16,416          16,416\n",
      "    ConvTranspose3d-25       [1, 128, 8, 8, 8]         262,272         262,272\n",
      "    ConvTranspose3d-26     [1, 64, 16, 16, 16]          65,600          65,600\n",
      "    ConvTranspose3d-27     [1, 32, 32, 32, 32]          16,416          16,416\n",
      "           ResBlock-28     [1, 32, 32, 32, 32]         142,560         142,560\n",
      "             Conv3d-29     [1, 64, 16, 16, 16]          16,448          16,448\n",
      "    ConvTranspose3d-30     [1, 64, 16, 16, 16]          65,600          65,600\n",
      "    ConvTranspose3d-31       [1, 128, 8, 8, 8]         262,272         262,272\n",
      "    ConvTranspose3d-32     [1, 64, 16, 16, 16]          65,600          65,600\n",
      "           ResBlock-33     [1, 64, 16, 16, 16]         569,792         569,792\n",
      "             Conv3d-34     [1, 64, 16, 16, 16]          16,448          16,448\n",
      "             Conv3d-35       [1, 128, 8, 8, 8]          65,664          65,664\n",
      "             Conv3d-36       [1, 128, 8, 8, 8]          65,664          65,664\n",
      "    ConvTranspose3d-37       [1, 128, 8, 8, 8]         262,272         262,272\n",
      "           ResBlock-38       [1, 128, 8, 8, 8]       2,278,272       2,278,272\n",
      "             Conv3d-39     [1, 64, 16, 16, 16]          16,448          16,448\n",
      "             Conv3d-40       [1, 128, 8, 8, 8]          65,664          65,664\n",
      "             Conv3d-41       [1, 256, 4, 4, 4]         262,400         262,400\n",
      "             Conv3d-42       [1, 128, 8, 8, 8]          65,664          65,664\n",
      "             Conv3d-43       [1, 256, 4, 4, 4]         262,400         262,400\n",
      "             Conv3d-44       [1, 256, 4, 4, 4]         262,400         262,400\n",
      "           ResBlock-45       [1, 256, 4, 4, 4]       9,111,296       9,111,296\n",
      "===============================================================================\n",
      "Total params: 17,364,800\n",
      "Trainable params: 17,364,800\n",
      "Non-trainable params: 0\n",
      "-------------------------------------------------------------------------------\n",
      "input: torch.Size([1, 32, 32, 32, 32]) torch.Size([1, 64, 16, 16, 16]) torch.Size([1, 128, 8, 8, 8]) torch.Size([1, 256, 4, 4, 4])\n",
      "output: torch.Size([1, 32, 32, 32, 32]) torch.Size([1, 64, 16, 16, 16]) torch.Size([1, 128, 8, 8, 8]) torch.Size([1, 256, 4, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "x1=torch.zeros(1,32,32,32,32)\n",
    "x2=torch.zeros(1,64,16,16,16)\n",
    "x3=torch.zeros(1,128,8,8,8)\n",
    "x4=torch.zeros(1,256,4,4,4)\n",
    "\n",
    "model=ASTB_ESA(proj_size=64)\n",
    "y1, y2, y3, y4=model(x1,x2,x3,x4)\n",
    "\n",
    "print(summary(model,x1,x2,x3,x4))\n",
    "print('input:',x1.shape,x2.shape,x3.shape,x4.shape)\n",
    "print('output:',y1.shape,y2.shape,y3.shape,y4.shape)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
