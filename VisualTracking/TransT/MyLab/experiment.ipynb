{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "- ltr/run_training.py를 이용해서 학습 가능\n",
    "- ltr/admin/local.py에 dataset 경로를 기록해야 됨 (train 경로를 가리키게)\n",
    "- ltr/train_settings/transt/transt.py를 수정해서 커스터마이징 (배치 사이즈, 데이터셋 등)\n",
    "- 원래는 Lasot, Got10k, TrackingNet, MSCOCO 데이터로 학습 -> Got10k로만 학습\n",
    "- 비디오 수가 500개인 GOT-10k-mini 데이터셋을 제작함 (ltr/dataset/got10k.py에 코드를 추가함)\n",
    "- 학습된 모델을 불러 오려면 checkpoint의 마지막 pth.tar파일을 이용\n",
    "\n",
    "ex) python run_training.py transt transt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  transt  transt\n",
      "=========================================\n",
      "DataLoader ok!!\n",
      "=========================================\n",
      "/home/ahnsunghyun/pytorch/VisualTracking/TransT/torchvision_edit/models_e/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ahnsunghyun/pytorch/VisualTracking/TransT/torchvision_edit/models_e/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "=========================================\n",
      "model load ok!!\n",
      "=========================================\n",
      "number of params: 42129966\n",
      "=========================================\n",
      "Training Log PATH <tensorboard>\n",
      "/media/ahnsunghyun/HDD/checkpoints/tensorboard/ltr/transt/transt\n",
      "=========================================\n",
      "=========================================\n",
      "Training Start!!\n",
      "=========================================\n",
      "Training crashed at epoch 2\n",
      "Traceback for the error!\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/base_trainer.py\", line 70, in train\n",
      "    self.train_epoch()\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/ltr_trainer.py\", line 87, in train_epoch\n",
      "    self.cycle_dataset(loader)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/ltr_trainer.py\", line 68, in cycle_dataset\n",
      "    loss, stats = self.actor(data)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/actors/tracking.py\", line 41, in __call__\n",
      "    loss_dict = self.objective(outputs, targets)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/tracking/transt.py\", line 211, in forward\n",
      "    losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes_pos))\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/tracking/transt.py\", line 187, in get_loss\n",
      "    return loss_map[loss](outputs, targets, indices, num_boxes)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/tracking/transt.py\", line 133, in loss_labels\n",
      "    loss_ce = nn.CrossEntropyLoss()(src_logits.transpose(1, 2), target_classes, reduction='none')\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "TypeError: forward() got an unexpected keyword argument 'reduction'\n",
      "\n",
      "Restarting training from last epoch ...\n",
      "Training crashed at epoch 2\n",
      "Traceback for the error!\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/base_trainer.py\", line 70, in train\n",
      "    self.train_epoch()\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/ltr_trainer.py\", line 87, in train_epoch\n",
      "    self.cycle_dataset(loader)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/ltr_trainer.py\", line 68, in cycle_dataset\n",
      "    loss, stats = self.actor(data)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/actors/tracking.py\", line 41, in __call__\n",
      "    loss_dict = self.objective(outputs, targets)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/tracking/transt.py\", line 211, in forward\n",
      "    losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes_pos))\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/tracking/transt.py\", line 187, in get_loss\n",
      "    return loss_map[loss](outputs, targets, indices, num_boxes)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/tracking/transt.py\", line 133, in loss_labels\n",
      "    loss_ce = nn.CrossEntropyLoss()(src_logits.transpose(1, 2), target_classes, reduction='none')\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "TypeError: forward() got an unexpected keyword argument 'reduction'\n",
      "\n",
      "Restarting training from last epoch ...\n",
      "Training crashed at epoch 2\n",
      "Traceback for the error!\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/base_trainer.py\", line 70, in train\n",
      "    self.train_epoch()\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/ltr_trainer.py\", line 87, in train_epoch\n",
      "    self.cycle_dataset(loader)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/ltr_trainer.py\", line 68, in cycle_dataset\n",
      "    loss, stats = self.actor(data)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/actors/tracking.py\", line 41, in __call__\n",
      "    loss_dict = self.objective(outputs, targets)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/tracking/transt.py\", line 211, in forward\n",
      "    losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes_pos))\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/tracking/transt.py\", line 187, in get_loss\n",
      "    return loss_map[loss](outputs, targets, indices, num_boxes)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/tracking/transt.py\", line 133, in loss_labels\n",
      "    loss_ce = nn.CrossEntropyLoss()(src_logits.transpose(1, 2), target_classes, reduction='none')\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "TypeError: forward() got an unexpected keyword argument 'reduction'\n",
      "\n",
      "Restarting training from last epoch ...\n",
      "Training crashed at epoch 2\n",
      "Traceback for the error!\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/base_trainer.py\", line 70, in train\n",
      "    self.train_epoch()\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/ltr_trainer.py\", line 87, in train_epoch\n",
      "    self.cycle_dataset(loader)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/ltr_trainer.py\", line 68, in cycle_dataset\n",
      "    loss, stats = self.actor(data)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/actors/tracking.py\", line 41, in __call__\n",
      "    loss_dict = self.objective(outputs, targets)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/tracking/transt.py\", line 211, in forward\n",
      "    losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes_pos))\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/tracking/transt.py\", line 187, in get_loss\n",
      "    return loss_map[loss](outputs, targets, indices, num_boxes)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/tracking/transt.py\", line 133, in loss_labels\n",
      "    loss_ce = nn.CrossEntropyLoss()(src_logits.transpose(1, 2), target_classes, reduction='none')\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "TypeError: forward() got an unexpected keyword argument 'reduction'\n",
      "\n",
      "Restarting training from last epoch ...\n",
      "Training crashed at epoch 2\n",
      "Traceback for the error!\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/base_trainer.py\", line 70, in train\n",
      "    self.train_epoch()\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/ltr_trainer.py\", line 87, in train_epoch\n",
      "    self.cycle_dataset(loader)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/ltr_trainer.py\", line 68, in cycle_dataset\n",
      "    loss, stats = self.actor(data)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/actors/tracking.py\", line 41, in __call__\n",
      "    loss_dict = self.objective(outputs, targets)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/tracking/transt.py\", line 211, in forward\n",
      "    losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes_pos))\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/tracking/transt.py\", line 187, in get_loss\n",
      "    return loss_map[loss](outputs, targets, indices, num_boxes)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/tracking/transt.py\", line 133, in loss_labels\n",
      "    loss_ce = nn.CrossEntropyLoss()(src_logits.transpose(1, 2), target_classes, reduction='none')\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "TypeError: forward() got an unexpected keyword argument 'reduction'\n",
      "\n",
      "Restarting training from last epoch ...\n",
      "Training crashed at epoch 2\n",
      "Traceback for the error!\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/base_trainer.py\", line 70, in train\n",
      "    self.train_epoch()\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/ltr_trainer.py\", line 87, in train_epoch\n",
      "    self.cycle_dataset(loader)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/ltr_trainer.py\", line 68, in cycle_dataset\n",
      "    loss, stats = self.actor(data)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/actors/tracking.py\", line 41, in __call__\n",
      "    loss_dict = self.objective(outputs, targets)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/tracking/transt.py\", line 211, in forward\n",
      "    losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes_pos))\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/tracking/transt.py\", line 187, in get_loss\n",
      "    return loss_map[loss](outputs, targets, indices, num_boxes)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/tracking/transt.py\", line 133, in loss_labels\n",
      "    loss_ce = nn.CrossEntropyLoss()(src_logits.transpose(1, 2), target_classes, reduction='none')\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "TypeError: forward() got an unexpected keyword argument 'reduction'\n",
      "\n",
      "Restarting training from last epoch ...\n",
      "Training crashed at epoch 2\n",
      "Traceback for the error!\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/base_trainer.py\", line 70, in train\n",
      "    self.train_epoch()\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/ltr_trainer.py\", line 87, in train_epoch\n",
      "    self.cycle_dataset(loader)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/ltr_trainer.py\", line 68, in cycle_dataset\n",
      "    loss, stats = self.actor(data)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/actors/tracking.py\", line 41, in __call__\n",
      "    loss_dict = self.objective(outputs, targets)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/tracking/transt.py\", line 211, in forward\n",
      "    losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes_pos))\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/tracking/transt.py\", line 187, in get_loss\n",
      "    return loss_map[loss](outputs, targets, indices, num_boxes)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/tracking/transt.py\", line 133, in loss_labels\n",
      "    loss_ce = nn.CrossEntropyLoss()(src_logits.transpose(1, 2), target_classes, reduction='none')\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "TypeError: forward() got an unexpected keyword argument 'reduction'\n",
      "\n",
      "Restarting training from last epoch ...\n",
      "Training crashed at epoch 2\n",
      "Traceback for the error!\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/base_trainer.py\", line 70, in train\n",
      "    self.train_epoch()\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/ltr_trainer.py\", line 87, in train_epoch\n",
      "    self.cycle_dataset(loader)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/ltr_trainer.py\", line 68, in cycle_dataset\n",
      "    loss, stats = self.actor(data)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/actors/tracking.py\", line 41, in __call__\n",
      "    loss_dict = self.objective(outputs, targets)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/tracking/transt.py\", line 211, in forward\n",
      "    losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes_pos))\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/tracking/transt.py\", line 187, in get_loss\n",
      "    return loss_map[loss](outputs, targets, indices, num_boxes)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/tracking/transt.py\", line 133, in loss_labels\n",
      "    loss_ce = nn.CrossEntropyLoss()(src_logits.transpose(1, 2), target_classes, reduction='none')\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "TypeError: forward() got an unexpected keyword argument 'reduction'\n",
      "\n",
      "Restarting training from last epoch ...\n",
      "Training crashed at epoch 2\n",
      "Traceback for the error!\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/base_trainer.py\", line 70, in train\n",
      "    self.train_epoch()\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/ltr_trainer.py\", line 87, in train_epoch\n",
      "    self.cycle_dataset(loader)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/ltr_trainer.py\", line 68, in cycle_dataset\n",
      "    loss, stats = self.actor(data)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/actors/tracking.py\", line 41, in __call__\n",
      "    loss_dict = self.objective(outputs, targets)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/tracking/transt.py\", line 211, in forward\n",
      "    losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes_pos))\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/tracking/transt.py\", line 187, in get_loss\n",
      "    return loss_map[loss](outputs, targets, indices, num_boxes)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/tracking/transt.py\", line 133, in loss_labels\n",
      "    loss_ce = nn.CrossEntropyLoss()(src_logits.transpose(1, 2), target_classes, reduction='none')\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "TypeError: forward() got an unexpected keyword argument 'reduction'\n",
      "\n",
      "Restarting training from last epoch ...\n",
      "Training crashed at epoch 2\n",
      "Traceback for the error!\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/base_trainer.py\", line 70, in train\n",
      "    self.train_epoch()\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/ltr_trainer.py\", line 87, in train_epoch\n",
      "    self.cycle_dataset(loader)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/ltr_trainer.py\", line 68, in cycle_dataset\n",
      "    loss, stats = self.actor(data)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/actors/tracking.py\", line 41, in __call__\n",
      "    loss_dict = self.objective(outputs, targets)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/tracking/transt.py\", line 211, in forward\n",
      "    losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes_pos))\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/tracking/transt.py\", line 187, in get_loss\n",
      "    return loss_map[loss](outputs, targets, indices, num_boxes)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/tracking/transt.py\", line 133, in loss_labels\n",
      "    loss_ce = nn.CrossEntropyLoss()(src_logits.transpose(1, 2), target_classes, reduction='none')\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "TypeError: forward() got an unexpected keyword argument 'reduction'\n",
      "\n",
      "Restarting training from last epoch ...\n",
      "=========================================\n",
      "Training End!!\n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "!python3 ../ltr/run_training.py transt transt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test\n",
    "- pysot_toolkit/test.py를 이용해서 평가 가능\n",
    "- test.py에 dataset_root과 net_path를 기록해야 됨\n",
    "- net_path가 pth파일(applilcation/octet_stream)이면 파일 위치를 작성함 <GitHub에서 제공하는 모델>\n",
    "- net_path가 체크포인트로 저장한 것이면 pth.tar이 위치한 경로를 작성함 <직접 훈련시킨 모델>\n",
    "- 결과는 results경로에 생성됨\n",
    "\n",
    "ex) python -u pysot_toolkit/test.py --dataset <name of dataset> --name 'transt' #test tracker #test tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../pysot_toolkit/..\n",
      "loading GOT-10k: 100%|███████████████████████| 180/180 [00:05<00:00, 32.66it/s, GOT-10k_Test_000180]\n",
      "=====================Test=======================\n",
      "/home/ahnsunghyun/pytorch/VisualTracking/TransT/checkpoints/checkpoints/ltr/transt/transt/TransT_ep0500.pth.tar\n",
      "================================================\n",
      "/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n",
      "(  1) Video: GOT-10k_Test_000001 Time:   8.9s Speed: 11.1fps\n",
      "(  2) Video: GOT-10k_Test_000002 Time:   3.3s Speed: 23.7fps\n",
      "(  3) Video: GOT-10k_Test_000003 Time:   2.6s Speed: 30.9fps\n",
      "(  4) Video: GOT-10k_Test_000004 Time:   2.3s Speed: 30.6fps\n",
      "(  5) Video: GOT-10k_Test_000005 Time:   2.5s Speed: 26.3fps\n",
      "(  6) Video: GOT-10k_Test_000006 Time:   3.8s Speed: 26.1fps\n",
      "(  7) Video: GOT-10k_Test_000007 Time:   3.6s Speed: 27.4fps\n",
      "(  8) Video: GOT-10k_Test_000008 Time:   3.7s Speed: 27.1fps\n",
      "(  9) Video: GOT-10k_Test_000009 Time:   3.2s Speed: 30.3fps\n",
      "( 10) Video: GOT-10k_Test_000010 Time:   3.5s Speed: 28.7fps\n",
      "( 11) Video: GOT-10k_Test_000011 Time:   3.9s Speed: 16.5fps\n",
      "( 12) Video: GOT-10k_Test_000012 Time:   3.8s Speed: 20.7fps\n",
      "( 13) Video: GOT-10k_Test_000013 Time:   2.5s Speed: 27.6fps\n",
      "( 14) Video: GOT-10k_Test_000014 Time:   3.4s Speed: 23.2fps\n",
      "( 15) Video: GOT-10k_Test_000015 Time:   3.1s Speed: 25.1fps\n",
      "( 16) Video: GOT-10k_Test_000016 Time:   2.4s Speed: 28.3fps\n",
      "( 17) Video: GOT-10k_Test_000017 Time:   2.7s Speed: 29.8fps\n",
      "( 18) Video: GOT-10k_Test_000018 Time:   3.1s Speed: 28.9fps\n",
      "( 19) Video: GOT-10k_Test_000019 Time:   2.6s Speed: 30.0fps\n",
      "( 20) Video: GOT-10k_Test_000020 Time:   3.3s Speed: 30.4fps\n",
      "( 21) Video: GOT-10k_Test_000021 Time:   2.1s Speed: 29.9fps\n",
      "( 22) Video: GOT-10k_Test_000022 Time:   4.1s Speed: 24.0fps\n",
      "( 23) Video: GOT-10k_Test_000023 Time:   3.7s Speed: 27.1fps\n",
      "( 24) Video: GOT-10k_Test_000024 Time:   3.5s Speed: 28.4fps\n",
      "( 25) Video: GOT-10k_Test_000025 Time:   3.1s Speed: 28.4fps\n",
      "( 26) Video: GOT-10k_Test_000026 Time:   2.9s Speed: 31.0fps\n",
      "( 27) Video: GOT-10k_Test_000027 Time:   3.3s Speed: 29.6fps\n",
      "( 28) Video: GOT-10k_Test_000028 Time:   3.6s Speed: 27.5fps\n",
      "( 29) Video: GOT-10k_Test_000029 Time:   3.5s Speed: 28.6fps\n",
      "( 30) Video: GOT-10k_Test_000030 Time:   3.3s Speed: 30.2fps\n",
      "( 31) Video: GOT-10k_Test_000031 Time:   3.2s Speed: 31.1fps\n",
      "( 32) Video: GOT-10k_Test_000032 Time:   3.2s Speed: 31.1fps\n",
      "( 33) Video: GOT-10k_Test_000033 Time:   1.9s Speed: 26.0fps\n",
      "( 34) Video: GOT-10k_Test_000034 Time:   2.8s Speed: 24.8fps\n",
      "( 35) Video: GOT-10k_Test_000035 Time:   2.6s Speed: 30.9fps\n",
      "( 36) Video: GOT-10k_Test_000036 Time:   2.9s Speed: 30.7fps\n",
      "( 37) Video: GOT-10k_Test_000037 Time:   3.8s Speed: 25.9fps\n",
      "( 38) Video: GOT-10k_Test_000038 Time:   3.9s Speed: 25.1fps\n",
      "( 39) Video: GOT-10k_Test_000039 Time:   3.5s Speed: 28.1fps\n",
      "( 40) Video: GOT-10k_Test_000040 Time:   4.9s Speed: 22.4fps\n",
      "( 41) Video: GOT-10k_Test_000041 Time:   2.9s Speed: 31.0fps\n",
      "( 42) Video: GOT-10k_Test_000042 Time:   3.2s Speed: 30.8fps\n",
      "( 43) Video: GOT-10k_Test_000043 Time:   3.2s Speed: 30.9fps\n",
      "( 44) Video: GOT-10k_Test_000044 Time:   3.2s Speed: 30.6fps\n",
      "( 45) Video: GOT-10k_Test_000045 Time:   3.2s Speed: 30.9fps\n",
      "( 46) Video: GOT-10k_Test_000046 Time:   3.4s Speed: 29.5fps\n",
      "( 47) Video: GOT-10k_Test_000047 Time:   3.3s Speed: 30.4fps\n",
      "( 48) Video: GOT-10k_Test_000048 Time:   3.3s Speed: 30.3fps\n",
      "( 49) Video: GOT-10k_Test_000049 Time:   8.7s Speed: 10.3fps\n",
      "( 50) Video: GOT-10k_Test_000050 Time:   2.6s Speed: 30.4fps\n",
      "( 51) Video: GOT-10k_Test_000051 Time:   3.9s Speed: 25.1fps\n",
      "( 52) Video: GOT-10k_Test_000052 Time:   2.3s Speed: 30.0fps\n",
      "( 53) Video: GOT-10k_Test_000053 Time:   2.3s Speed: 30.5fps\n",
      "( 54) Video: GOT-10k_Test_000054 Time:   2.9s Speed: 27.1fps\n",
      "( 55) Video: GOT-10k_Test_000055 Time:   3.2s Speed: 27.7fps\n",
      "( 56) Video: GOT-10k_Test_000056 Time:   4.9s Speed: 20.2fps\n",
      "( 57) Video: GOT-10k_Test_000057 Time:   3.8s Speed: 26.0fps\n",
      "( 58) Video: GOT-10k_Test_000058 Time:   3.5s Speed: 25.1fps\n",
      "( 59) Video: GOT-10k_Test_000059 Time:   2.6s Speed: 26.2fps\n",
      "( 60) Video: GOT-10k_Test_000060 Time:   5.4s Speed: 21.9fps\n",
      "( 61) Video: GOT-10k_Test_000061 Time:   3.7s Speed: 26.7fps\n",
      "( 62) Video: GOT-10k_Test_000062 Time:   3.5s Speed: 28.3fps\n",
      "( 63) Video: GOT-10k_Test_000063 Time:   2.8s Speed: 30.8fps\n",
      "( 64) Video: GOT-10k_Test_000064 Time:   8.3s Speed: 30.2fps\n",
      "( 65) Video: GOT-10k_Test_000065 Time:   3.0s Speed: 30.0fps\n",
      "( 66) Video: GOT-10k_Test_000066 Time:   4.0s Speed: 30.0fps\n",
      "( 67) Video: GOT-10k_Test_000067 Time:   3.3s Speed: 27.6fps\n",
      "( 68) Video: GOT-10k_Test_000068 Time:   3.4s Speed: 29.2fps\n",
      "( 69) Video: GOT-10k_Test_000069 Time:   3.6s Speed: 31.0fps\n",
      "( 70) Video: GOT-10k_Test_000070 Time:   5.2s Speed: 30.5fps\n",
      "( 71) Video: GOT-10k_Test_000071 Time:   3.8s Speed: 23.7fps\n",
      "( 72) Video: GOT-10k_Test_000072 Time:   3.9s Speed: 25.3fps\n",
      "( 73) Video: GOT-10k_Test_000073 Time:   3.1s Speed: 29.5fps\n",
      "( 74) Video: GOT-10k_Test_000074 Time:   4.3s Speed: 25.4fps\n",
      "( 75) Video: GOT-10k_Test_000075 Time:   4.5s Speed: 22.1fps\n",
      "( 76) Video: GOT-10k_Test_000076 Time:   4.8s Speed: 21.0fps\n",
      "( 77) Video: GOT-10k_Test_000077 Time:   4.1s Speed: 31.2fps\n",
      "( 78) Video: GOT-10k_Test_000078 Time:   2.5s Speed: 31.0fps\n",
      "( 79) Video: GOT-10k_Test_000079 Time:   4.5s Speed: 31.0fps\n",
      "( 80) Video: GOT-10k_Test_000080 Time:   4.0s Speed: 25.2fps\n",
      "( 81) Video: GOT-10k_Test_000081 Time:   3.8s Speed: 26.5fps\n",
      "( 82) Video: GOT-10k_Test_000082 Time:   4.2s Speed: 19.1fps\n",
      "( 83) Video: GOT-10k_Test_000083 Time:   3.8s Speed: 21.2fps\n",
      "( 84) Video: GOT-10k_Test_000084 Time:   2.2s Speed: 29.2fps\n",
      "( 85) Video: GOT-10k_Test_000085 Time:   8.7s Speed: 24.1fps\n",
      "( 86) Video: GOT-10k_Test_000086 Time:   6.6s Speed: 22.7fps\n",
      "( 87) Video: GOT-10k_Test_000087 Time:   3.1s Speed: 31.1fps\n",
      "( 88) Video: GOT-10k_Test_000088 Time:   2.3s Speed: 30.1fps\n",
      "( 89) Video: GOT-10k_Test_000089 Time:   4.2s Speed: 30.8fps\n",
      "( 90) Video: GOT-10k_Test_000090 Time:   4.4s Speed: 22.7fps\n",
      "( 91) Video: GOT-10k_Test_000091 Time:  10.2s Speed: 27.5fps\n",
      "( 92) Video: GOT-10k_Test_000092 Time:  35.1s Speed: 26.2fps\n",
      "( 93) Video: GOT-10k_Test_000093 Time:   3.4s Speed: 29.5fps\n",
      "( 94) Video: GOT-10k_Test_000094 Time:   2.6s Speed: 30.4fps\n",
      "( 95) Video: GOT-10k_Test_000095 Time:   3.2s Speed: 30.7fps\n",
      "( 96) Video: GOT-10k_Test_000096 Time:   3.2s Speed: 30.5fps\n",
      "( 97) Video: GOT-10k_Test_000097 Time:   2.8s Speed: 28.9fps\n",
      "( 98) Video: GOT-10k_Test_000098 Time:   3.9s Speed: 20.6fps\n",
      "( 99) Video: GOT-10k_Test_000099 Time:   4.2s Speed: 21.6fps\n",
      "(100) Video: GOT-10k_Test_000100 Time:   3.0s Speed: 30.0fps\n",
      "(101) Video: GOT-10k_Test_000101 Time:   3.4s Speed: 26.2fps\n",
      "(102) Video: GOT-10k_Test_000102 Time:   3.2s Speed: 31.1fps\n",
      "(103) Video: GOT-10k_Test_000103 Time:   2.3s Speed: 30.5fps\n",
      "(104) Video: GOT-10k_Test_000104 Time:   3.4s Speed: 29.3fps\n",
      "(105) Video: GOT-10k_Test_000105 Time:   3.2s Speed: 31.0fps\n",
      "(106) Video: GOT-10k_Test_000106 Time:   3.3s Speed: 30.1fps\n",
      "(107) Video: GOT-10k_Test_000107 Time:   3.4s Speed: 22.4fps\n",
      "(108) Video: GOT-10k_Test_000108 Time:   3.8s Speed: 26.5fps\n",
      "(109) Video: GOT-10k_Test_000109 Time:   2.5s Speed: 28.0fps\n",
      "(110) Video: GOT-10k_Test_000110 Time:   5.9s Speed: 11.6fps\n",
      "(111) Video: GOT-10k_Test_000111 Time:  14.9s Speed: 20.7fps\n",
      "(112) Video: GOT-10k_Test_000112 Time:   3.9s Speed: 24.5fps\n",
      "(113) Video: GOT-10k_Test_000113 Time:   3.4s Speed: 29.4fps\n",
      "(114) Video: GOT-10k_Test_000114 Time:   5.2s Speed: 20.8fps\n",
      "(115) Video: GOT-10k_Test_000115 Time:   5.8s Speed: 24.0fps\n",
      "(116) Video: GOT-10k_Test_000116 Time:   5.5s Speed: 28.8fps\n",
      "(117) Video: GOT-10k_Test_000117 Time:  11.3s Speed: 31.0fps\n",
      "(118) Video: GOT-10k_Test_000118 Time:   4.8s Speed: 14.7fps\n",
      "(119) Video: GOT-10k_Test_000119 Time:   2.5s Speed: 28.1fps\n",
      "(120) Video: GOT-10k_Test_000120 Time:   6.8s Speed: 29.1fps\n",
      "(121) Video: GOT-10k_Test_000121 Time:   6.6s Speed: 28.8fps\n",
      "(122) Video: GOT-10k_Test_000122 Time:   3.7s Speed: 26.5fps\n",
      "(123) Video: GOT-10k_Test_000123 Time:   3.2s Speed: 30.9fps\n",
      "(124) Video: GOT-10k_Test_000124 Time:   2.6s Speed: 31.1fps\n",
      "(125) Video: GOT-10k_Test_000125 Time:   2.3s Speed: 31.1fps\n",
      "(126) Video: GOT-10k_Test_000126 Time:   2.6s Speed: 31.1fps\n",
      "(127) Video: GOT-10k_Test_000127 Time:   2.3s Speed: 31.1fps\n",
      "(128) Video: GOT-10k_Test_000128 Time:   2.6s Speed: 30.9fps\n",
      "(129) Video: GOT-10k_Test_000129 Time:   3.9s Speed: 30.9fps\n",
      "(130) Video: GOT-10k_Test_000130 Time:   3.9s Speed: 31.0fps\n",
      "(131) Video: GOT-10k_Test_000131 Time:   4.6s Speed: 24.0fps\n",
      "(132) Video: GOT-10k_Test_000132 Time:   5.1s Speed: 15.6fps\n",
      "(133) Video: GOT-10k_Test_000133 Time:   9.3s Speed: 31.3fps\n",
      "(134) Video: GOT-10k_Test_000134 Time:  10.4s Speed: 29.8fps\n",
      "(135) Video: GOT-10k_Test_000135 Time:  15.7s Speed: 9.6fps\n",
      "(136) Video: GOT-10k_Test_000136 Time:   7.6s Speed: 22.9fps\n",
      "(137) Video: GOT-10k_Test_000137 Time:  17.0s Speed: 20.6fps\n",
      "(138) Video: GOT-10k_Test_000138 Time:  34.9s Speed: 6.3fps\n",
      "(139) Video: GOT-10k_Test_000139 Time:   5.3s Speed: 28.1fps\n",
      "(140) Video: GOT-10k_Test_000140 Time:   5.7s Speed: 24.5fps\n",
      "(141) Video: GOT-10k_Test_000141 Time:   3.7s Speed: 27.1fps\n",
      "(142) Video: GOT-10k_Test_000142 Time:   7.0s Speed: 31.3fps\n",
      "(143) Video: GOT-10k_Test_000143 Time:   2.6s Speed: 30.6fps\n",
      "(144) Video: GOT-10k_Test_000144 Time:  10.5s Speed: 28.5fps\n",
      "(145) Video: GOT-10k_Test_000145 Time:   7.9s Speed: 30.5fps\n",
      "(146) Video: GOT-10k_Test_000146 Time:   4.3s Speed: 29.9fps\n",
      "(147) Video: GOT-10k_Test_000147 Time:   5.5s Speed: 31.0fps\n",
      "(148) Video: GOT-10k_Test_000148 Time:   4.1s Speed: 28.1fps\n",
      "(149) Video: GOT-10k_Test_000149 Time:   6.4s Speed: 28.3fps\n",
      "(150) Video: GOT-10k_Test_000150 Time:  16.3s Speed: 30.1fps\n",
      "(151) Video: GOT-10k_Test_000151 Time:   8.3s Speed: 30.0fps\n",
      "(152) Video: GOT-10k_Test_000152 Time:   6.0s Speed: 30.1fps\n",
      "(153) Video: GOT-10k_Test_000153 Time:   3.3s Speed: 27.6fps\n",
      "(154) Video: GOT-10k_Test_000154 Time:   4.3s Speed: 20.9fps\n",
      "(155) Video: GOT-10k_Test_000155 Time:   9.9s Speed: 31.3fps\n",
      "(156) Video: GOT-10k_Test_000156 Time:   5.3s Speed: 24.5fps\n",
      "(157) Video: GOT-10k_Test_000157 Time:   8.3s Speed: 28.9fps\n",
      "(158) Video: GOT-10k_Test_000158 Time:   6.4s Speed: 32.6fps\n",
      "(159) Video: GOT-10k_Test_000159 Time:   2.9s Speed: 34.3fps\n",
      "(160) Video: GOT-10k_Test_000160 Time:  10.5s Speed: 34.3fps\n",
      "(161) Video: GOT-10k_Test_000161 Time:   6.5s Speed: 32.5fps\n",
      "(162) Video: GOT-10k_Test_000162 Time:   2.9s Speed: 34.1fps\n",
      "(163) Video: GOT-10k_Test_000163 Time:   4.5s Speed: 29.2fps\n",
      "(164) Video: GOT-10k_Test_000164 Time:  19.4s Speed: 35.1fps\n",
      "(165) Video: GOT-10k_Test_000165 Time:  30.1s Speed: 6.6fps\n",
      "(166) Video: GOT-10k_Test_000166 Time:   2.4s Speed: 28.9fps\n",
      "(167) Video: GOT-10k_Test_000167 Time:   2.9s Speed: 34.1fps\n",
      "(168) Video: GOT-10k_Test_000168 Time:   2.3s Speed: 34.1fps\n",
      "(169) Video: GOT-10k_Test_000169 Time:   2.7s Speed: 33.9fps\n",
      "(170) Video: GOT-10k_Test_000170 Time:   3.1s Speed: 28.7fps\n",
      "(171) Video: GOT-10k_Test_000171 Time:   2.7s Speed: 32.7fps\n",
      "(172) Video: GOT-10k_Test_000172 Time:   2.4s Speed: 33.5fps\n",
      "(173) Video: GOT-10k_Test_000173 Time:   2.6s Speed: 31.2fps\n",
      "(174) Video: GOT-10k_Test_000174 Time:   2.7s Speed: 32.4fps\n",
      "(175) Video: GOT-10k_Test_000175 Time:   2.7s Speed: 33.5fps\n",
      "(176) Video: GOT-10k_Test_000176 Time:   2.9s Speed: 34.6fps\n",
      "(177) Video: GOT-10k_Test_000177 Time:   2.9s Speed: 34.3fps\n",
      "(178) Video: GOT-10k_Test_000178 Time:   4.7s Speed: 25.5fps\n",
      "(179) Video: GOT-10k_Test_000179 Time:   2.9s Speed: 31.4fps\n",
      "(180) Video: GOT-10k_Test_000180 Time:   2.7s Speed: 28.8fps\n"
     ]
    }
   ],
   "source": [
    "# pre-trained된 transt 모델로 GOT-10k 테스트\n",
    "!python3 -u ../pysot_toolkit/test.py --dataset 'GOT-10k' --name 'transt_convnext_500' #test tracker #test tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "- pysot_toolkit/eval.py를 이용해서 평가 가능\n",
    "- OTB, LaSOT, UAV, NFS, VOT2016~2019만 평가 가능\n",
    "- GOT-10k를 평가하려면 got10k_toolkit을 참조\n",
    "\n",
    "ex) python pysot_toolkit/eval.py --tracker_path results/ --dataset <name of dataset> --num 1 --tracker_prefix 'transt' #eval tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records saved at ../transt_convnext_500.zip\n",
      "\u001b[93mLogin and follow instructions on\n",
      "http://got-10k.aitestunion.com/submit_instructions\n",
      "to upload and evaluate your tracking results\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 ../got10k_toolkit/toolkit/evaluate_got.py # got-10k 평가 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
