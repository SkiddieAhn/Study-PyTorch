{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training\n",
    "- ltr/run_training.py를 이용해서 학습 가능\n",
    "- ltr/admin/local.py에 dataset 경로를 기록해야 됨 (train 경로를 가리키게)\n",
    "- ltr/train_settings/transt/transt.py를 수정해서 커스터마이징 (배치 사이즈, 데이터셋 등)\n",
    "- 원래는 Lasot, Got10k, TrackingNet, MSCOCO 데이터로 학습 -> Got10k로만 학습\n",
    "- 비디오 수가 500개인 GOT-10k-mini 데이터셋을 제작함 (ltr/dataset/got10k.py에 코드를 추가함)\n",
    "- 학습된 모델을 불러 오려면 checkpoint의 마지막 pth.tar파일을 이용\n",
    "\n",
    "ex) python run_training.py transt transt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:  transt  transt\n",
      "=========================================\n",
      "DataLoader ok!!\n",
      "=========================================\n",
      "Loaded pretrained weights for efficientnet-b7\n",
      "=========================================\n",
      "model load ok!!\n",
      "=========================================\n",
      "number of params: 83422966\n",
      "=========================================\n",
      "Training Log PATH <tensorboard>\n",
      "/media/ahnsunghyun/HDD/checkpoints/tensorboard/ltr/transt/transt\n",
      "=========================================\n",
      "2022-06-29 16:04:34.447519: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "=========================================\n",
      "Training Start!!\n",
      "=========================================\n",
      "No matching checkpoint file found\n",
      "/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/functional.py:3502: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\n",
      "Training crashed at epoch 1\n",
      "Traceback for the error!\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/base_trainer.py\", line 70, in train\n",
      "    self.train_epoch()\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/ltr_trainer.py\", line 87, in train_epoch\n",
      "    self.cycle_dataset(loader)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/ltr_trainer.py\", line 68, in cycle_dataset\n",
      "    loss, stats = self.actor(data)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/actors/tracking.py\", line 17, in __call__\n",
      "    outputs = self.net(data['search_images'], data['template_images'])\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 165, in forward\n",
      "    return self.module(*inputs[0], **kwargs[0])\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/tracking/transt.py\", line 53, in forward\n",
      "    feature_search, pos_search = self.backbone(search)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/backbone/transt_backbone.py\", line 120, in forward\n",
      "    xs = self[0](tensor_list)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/backbone/transt_backbone.py\", line 82, in forward\n",
      "    x = self.body(tensor_list.tensors)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/backbone/efficientnet.py\", line 65, in forward\n",
      "    x = self.effConv(x) # (600x600) -> (76x76)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/efficientnet_pytorch_edit/model.py\", line 316, in forward\n",
      "    x = self.extract_features(inputs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/efficientnet_pytorch_edit/model.py\", line 298, in extract_features\n",
      "    x = block(x, drop_connect_rate=drop_connect_rate)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/efficientnet_pytorch_edit/model.py\", line 107, in forward\n",
      "    x = self._swish(x)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/efficientnet_pytorch_edit/utils.py\", line 80, in forward\n",
      "    return SwishImplementation.apply(x)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/efficientnet_pytorch_edit/utils.py\", line 67, in forward\n",
      "    result = i * torch.sigmoid(i)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 5.79 GiB total capacity; 4.21 GiB already allocated; 83.00 MiB free; 4.35 GiB reserved in total by PyTorch)\n",
      "\n",
      "Restarting training from last epoch ...\n",
      "No matching checkpoint file found\n",
      "Training crashed at epoch 1\n",
      "Traceback for the error!\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/base_trainer.py\", line 70, in train\n",
      "    self.train_epoch()\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/ltr_trainer.py\", line 87, in train_epoch\n",
      "    self.cycle_dataset(loader)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/ltr_trainer.py\", line 68, in cycle_dataset\n",
      "    loss, stats = self.actor(data)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/actors/tracking.py\", line 17, in __call__\n",
      "    outputs = self.net(data['search_images'], data['template_images'])\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 165, in forward\n",
      "    return self.module(*inputs[0], **kwargs[0])\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/tracking/transt.py\", line 53, in forward\n",
      "    feature_search, pos_search = self.backbone(search)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/backbone/transt_backbone.py\", line 120, in forward\n",
      "    xs = self[0](tensor_list)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/backbone/transt_backbone.py\", line 82, in forward\n",
      "    x = self.body(tensor_list.tensors)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/backbone/efficientnet.py\", line 65, in forward\n",
      "    x = self.effConv(x) # (600x600) -> (76x76)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/efficientnet_pytorch_edit/model.py\", line 316, in forward\n",
      "    x = self.extract_features(inputs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/efficientnet_pytorch_edit/model.py\", line 298, in extract_features\n",
      "    x = block(x, drop_connect_rate=drop_connect_rate)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/efficientnet_pytorch_edit/model.py\", line 106, in forward\n",
      "    x = self._bn0(x)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py\", line 135, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/functional.py\", line 2149, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 5.79 GiB total capacity; 4.11 GiB already allocated; 77.56 MiB free; 4.35 GiB reserved in total by PyTorch)\n",
      "\n",
      "Restarting training from last epoch ...\n",
      "No matching checkpoint file found\n",
      "Training crashed at epoch 1\n",
      "Traceback for the error!\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/base_trainer.py\", line 70, in train\n",
      "    self.train_epoch()\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/ltr_trainer.py\", line 87, in train_epoch\n",
      "    self.cycle_dataset(loader)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/ltr_trainer.py\", line 68, in cycle_dataset\n",
      "    loss, stats = self.actor(data)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/actors/tracking.py\", line 17, in __call__\n",
      "    outputs = self.net(data['search_images'], data['template_images'])\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 165, in forward\n",
      "    return self.module(*inputs[0], **kwargs[0])\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/tracking/transt.py\", line 53, in forward\n",
      "    feature_search, pos_search = self.backbone(search)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/backbone/transt_backbone.py\", line 120, in forward\n",
      "    xs = self[0](tensor_list)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/backbone/transt_backbone.py\", line 82, in forward\n",
      "    x = self.body(tensor_list.tensors)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/backbone/efficientnet.py\", line 65, in forward\n",
      "    x = self.effConv(x) # (600x600) -> (76x76)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/efficientnet_pytorch_edit/model.py\", line 316, in forward\n",
      "    x = self.extract_features(inputs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/efficientnet_pytorch_edit/model.py\", line 298, in extract_features\n",
      "    x = block(x, drop_connect_rate=drop_connect_rate)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/efficientnet_pytorch_edit/model.py\", line 106, in forward\n",
      "    x = self._bn0(x)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py\", line 135, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/functional.py\", line 2149, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 5.79 GiB total capacity; 4.11 GiB already allocated; 80.56 MiB free; 4.35 GiB reserved in total by PyTorch)\n",
      "\n",
      "Restarting training from last epoch ...\n",
      "No matching checkpoint file found\n",
      "Training crashed at epoch 1\n",
      "Traceback for the error!\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/base_trainer.py\", line 70, in train\n",
      "    self.train_epoch()\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/ltr_trainer.py\", line 87, in train_epoch\n",
      "    self.cycle_dataset(loader)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/ltr_trainer.py\", line 68, in cycle_dataset\n",
      "    loss, stats = self.actor(data)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/actors/tracking.py\", line 17, in __call__\n",
      "    outputs = self.net(data['search_images'], data['template_images'])\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 165, in forward\n",
      "    return self.module(*inputs[0], **kwargs[0])\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/tracking/transt.py\", line 53, in forward\n",
      "    feature_search, pos_search = self.backbone(search)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/backbone/transt_backbone.py\", line 120, in forward\n",
      "    xs = self[0](tensor_list)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/backbone/transt_backbone.py\", line 82, in forward\n",
      "    x = self.body(tensor_list.tensors)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/backbone/efficientnet.py\", line 65, in forward\n",
      "    x = self.effConv(x) # (600x600) -> (76x76)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/efficientnet_pytorch_edit/model.py\", line 316, in forward\n",
      "    x = self.extract_features(inputs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/efficientnet_pytorch_edit/model.py\", line 298, in extract_features\n",
      "    x = block(x, drop_connect_rate=drop_connect_rate)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/efficientnet_pytorch_edit/model.py\", line 106, in forward\n",
      "    x = self._bn0(x)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py\", line 135, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/functional.py\", line 2149, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 5.79 GiB total capacity; 4.11 GiB already allocated; 89.25 MiB free; 4.35 GiB reserved in total by PyTorch)\n",
      "\n",
      "Restarting training from last epoch ...\n",
      "No matching checkpoint file found\n",
      "Training crashed at epoch 1\n",
      "Traceback for the error!\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/base_trainer.py\", line 70, in train\n",
      "    self.train_epoch()\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/ltr_trainer.py\", line 87, in train_epoch\n",
      "    self.cycle_dataset(loader)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/ltr_trainer.py\", line 68, in cycle_dataset\n",
      "    loss, stats = self.actor(data)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/actors/tracking.py\", line 17, in __call__\n",
      "    outputs = self.net(data['search_images'], data['template_images'])\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 165, in forward\n",
      "    return self.module(*inputs[0], **kwargs[0])\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/tracking/transt.py\", line 53, in forward\n",
      "    feature_search, pos_search = self.backbone(search)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/backbone/transt_backbone.py\", line 120, in forward\n",
      "    xs = self[0](tensor_list)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/backbone/transt_backbone.py\", line 82, in forward\n",
      "    x = self.body(tensor_list.tensors)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/backbone/efficientnet.py\", line 65, in forward\n",
      "    x = self.effConv(x) # (600x600) -> (76x76)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/efficientnet_pytorch_edit/model.py\", line 316, in forward\n",
      "    x = self.extract_features(inputs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/efficientnet_pytorch_edit/model.py\", line 298, in extract_features\n",
      "    x = block(x, drop_connect_rate=drop_connect_rate)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/efficientnet_pytorch_edit/model.py\", line 106, in forward\n",
      "    x = self._bn0(x)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py\", line 135, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/functional.py\", line 2149, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 5.79 GiB total capacity; 4.11 GiB already allocated; 87.25 MiB free; 4.35 GiB reserved in total by PyTorch)\n",
      "\n",
      "Restarting training from last epoch ...\n",
      "No matching checkpoint file found\n",
      "Training crashed at epoch 1\n",
      "Traceback for the error!\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/base_trainer.py\", line 70, in train\n",
      "    self.train_epoch()\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/ltr_trainer.py\", line 87, in train_epoch\n",
      "    self.cycle_dataset(loader)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/ltr_trainer.py\", line 68, in cycle_dataset\n",
      "    loss, stats = self.actor(data)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/actors/tracking.py\", line 17, in __call__\n",
      "    outputs = self.net(data['search_images'], data['template_images'])\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 165, in forward\n",
      "    return self.module(*inputs[0], **kwargs[0])\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/tracking/transt.py\", line 53, in forward\n",
      "    feature_search, pos_search = self.backbone(search)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/backbone/transt_backbone.py\", line 120, in forward\n",
      "    xs = self[0](tensor_list)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/backbone/transt_backbone.py\", line 82, in forward\n",
      "    x = self.body(tensor_list.tensors)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/backbone/efficientnet.py\", line 65, in forward\n",
      "    x = self.effConv(x) # (600x600) -> (76x76)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/efficientnet_pytorch_edit/model.py\", line 316, in forward\n",
      "    x = self.extract_features(inputs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/efficientnet_pytorch_edit/model.py\", line 298, in extract_features\n",
      "    x = block(x, drop_connect_rate=drop_connect_rate)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/efficientnet_pytorch_edit/model.py\", line 106, in forward\n",
      "    x = self._bn0(x)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py\", line 135, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/functional.py\", line 2149, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 5.79 GiB total capacity; 4.11 GiB already allocated; 87.25 MiB free; 4.35 GiB reserved in total by PyTorch)\n",
      "\n",
      "Restarting training from last epoch ...\n",
      "No matching checkpoint file found\n",
      "Training crashed at epoch 1\n",
      "Traceback for the error!\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/base_trainer.py\", line 70, in train\n",
      "    self.train_epoch()\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/ltr_trainer.py\", line 87, in train_epoch\n",
      "    self.cycle_dataset(loader)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/ltr_trainer.py\", line 68, in cycle_dataset\n",
      "    loss, stats = self.actor(data)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/actors/tracking.py\", line 17, in __call__\n",
      "    outputs = self.net(data['search_images'], data['template_images'])\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 165, in forward\n",
      "    return self.module(*inputs[0], **kwargs[0])\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/tracking/transt.py\", line 53, in forward\n",
      "    feature_search, pos_search = self.backbone(search)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/backbone/transt_backbone.py\", line 120, in forward\n",
      "    xs = self[0](tensor_list)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/backbone/transt_backbone.py\", line 82, in forward\n",
      "    x = self.body(tensor_list.tensors)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/backbone/efficientnet.py\", line 65, in forward\n",
      "    x = self.effConv(x) # (600x600) -> (76x76)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/efficientnet_pytorch_edit/model.py\", line 316, in forward\n",
      "    x = self.extract_features(inputs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/efficientnet_pytorch_edit/model.py\", line 298, in extract_features\n",
      "    x = block(x, drop_connect_rate=drop_connect_rate)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/efficientnet_pytorch_edit/model.py\", line 106, in forward\n",
      "    x = self._bn0(x)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py\", line 135, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/functional.py\", line 2149, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 5.79 GiB total capacity; 4.11 GiB already allocated; 91.25 MiB free; 4.35 GiB reserved in total by PyTorch)\n",
      "\n",
      "Restarting training from last epoch ...\n",
      "No matching checkpoint file found\n",
      "Training crashed at epoch 1\n",
      "Traceback for the error!\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/base_trainer.py\", line 70, in train\n",
      "    self.train_epoch()\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/ltr_trainer.py\", line 87, in train_epoch\n",
      "    self.cycle_dataset(loader)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/ltr_trainer.py\", line 68, in cycle_dataset\n",
      "    loss, stats = self.actor(data)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/actors/tracking.py\", line 17, in __call__\n",
      "    outputs = self.net(data['search_images'], data['template_images'])\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 165, in forward\n",
      "    return self.module(*inputs[0], **kwargs[0])\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/tracking/transt.py\", line 53, in forward\n",
      "    feature_search, pos_search = self.backbone(search)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/backbone/transt_backbone.py\", line 120, in forward\n",
      "    xs = self[0](tensor_list)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/backbone/transt_backbone.py\", line 82, in forward\n",
      "    x = self.body(tensor_list.tensors)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/backbone/efficientnet.py\", line 65, in forward\n",
      "    x = self.effConv(x) # (600x600) -> (76x76)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/efficientnet_pytorch_edit/model.py\", line 316, in forward\n",
      "    x = self.extract_features(inputs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/efficientnet_pytorch_edit/model.py\", line 298, in extract_features\n",
      "    x = block(x, drop_connect_rate=drop_connect_rate)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/efficientnet_pytorch_edit/model.py\", line 106, in forward\n",
      "    x = self._bn0(x)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py\", line 135, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/functional.py\", line 2149, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 5.79 GiB total capacity; 4.11 GiB already allocated; 87.25 MiB free; 4.35 GiB reserved in total by PyTorch)\n",
      "\n",
      "Restarting training from last epoch ...\n",
      "No matching checkpoint file found\n",
      "Training crashed at epoch 1\n",
      "Traceback for the error!\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/base_trainer.py\", line 70, in train\n",
      "    self.train_epoch()\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/ltr_trainer.py\", line 87, in train_epoch\n",
      "    self.cycle_dataset(loader)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/ltr_trainer.py\", line 68, in cycle_dataset\n",
      "    loss, stats = self.actor(data)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/actors/tracking.py\", line 17, in __call__\n",
      "    outputs = self.net(data['search_images'], data['template_images'])\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 165, in forward\n",
      "    return self.module(*inputs[0], **kwargs[0])\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/tracking/transt.py\", line 53, in forward\n",
      "    feature_search, pos_search = self.backbone(search)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/backbone/transt_backbone.py\", line 120, in forward\n",
      "    xs = self[0](tensor_list)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/backbone/transt_backbone.py\", line 82, in forward\n",
      "    x = self.body(tensor_list.tensors)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/backbone/efficientnet.py\", line 65, in forward\n",
      "    x = self.effConv(x) # (600x600) -> (76x76)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/efficientnet_pytorch_edit/model.py\", line 316, in forward\n",
      "    x = self.extract_features(inputs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/efficientnet_pytorch_edit/model.py\", line 298, in extract_features\n",
      "    x = block(x, drop_connect_rate=drop_connect_rate)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/efficientnet_pytorch_edit/model.py\", line 106, in forward\n",
      "    x = self._bn0(x)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py\", line 135, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/functional.py\", line 2149, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 5.79 GiB total capacity; 4.11 GiB already allocated; 87.25 MiB free; 4.35 GiB reserved in total by PyTorch)\n",
      "\n",
      "Restarting training from last epoch ...\n",
      "No matching checkpoint file found\n",
      "Training crashed at epoch 1\n",
      "Traceback for the error!\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/base_trainer.py\", line 70, in train\n",
      "    self.train_epoch()\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/ltr_trainer.py\", line 87, in train_epoch\n",
      "    self.cycle_dataset(loader)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/trainers/ltr_trainer.py\", line 68, in cycle_dataset\n",
      "    loss, stats = self.actor(data)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/actors/tracking.py\", line 17, in __call__\n",
      "    outputs = self.net(data['search_images'], data['template_images'])\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/parallel/data_parallel.py\", line 165, in forward\n",
      "    return self.module(*inputs[0], **kwargs[0])\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/tracking/transt.py\", line 53, in forward\n",
      "    feature_search, pos_search = self.backbone(search)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/backbone/transt_backbone.py\", line 120, in forward\n",
      "    xs = self[0](tensor_list)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/backbone/transt_backbone.py\", line 82, in forward\n",
      "    x = self.body(tensor_list.tensors)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/ltr/models/backbone/efficientnet.py\", line 65, in forward\n",
      "    x = self.effConv(x) # (600x600) -> (76x76)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/efficientnet_pytorch_edit/model.py\", line 316, in forward\n",
      "    x = self.extract_features(inputs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/efficientnet_pytorch_edit/model.py\", line 298, in extract_features\n",
      "    x = block(x, drop_connect_rate=drop_connect_rate)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/pytorch/VisualTracking/TransT/efficientnet_pytorch_edit/model.py\", line 106, in forward\n",
      "    x = self._bn0(x)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py\", line 135, in forward\n",
      "    return F.batch_norm(\n",
      "  File \"/home/ahnsunghyun/.local/lib/python3.8/site-packages/torch/nn/functional.py\", line 2149, in batch_norm\n",
      "    return torch.batch_norm(\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 5.79 GiB total capacity; 4.11 GiB already allocated; 87.25 MiB free; 4.35 GiB reserved in total by PyTorch)\n",
      "\n",
      "Restarting training from last epoch ...\n",
      "=========================================\n",
      "Training End!!\n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "!python3 ../ltr/run_training.py transt transt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test\n",
    "- pysot_toolkit/test.py를 이용해서 평가 가능\n",
    "- test.py에 dataset_root과 net_path를 기록해야 됨\n",
    "- net_path가 pth파일(applilcation/octet_stream)이면 파일 위치를 작성함 <GitHub에서 제공하는 모델>\n",
    "- net_path가 체크포인트로 저장한 것이면 pth.tar이 위치한 경로를 작성함 <직접 훈련시킨 모델>\n",
    "- 결과는 results경로에 생성됨\n",
    "\n",
    "ex) python -u pysot_toolkit/test.py --dataset <name of dataset> --name 'transt' #test tracker #test tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../pysot_toolkit/..\n",
      "loading GOT-10k: 100%|███████████████████████| 180/180 [00:02<00:00, 74.35it/s, GOT-10k_Test_000180]\n",
      "=====================Test=======================\n",
      "/media/ahnsunghyun/HDD/checkpoints/checkpoints/ltr/transt/transt/TransT_ep0050.pth.tar\n",
      "================================================\n",
      "Loaded pretrained weights for efficientnet-b7\n",
      "(  1) Video: GOT-10k_Test_000001 Time:  11.1s Speed: 8.9fps\n",
      "(  2) Video: GOT-10k_Test_000002 Time:   3.1s Speed: 25.5fps\n",
      "(  3) Video: GOT-10k_Test_000003 Time:   2.8s Speed: 27.8fps\n",
      "(  4) Video: GOT-10k_Test_000004 Time:   2.5s Speed: 27.9fps\n",
      "(  5) Video: GOT-10k_Test_000005 Time:   2.5s Speed: 27.0fps\n",
      "(  6) Video: GOT-10k_Test_000006 Time:   3.9s Speed: 25.3fps\n",
      "(  7) Video: GOT-10k_Test_000007 Time:   4.0s Speed: 24.6fps\n",
      "(  8) Video: GOT-10k_Test_000008 Time:   3.6s Speed: 27.7fps\n",
      "(  9) Video: GOT-10k_Test_000009 Time:   3.5s Speed: 27.4fps\n",
      "( 10) Video: GOT-10k_Test_000010 Time:   3.6s Speed: 27.3fps\n",
      "( 11) Video: GOT-10k_Test_000011 Time:   2.6s Speed: 25.1fps\n",
      "( 12) Video: GOT-10k_Test_000012 Time:   3.7s Speed: 21.4fps\n",
      "( 13) Video: GOT-10k_Test_000013 Time:   2.5s Speed: 27.1fps\n",
      "( 14) Video: GOT-10k_Test_000014 Time:   3.1s Speed: 25.7fps\n",
      "( 15) Video: GOT-10k_Test_000015 Time:   2.8s Speed: 28.2fps\n",
      "( 16) Video: GOT-10k_Test_000016 Time:   2.6s Speed: 26.5fps\n",
      "( 17) Video: GOT-10k_Test_000017 Time:   2.8s Speed: 28.4fps\n",
      "( 18) Video: GOT-10k_Test_000018 Time:   3.2s Speed: 28.1fps\n",
      "( 19) Video: GOT-10k_Test_000019 Time:   2.8s Speed: 28.2fps\n",
      "( 20) Video: GOT-10k_Test_000020 Time:   3.5s Speed: 28.2fps\n",
      "( 21) Video: GOT-10k_Test_000021 Time:   2.3s Speed: 27.3fps\n",
      "( 22) Video: GOT-10k_Test_000022 Time:   3.7s Speed: 26.7fps\n",
      "( 23) Video: GOT-10k_Test_000023 Time:   3.7s Speed: 26.6fps\n",
      "( 24) Video: GOT-10k_Test_000024 Time:   3.5s Speed: 28.2fps\n",
      "( 25) Video: GOT-10k_Test_000025 Time:   3.2s Speed: 27.9fps\n",
      "( 26) Video: GOT-10k_Test_000026 Time:   3.2s Speed: 28.1fps\n",
      "( 27) Video: GOT-10k_Test_000027 Time:   3.6s Speed: 27.5fps\n",
      "( 28) Video: GOT-10k_Test_000028 Time:   3.6s Speed: 27.3fps\n",
      "( 29) Video: GOT-10k_Test_000029 Time:   3.7s Speed: 26.9fps\n",
      "( 30) Video: GOT-10k_Test_000030 Time:   3.6s Speed: 27.3fps\n",
      "( 31) Video: GOT-10k_Test_000031 Time:   3.5s Speed: 28.3fps\n",
      "( 32) Video: GOT-10k_Test_000032 Time:   3.5s Speed: 28.2fps\n",
      "( 33) Video: GOT-10k_Test_000033 Time:   2.0s Speed: 25.0fps\n",
      "( 34) Video: GOT-10k_Test_000034 Time:   2.7s Speed: 25.5fps\n",
      "( 35) Video: GOT-10k_Test_000035 Time:   2.8s Speed: 28.0fps\n",
      "( 36) Video: GOT-10k_Test_000036 Time:   3.2s Speed: 28.1fps\n",
      "( 37) Video: GOT-10k_Test_000037 Time:   3.7s Speed: 26.5fps\n",
      "( 38) Video: GOT-10k_Test_000038 Time:   4.0s Speed: 24.6fps\n",
      "( 39) Video: GOT-10k_Test_000039 Time:   3.6s Speed: 27.4fps\n",
      "( 40) Video: GOT-10k_Test_000040 Time:   4.8s Speed: 22.8fps\n",
      "( 41) Video: GOT-10k_Test_000041 Time:   3.1s Speed: 28.3fps\n",
      "( 42) Video: GOT-10k_Test_000042 Time:   3.5s Speed: 28.3fps\n",
      "( 43) Video: GOT-10k_Test_000043 Time:   3.5s Speed: 28.0fps\n",
      "( 44) Video: GOT-10k_Test_000044 Time:   3.5s Speed: 28.5fps\n",
      "( 45) Video: GOT-10k_Test_000045 Time:   3.5s Speed: 28.5fps\n",
      "( 46) Video: GOT-10k_Test_000046 Time:   3.6s Speed: 27.9fps\n",
      "( 47) Video: GOT-10k_Test_000047 Time:   3.5s Speed: 28.0fps\n",
      "( 48) Video: GOT-10k_Test_000048 Time:   3.6s Speed: 27.6fps\n",
      "( 49) Video: GOT-10k_Test_000049 Time:   3.4s Speed: 25.9fps\n",
      "( 50) Video: GOT-10k_Test_000050 Time:   2.8s Speed: 28.1fps\n",
      "( 51) Video: GOT-10k_Test_000051 Time:   4.1s Speed: 23.8fps\n",
      "( 52) Video: GOT-10k_Test_000052 Time:   2.6s Speed: 26.1fps\n",
      "( 53) Video: GOT-10k_Test_000053 Time:   2.5s Speed: 27.2fps\n",
      "( 54) Video: GOT-10k_Test_000054 Time:   3.1s Speed: 25.4fps\n",
      "( 55) Video: GOT-10k_Test_000055 Time:   3.3s Speed: 26.7fps\n",
      "( 56) Video: GOT-10k_Test_000056 Time:   4.6s Speed: 21.5fps\n",
      "( 57) Video: GOT-10k_Test_000057 Time:   3.7s Speed: 26.7fps\n",
      "( 58) Video: GOT-10k_Test_000058 Time:   3.3s Speed: 26.9fps\n",
      "( 59) Video: GOT-10k_Test_000059 Time:   2.4s Speed: 27.8fps\n",
      "( 60) Video: GOT-10k_Test_000060 Time:   4.3s Speed: 27.4fps\n",
      "( 61) Video: GOT-10k_Test_000061 Time:   3.6s Speed: 27.4fps\n",
      "( 62) Video: GOT-10k_Test_000062 Time:   3.7s Speed: 27.1fps\n",
      "( 63) Video: GOT-10k_Test_000063 Time:   3.1s Speed: 28.3fps\n",
      "( 64) Video: GOT-10k_Test_000064 Time:   9.0s Speed: 27.8fps\n",
      "( 65) Video: GOT-10k_Test_000065 Time:   3.3s Speed: 27.6fps\n",
      "( 66) Video: GOT-10k_Test_000066 Time:   4.3s Speed: 27.8fps\n",
      "( 67) Video: GOT-10k_Test_000067 Time:   3.2s Speed: 28.1fps\n",
      "( 68) Video: GOT-10k_Test_000068 Time:   3.7s Speed: 27.3fps\n",
      "( 69) Video: GOT-10k_Test_000069 Time:   3.9s Speed: 28.4fps\n",
      "( 70) Video: GOT-10k_Test_000070 Time:   5.7s Speed: 28.0fps\n",
      "( 71) Video: GOT-10k_Test_000071 Time:   3.6s Speed: 25.1fps\n",
      "( 72) Video: GOT-10k_Test_000072 Time:   3.5s Speed: 28.5fps\n",
      "( 73) Video: GOT-10k_Test_000073 Time:   3.2s Speed: 28.5fps\n",
      "( 74) Video: GOT-10k_Test_000074 Time:   4.0s Speed: 27.7fps\n",
      "( 75) Video: GOT-10k_Test_000075 Time:   3.8s Speed: 26.6fps\n",
      "( 76) Video: GOT-10k_Test_000076 Time:   3.6s Speed: 27.5fps\n",
      "( 77) Video: GOT-10k_Test_000077 Time:   4.5s Speed: 28.7fps\n",
      "( 78) Video: GOT-10k_Test_000078 Time:   2.8s Speed: 27.9fps\n",
      "( 79) Video: GOT-10k_Test_000079 Time:   4.9s Speed: 28.1fps\n",
      "( 80) Video: GOT-10k_Test_000080 Time:   3.6s Speed: 28.1fps\n",
      "( 81) Video: GOT-10k_Test_000081 Time:   3.8s Speed: 26.2fps\n",
      "( 82) Video: GOT-10k_Test_000082 Time:   3.6s Speed: 22.4fps\n",
      "( 83) Video: GOT-10k_Test_000083 Time:   8.3s Speed: 9.6fps\n",
      "( 84) Video: GOT-10k_Test_000084 Time:   2.4s Speed: 27.3fps\n",
      "( 85) Video: GOT-10k_Test_000085 Time:   8.7s Speed: 23.9fps\n",
      "( 86) Video: GOT-10k_Test_000086 Time:   8.3s Speed: 17.9fps\n",
      "( 87) Video: GOT-10k_Test_000087 Time:   3.3s Speed: 28.4fps\n",
      "( 88) Video: GOT-10k_Test_000088 Time:   2.5s Speed: 28.0fps\n",
      "( 89) Video: GOT-10k_Test_000089 Time:   4.5s Speed: 28.4fps\n",
      "( 90) Video: GOT-10k_Test_000090 Time:   3.6s Speed: 27.8fps\n",
      "( 91) Video: GOT-10k_Test_000091 Time:  10.3s Speed: 27.1fps\n",
      "( 92) Video: GOT-10k_Test_000092 Time:  33.8s Speed: 27.2fps\n",
      "( 93) Video: GOT-10k_Test_000093 Time:   3.6s Speed: 27.5fps\n",
      "( 94) Video: GOT-10k_Test_000094 Time:   2.9s Speed: 27.8fps\n",
      "( 95) Video: GOT-10k_Test_000095 Time:   3.6s Speed: 27.8fps\n",
      "( 96) Video: GOT-10k_Test_000096 Time:   3.7s Speed: 26.4fps\n",
      "( 97) Video: GOT-10k_Test_000097 Time:   2.9s Speed: 28.5fps\n",
      "( 98) Video: GOT-10k_Test_000098 Time:   2.9s Speed: 27.2fps\n",
      "( 99) Video: GOT-10k_Test_000099 Time:   3.0s Speed: 29.8fps\n",
      "(100) Video: GOT-10k_Test_000100 Time:   3.0s Speed: 30.0fps\n",
      "(101) Video: GOT-10k_Test_000101 Time:   2.9s Speed: 30.6fps\n",
      "(102) Video: GOT-10k_Test_000102 Time:   3.3s Speed: 30.4fps\n",
      "(103) Video: GOT-10k_Test_000103 Time:   2.8s Speed: 25.3fps\n",
      "(104) Video: GOT-10k_Test_000104 Time:   3.5s Speed: 28.5fps\n",
      "(105) Video: GOT-10k_Test_000105 Time:   3.3s Speed: 30.4fps\n",
      "(106) Video: GOT-10k_Test_000106 Time:   3.3s Speed: 30.6fps\n",
      "(107) Video: GOT-10k_Test_000107 Time:   3.7s Speed: 21.0fps\n",
      "(108) Video: GOT-10k_Test_000108 Time:   3.5s Speed: 28.9fps\n",
      "(109) Video: GOT-10k_Test_000109 Time:   2.4s Speed: 29.1fps\n",
      "(110) Video: GOT-10k_Test_000110 Time:   4.3s Speed: 15.9fps\n",
      "(111) Video: GOT-10k_Test_000111 Time:  11.0s Speed: 28.0fps\n",
      "(112) Video: GOT-10k_Test_000112 Time:   5.5s Speed: 17.6fps\n",
      "(113) Video: GOT-10k_Test_000113 Time:   3.3s Speed: 30.3fps\n",
      "(114) Video: GOT-10k_Test_000114 Time:   3.8s Speed: 28.9fps\n",
      "(115) Video: GOT-10k_Test_000115 Time:   5.1s Speed: 27.5fps\n",
      "(116) Video: GOT-10k_Test_000116 Time:   5.1s Speed: 31.1fps\n",
      "(117) Video: GOT-10k_Test_000117 Time:  11.9s Speed: 29.4fps\n",
      "(118) Video: GOT-10k_Test_000118 Time:   2.6s Speed: 27.4fps\n",
      "(119) Video: GOT-10k_Test_000119 Time:   2.5s Speed: 27.7fps\n",
      "(120) Video: GOT-10k_Test_000120 Time:   6.5s Speed: 30.6fps\n",
      "(121) Video: GOT-10k_Test_000121 Time:   6.3s Speed: 30.2fps\n",
      "(122) Video: GOT-10k_Test_000122 Time:   3.6s Speed: 27.2fps\n",
      "(123) Video: GOT-10k_Test_000123 Time:   3.3s Speed: 30.0fps\n",
      "(124) Video: GOT-10k_Test_000124 Time:   2.6s Speed: 30.6fps\n",
      "(125) Video: GOT-10k_Test_000125 Time:   2.3s Speed: 30.3fps\n",
      "(126) Video: GOT-10k_Test_000126 Time:   2.6s Speed: 30.5fps\n",
      "(127) Video: GOT-10k_Test_000127 Time:   2.3s Speed: 30.5fps\n",
      "(128) Video: GOT-10k_Test_000128 Time:   2.7s Speed: 29.9fps\n",
      "(129) Video: GOT-10k_Test_000129 Time:   3.9s Speed: 30.5fps\n",
      "(130) Video: GOT-10k_Test_000130 Time:   3.9s Speed: 31.0fps\n",
      "(131) Video: GOT-10k_Test_000131 Time:   3.9s Speed: 28.0fps\n",
      "(132) Video: GOT-10k_Test_000132 Time:   3.4s Speed: 23.8fps\n",
      "(133) Video: GOT-10k_Test_000133 Time:   9.3s Speed: 31.0fps\n",
      "(134) Video: GOT-10k_Test_000134 Time:   9.9s Speed: 31.2fps\n",
      "(135) Video: GOT-10k_Test_000135 Time:   5.8s Speed: 25.6fps\n",
      "(136) Video: GOT-10k_Test_000136 Time:   7.2s Speed: 24.1fps\n",
      "(137) Video: GOT-10k_Test_000137 Time:  12.2s Speed: 28.7fps\n",
      "(138) Video: GOT-10k_Test_000138 Time:  25.4s Speed: 8.6fps\n",
      "(139) Video: GOT-10k_Test_000139 Time:   5.1s Speed: 29.2fps\n",
      "(140) Video: GOT-10k_Test_000140 Time:   4.6s Speed: 30.5fps\n",
      "(141) Video: GOT-10k_Test_000141 Time:   3.4s Speed: 29.0fps\n",
      "(142) Video: GOT-10k_Test_000142 Time:   7.1s Speed: 30.9fps\n",
      "(143) Video: GOT-10k_Test_000143 Time:   2.7s Speed: 30.1fps\n",
      "(144) Video: GOT-10k_Test_000144 Time:  10.0s Speed: 30.0fps\n",
      "(145) Video: GOT-10k_Test_000145 Time:   7.7s Speed: 31.0fps\n",
      "(146) Video: GOT-10k_Test_000146 Time:   4.2s Speed: 30.8fps\n",
      "(147) Video: GOT-10k_Test_000147 Time:   5.6s Speed: 30.2fps\n",
      "(148) Video: GOT-10k_Test_000148 Time:   3.8s Speed: 30.4fps\n",
      "(149) Video: GOT-10k_Test_000149 Time:   5.9s Speed: 30.3fps\n",
      "(150) Video: GOT-10k_Test_000150 Time:  27.9s Speed: 17.5fps\n",
      "(151) Video: GOT-10k_Test_000151 Time:   8.6s Speed: 29.1fps\n",
      "(152) Video: GOT-10k_Test_000152 Time:  18.6s Speed: 9.7fps\n",
      "(153) Video: GOT-10k_Test_000153 Time:   3.0s Speed: 29.6fps\n",
      "(154) Video: GOT-10k_Test_000154 Time:   3.1s Speed: 28.9fps\n",
      "(155) Video: GOT-10k_Test_000155 Time:  10.0s Speed: 31.1fps\n",
      "(156) Video: GOT-10k_Test_000156 Time:   8.4s Speed: 15.4fps\n",
      "(157) Video: GOT-10k_Test_000157 Time:   8.0s Speed: 29.8fps\n",
      "(158) Video: GOT-10k_Test_000158 Time:   6.8s Speed: 30.7fps\n",
      "(159) Video: GOT-10k_Test_000159 Time:   3.3s Speed: 30.4fps\n",
      "(160) Video: GOT-10k_Test_000160 Time:  11.7s Speed: 30.7fps\n",
      "(161) Video: GOT-10k_Test_000161 Time:   7.1s Speed: 29.7fps\n",
      "(162) Video: GOT-10k_Test_000162 Time:   3.3s Speed: 29.9fps\n",
      "(163) Video: GOT-10k_Test_000163 Time:   4.3s Speed: 30.3fps\n",
      "(164) Video: GOT-10k_Test_000164 Time:  21.8s Speed: 31.2fps\n",
      "(165) Video: GOT-10k_Test_000165 Time:   8.3s Speed: 24.0fps\n",
      "(166) Video: GOT-10k_Test_000166 Time:   2.5s Speed: 27.8fps\n",
      "(167) Video: GOT-10k_Test_000167 Time:   3.3s Speed: 30.2fps\n",
      "(168) Video: GOT-10k_Test_000168 Time:   2.7s Speed: 30.2fps\n",
      "(169) Video: GOT-10k_Test_000169 Time:   3.0s Speed: 29.8fps\n",
      "(170) Video: GOT-10k_Test_000170 Time:   3.1s Speed: 29.2fps\n",
      "(171) Video: GOT-10k_Test_000171 Time:   3.0s Speed: 29.5fps\n",
      "(172) Video: GOT-10k_Test_000172 Time:   2.7s Speed: 29.7fps\n",
      "(173) Video: GOT-10k_Test_000173 Time:   2.7s Speed: 29.9fps\n",
      "(174) Video: GOT-10k_Test_000174 Time:   2.9s Speed: 30.3fps\n",
      "(175) Video: GOT-10k_Test_000175 Time:   3.0s Speed: 30.3fps\n",
      "(176) Video: GOT-10k_Test_000176 Time:   3.3s Speed: 30.4fps\n",
      "(177) Video: GOT-10k_Test_000177 Time:   3.3s Speed: 30.1fps\n",
      "(178) Video: GOT-10k_Test_000178 Time:   3.9s Speed: 30.5fps\n",
      "(179) Video: GOT-10k_Test_000179 Time:   3.1s Speed: 28.8fps\n",
      "(180) Video: GOT-10k_Test_000180 Time:   3.0s Speed: 26.2fps\n"
     ]
    }
   ],
   "source": [
    "# pre-trained된 transt 모델로 GOT-10k 테스트\n",
    "!python3 -u ../pysot_toolkit/test.py --dataset 'GOT-10k' --name 'transt_eff' #test tracker #test tracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation\n",
    "- pysot_toolkit/eval.py를 이용해서 평가 가능\n",
    "- OTB, LaSOT, UAV, NFS, VOT2016~2019만 평가 가능\n",
    "- GOT-10k를 평가하려면 got10k_toolkit을 참조\n",
    "\n",
    "ex) python pysot_toolkit/eval.py --tracker_path results/ --dataset <name of dataset> --num 1 --tracker_prefix 'transt' #eval tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records saved at ../transt_eff.zip\n",
      "\u001b[93mLogin and follow instructions on\n",
      "http://got-10k.aitestunion.com/submit_instructions\n",
      "to upload and evaluate your tracking results\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python3 ../got10k_toolkit/toolkit/evaluate_got.py # got-10k 평가 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
