{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May 28 18:03:03 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:81:00.0 Off |                  N/A |\n",
      "| 39%   30C    P8    17W / 350W |      3MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce ...  On   | 00000000:C1:00.0 Off |                  N/A |\n",
      "| 39%   32C    P8    20W / 350W |      3MiB / 24268MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/sunghyunahn/anomaly_detection/avss_anomaly_detection/network')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "from torch import nn, einsum\n",
    "from einops import rearrange\n",
    "from pytorch_model_summary import summary\n",
    "from monai.networks.layers.utils import get_norm_layer\n",
    "from dynunet_block import get_conv_layer, UnetResBlock\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DSConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DSConv(nn.Module):\n",
    "    \"\"\"\n",
    "    Depthwise seperable convolution. \n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, dilation=1, spatial_dims=3):\n",
    "        super().__init__()\n",
    "\n",
    "        if spatial_dims == 3:\n",
    "            self.depthwise = nn.Conv3d(in_channels, in_channels, kernel_size, stride, \n",
    "                                padding, dilation, groups=in_channels, bias=False)\n",
    "            self.pointwise = nn.Conv3d(in_channels, out_channels, kernel_size=1, \n",
    "                                    stride=1, padding=0, dilation=1, groups=1, bias=False)\n",
    "            self.bn = nn.BatchNorm3d(out_channels, momentum=0.9997, eps=4e-5)\n",
    "\n",
    "        else:\n",
    "            self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size, stride, \n",
    "                                padding, dilation, groups=in_channels, bias=False)\n",
    "            self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, \n",
    "                                    stride=1, padding=0, dilation=1, groups=1, bias=False)\n",
    "            self.bn = nn.BatchNorm2d(out_channels, momentum=0.9997, eps=4e-5)\n",
    "\n",
    "        self.act = nn.ReLU()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.depthwise(inputs)\n",
    "        x = self.pointwise(x)\n",
    "        x = self.bn(x)\n",
    "        return self.act(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------\n",
      "      Layer (type)          Output Shape         Param #     Tr. Param #\n",
      "=========================================================================\n",
      "          Conv3d-1     [1, 384, 4, 8, 8]          10,368          10,368\n",
      "          Conv3d-2     [1, 384, 4, 8, 8]         147,456         147,456\n",
      "     BatchNorm3d-3     [1, 384, 4, 8, 8]             768             768\n",
      "            ReLU-4     [1, 384, 4, 8, 8]               0               0\n",
      "=========================================================================\n",
      "Total params: 158,592\n",
      "Trainable params: 158,592\n",
      "Non-trainable params: 0\n",
      "-------------------------------------------------------------------------\n",
      "input: torch.Size([1, 384, 4, 8, 8])\n",
      "output: torch.Size([1, 384, 4, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "x=torch.zeros(1,384,4,8,8).cuda() # [B,C,D,H,W] input: 8 x 8 x 4 x 384\n",
    "model=DSConv(in_channels=x.shape[1], out_channels=x.shape[1]).to(device)\n",
    "\n",
    "print(summary(model,x))\n",
    "print('input:',x.shape)\n",
    "print('output:',model(x).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patch Embedding  \n",
    "(Partition+Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, spatial_dims=3, in_channels=3, out_channels=24, kernel_size=(1,4,4), stride=(1,4,4), dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.conv=get_conv_layer(spatial_dims, in_channels, out_channels, kernel_size, stride, dropout, conv_only=True)\n",
    "        self.norm=get_norm_layer(name=(\"group\", {\"num_groups\": in_channels}), channels=out_channels)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.conv(x)\n",
    "        x=self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "      Layer (type)           Output Shape         Param #     Tr. Param #\n",
      "==========================================================================\n",
      "          Conv3d-1     [1, 24, 4, 64, 64]           1,152           1,152\n",
      "       GroupNorm-2     [1, 24, 4, 64, 64]              48              48\n",
      "==========================================================================\n",
      "Total params: 1,200\n",
      "Trainable params: 1,200\n",
      "Non-trainable params: 0\n",
      "--------------------------------------------------------------------------\n",
      "input: torch.Size([1, 3, 4, 256, 256])\n",
      "output: torch.Size([1, 24, 4, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "x=torch.zeros(1,3,4,256,256).cuda() # [B,C,D,H,W] input: 256 x 256 x 4 x 3\n",
    "model=PatchEmbedding().to(device)\n",
    "\n",
    "print(summary(model,x))\n",
    "print('input:',x.shape)\n",
    "print('output:',model(x).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Downsample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, spatial_dims=3, kernel_size=(1,2,2), stride=(1,2,2), dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.conv=get_conv_layer(spatial_dims, in_channels, out_channels, kernel_size, stride, dropout, conv_only=True)\n",
    "        self.norm=get_norm_layer(name=(\"group\", {\"num_groups\": in_channels}), channels=out_channels)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.conv(x)\n",
    "        x=self.norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "      Layer (type)           Output Shape         Param #     Tr. Param #\n",
      "==========================================================================\n",
      "          Conv3d-1     [1, 48, 4, 32, 32]           4,608           4,608\n",
      "       GroupNorm-2     [1, 48, 4, 32, 32]              96              96\n",
      "==========================================================================\n",
      "Total params: 4,704\n",
      "Trainable params: 4,704\n",
      "Non-trainable params: 0\n",
      "--------------------------------------------------------------------------\n",
      "input: torch.Size([1, 24, 4, 64, 64])\n",
      "output: torch.Size([1, 48, 4, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "x=torch.zeros(1,24,4,64,64).cuda() # [B,C,D,H,W] input: 64 x 64 x 4 x 24\n",
    "model=Downsample(in_channels=x.shape[1], out_channels=x.shape[1]*2).to(device)\n",
    "\n",
    "print(summary(model,x))\n",
    "print('input:',x.shape)\n",
    "print('output:',model(x).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Upsample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, spatial_dims=2, kernel_size=(2,2), stride=(2,2), dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.deconv=get_conv_layer(spatial_dims, in_channels, out_channels, kernel_size, stride, dropout, conv_only=True, is_transposed=True)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.deconv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------\n",
      "        Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=========================================================================\n",
      "   ConvTranspose2d-1     [1, 24, 64, 64]           4,608           4,608\n",
      "=========================================================================\n",
      "Total params: 4,608\n",
      "Trainable params: 4,608\n",
      "Non-trainable params: 0\n",
      "-------------------------------------------------------------------------\n",
      "input: torch.Size([1, 48, 32, 32])\n",
      "output: torch.Size([1, 24, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "x=torch.zeros(1,48,32,32).cuda() # [B,C,H,W] input: 32 x 32 x 48\n",
    "model=Upsample(in_channels=x.shape[1], out_channels=x.shape[1]//2).to(device)\n",
    "\n",
    "print(summary(model,x))\n",
    "print('input:',x.shape)\n",
    "print('output:',model(x).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video Patch Merging (1,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoPatchMerging(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        '''\n",
    "        we assume that h,w,d are even numbers.\n",
    "        out_channels = 2 * in_channels.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.dim = in_channels\n",
    "        self.reduction = nn.Linear(4 * in_channels, 2 * in_channels, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        x: B,C,D,H,W\n",
    "        '''\n",
    "        x = x.permute(0,2,3,4,1) # [B, D, H, W, C]\n",
    "\n",
    "        x0 = x[:, :, 0::2, 0::2, :]  # [B, D, H/2, W/2, C]\n",
    "        x1 = x[:, :, 1::2, 0::2, :] \n",
    "        x2 = x[:, :, 0::2, 1::2, :]  \n",
    "        x3 = x[:, :, 1::2, 1::2, :]  \n",
    "        x = torch.cat([x0, x1, x2, x3], -1)  # [B, D, H/2, W/2, 4C]\n",
    "\n",
    "        x = self.reduction(x) # [B, D, H/2, W/2, 2C]\n",
    "        x = x.permute(0, 4, 1, 2, 3) # [B, 2C, D, H/2, W/2]\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "      Layer (type)           Output Shape         Param #     Tr. Param #\n",
      "==========================================================================\n",
      "          Linear-1     [1, 4, 32, 32, 48]           4,608           4,608\n",
      "==========================================================================\n",
      "Total params: 4,608\n",
      "Trainable params: 4,608\n",
      "Non-trainable params: 0\n",
      "--------------------------------------------------------------------------\n",
      "input: torch.Size([1, 24, 4, 64, 64])\n",
      "output: torch.Size([1, 48, 4, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "x=torch.zeros(1,24,4,64,64).cuda() # [B,C,D,H,W] input: 64 x 64 x 4 x 24\n",
    "model=VideoPatchMerging(in_channels=x.shape[1]).to(device)\n",
    "\n",
    "print(summary(model,x))\n",
    "print('input:',x.shape)\n",
    "print('output:',model(x).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patch Expanding (2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchExpanding(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.expand = nn.Linear(in_channels, 2 * in_channels, bias=False)\n",
    "\n",
    "    def forward(self, y):\n",
    "        \"\"\"\n",
    "        y: B,C,H,W\n",
    "        \"\"\"\n",
    "        y=y.permute(0,2,3,1) # [B, H, W, C]\n",
    "        B, H, W, C = y.size()\n",
    "\n",
    "        y=self.expand(y) # B, H, W, 2*C\n",
    "    \n",
    "        y=rearrange(y,'b h w (p1 p2 c)-> b (h p1) (w p2) c', p1=2, p2=2, c=C//2) # B, 2*H, 2*W, C//2\n",
    "\n",
    "        y=y.permute(0,3,1,2) # B, C//2, 2*H, 2*W\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "      Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=======================================================================\n",
      "          Linear-1     [1, 32, 32, 96]           4,608           4,608\n",
      "=======================================================================\n",
      "Total params: 4,608\n",
      "Trainable params: 4,608\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n",
      "input: torch.Size([1, 48, 32, 32])\n",
      "output: torch.Size([1, 24, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "x=torch.zeros(1,48,32,32).cuda() # [B,C,H,W] input: 32 x 32 x 48\n",
    "model=PatchExpanding(in_channels=x.shape[1]).to(device)\n",
    "\n",
    "print(summary(model,x))\n",
    "print('input:',x.shape)\n",
    "print('output:',model(x).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Globalpool(nn.Module):\n",
    "    def __init__(self, height, width):\n",
    "        super().__init__()\n",
    "        self.pool=nn.AdaptiveMaxPool3d((1, height, width))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.pool(x) # [B,C,1,H,W]\n",
    "        x=x.squeeze(2) # [B,C,H,W]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------\n",
      "          Layer (type)           Output Shape         Param #     Tr. Param #\n",
      "==============================================================================\n",
      "   AdaptiveMaxPool3d-1     [1, 24, 1, 64, 64]               0               0\n",
      "==============================================================================\n",
      "Total params: 0\n",
      "Trainable params: 0\n",
      "Non-trainable params: 0\n",
      "------------------------------------------------------------------------------\n",
      "input: torch.Size([1, 24, 4, 64, 64])\n",
      "output: torch.Size([1, 24, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "x=torch.zeros(1,24,4,64,64).cuda() # [B,C,D,H,W] input: 64 x 64 x 4 x 24\n",
    "model=Globalpool(height=x.shape[3], width=x.shape[4]).to(device)\n",
    "\n",
    "print(summary(model,x))\n",
    "print('input:',x.shape)\n",
    "print('output:',model(x).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, spatial_dims=2, kernel_size=3, stride=1, norm_name=\"instance\",depth=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.depth = depth \n",
    "        self.resblock_set = nn.ModuleList()\n",
    "\n",
    "        for i in range(depth):\n",
    "            if i==0:\n",
    "                self.resblock_set.append(UnetResBlock(spatial_dims=spatial_dims, in_channels=in_channels, out_channels=out_channels, \n",
    "                                        kernel_size=kernel_size, stride=stride, norm_name=norm_name))\n",
    "            else:\n",
    "                self.resblock_set.append(UnetResBlock(spatial_dims=spatial_dims, in_channels=out_channels, out_channels=out_channels, \n",
    "                         kernel_size=kernel_size, stride=stride, norm_name=norm_name))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        for i in range(self.depth):\n",
    "            x = self.resblock_set[i](x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "      Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=======================================================================\n",
      "    UnetResBlock-1     [1, 12, 64, 64]           4,176           4,176\n",
      "    UnetResBlock-2     [1, 12, 64, 64]           2,592           2,592\n",
      "=======================================================================\n",
      "Total params: 6,768\n",
      "Trainable params: 6,768\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n",
      "input: torch.Size([1, 24, 64, 64])\n",
      "output: torch.Size([1, 12, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "x=torch.zeros(1,24,64,64).cuda() # [B,C,H,W] input: 64 x 64 x 24\n",
    "model=ResBlock(in_channels=x.shape[1], out_channels=x.shape[1]//2, depth=2).to(device)\n",
    "\n",
    "print(summary(model,x))\n",
    "print('input:',x.shape)\n",
    "print('output:',model(x).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConcatConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConcatConv(nn.Module):\n",
    "    def __init__(self, in_channels, depth=1):\n",
    "        super().__init__()\n",
    "        self.conv = ResBlock(in_channels=in_channels*2, out_channels=in_channels, depth=depth)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        '''\n",
    "        x1, x2: [B, C, H, W]\n",
    "        '''\n",
    "        x = torch.cat((x1,x2),dim=1)\n",
    "        x = self.conv(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "      Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=======================================================================\n",
      "        ResBlock-1     [1, 24, 64, 64]          16,704          16,704\n",
      "=======================================================================\n",
      "Total params: 16,704\n",
      "Trainable params: 16,704\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n",
      "input: torch.Size([1, 24, 64, 64]) torch.Size([1, 24, 64, 64])\n",
      "output: torch.Size([1, 24, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "x1=torch.zeros(1,24,64,64).cuda() # [B,C,H,W] input: 64 x 64 x 24\n",
    "x2=torch.zeros(1,24,64,64).cuda()\n",
    "\n",
    "model=ConcatConv(in_channels=24).to(device)\n",
    "\n",
    "print(summary(model,x1,x2))\n",
    "print('input:',x1.shape, x2.shape)\n",
    "print('output:',model(x1,x2).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=3, spatial_dims=2, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.conv = ResBlock(in_channels=in_channels, out_channels=in_channels)\n",
    "        self.head=get_conv_layer(spatial_dims, in_channels, out_channels, kernel_size=1, dropout=dropout, bias=True, conv_only=True)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.conv(x)\n",
    "        x=self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------\n",
      "      Layer (type)          Output Shape         Param #     Tr. Param #\n",
      "=========================================================================\n",
      "        ResBlock-1     [1, 12, 256, 256]           5,184           5,184\n",
      "          Conv2d-2      [1, 3, 256, 256]              39              39\n",
      "=========================================================================\n",
      "Total params: 5,223\n",
      "Trainable params: 5,223\n",
      "Non-trainable params: 0\n",
      "-------------------------------------------------------------------------\n",
      "input: torch.Size([1, 12, 256, 256])\n",
      "output: torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "x=torch.zeros(1,12,256,256).cuda() # [B,C,H,W] input: 256 x 256 x 12\n",
    "model=Head(in_channels=x.shape[1]).to(device)\n",
    "\n",
    "print(summary(model,x))\n",
    "print('input:',x.shape)\n",
    "print('output:',model(x).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Encoder] Spatial Attention (3D)  \n",
    "-> Q, K, V : HW x DC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAttn(nn.Module):\n",
    "    def __init__(self, input_size, dim, num_heads=4, qkv_bias=False, attn_drop=0.1, proj_drop=0.1):\n",
    "        '''\n",
    "        input_size: resolution (H*W)\n",
    "        dim: channel * depth (C*D)\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
    "\n",
    "        # qkv are 3 linear layers (query, key, value)\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop) \n",
    "\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "    \n",
    "    def forward(self, x):        \n",
    "        '''\n",
    "        Spatial Attention\n",
    "        : no projection \n",
    "\n",
    "        x: [B, HW, DC] \n",
    "        '''\n",
    "        B, HW, DC = x.shape \n",
    "\n",
    "        qkv = self.qkv(x).reshape(B, HW, 3, self.num_heads, DC // self.num_heads) # B x HW x 3 x h x C/h\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4) # 3 x B x h x HW x DC/h\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2] # B x h x HW x DC/h\n",
    "\n",
    "        q = torch.nn.functional.normalize(q, dim=-2)\n",
    "        k = torch.nn.functional.normalize(k, dim=-2)\n",
    "        k_t = k.permute(0, 1, 3, 2) # K_T : B x h x DC/h x HW\n",
    "\n",
    "        attn_SA = (q @ k_t) * self.temperature  # [Q x K_T] B x h x HW x HW\n",
    "        \n",
    "        attn_SA = attn_SA.softmax(dim=-1)\n",
    "        attn_SA = self.attn_drop(attn_SA) # [Spatial Attn Map] B x h x HW x HW\n",
    "        \n",
    "        # [Spatial Attn Map x V] B x h x HW x DC/h -> B x HW x h x DC/h -> B x HW x DC\n",
    "        x_SA = (attn_SA @ v).permute(0, 2, 1, 3).reshape(B, HW, DC) \n",
    "        \n",
    "        # linear projection for msa\n",
    "        x = self.proj(x_SA)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAttnBlock(nn.Module):\n",
    "     def __init__(self, conv_hidden, input_size, dim, num_heads=4, qkv_bias=False, attn_drop=0.1, proj_drop=0.1,is_pos_embed=False):\n",
    "          '''\n",
    "          input_size: resolution (H*W)\n",
    "          dim: channel * depth (C*D)\n",
    "          '''\n",
    "          super().__init__()\n",
    "\n",
    "          self.norm = nn.LayerNorm(dim)\n",
    "          self.is_pos_embed = is_pos_embed\n",
    "          self.pos_embed = nn.Parameter(torch.zeros(1, input_size, dim))\n",
    "          self.spatial_attn = SpatialAttn(input_size, dim, num_heads, qkv_bias, attn_drop, proj_drop)\n",
    "          self.dsconv = DSConv(in_channels=conv_hidden, out_channels=conv_hidden)\n",
    "\n",
    "     def forward(self,x):\n",
    "          '''\n",
    "          x: [B, C, D, H, W]\n",
    "          '''\n",
    "          B, C, D, H, W = x.shape\n",
    "          save = x\n",
    "          \n",
    "          x = rearrange(x,'b c d h w-> b (h w) (c d)', b=B, c=C, d=D, h=H, w=W) # [B,HW,DC]\n",
    "          if self.is_pos_embed:\n",
    "               x = x + self.pos_embed\n",
    "\n",
    "          # spatial attn -> norm\n",
    "          x = self.norm(self.spatial_attn(x))\n",
    "          x = rearrange(x,'b (h w) (c d)-> b c d h w', b=B, c=C, d=D, h=H, w=W) # [B,C,D,H,W]\n",
    "          x += save\n",
    "\n",
    "          # conv -> norm\n",
    "          x += self.dsconv(x)\n",
    "\n",
    "          return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "      Layer (type)           Output Shape         Param #     Tr. Param #\n",
      "==========================================================================\n",
      "     SpatialAttn-1          [1, 4096, 96]          36,964          36,964\n",
      "       LayerNorm-2          [1, 4096, 96]             192             192\n",
      "          DSConv-3     [1, 24, 4, 64, 64]           1,272           1,272\n",
      "==========================================================================\n",
      "Total params: 38,428\n",
      "Trainable params: 38,428\n",
      "Non-trainable params: 0\n",
      "--------------------------------------------------------------------------\n",
      "input: torch.Size([1, 24, 4, 64, 64])\n",
      "output: torch.Size([1, 24, 4, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "x=torch.zeros(1,24,4,64,64).cuda() # [B,C,D,H,W] input: 64 x 64 x 4 x 24\n",
    "model=SpatialAttnBlock(conv_hidden=x.shape[1], input_size=x.shape[3]*x.shape[4], dim=x.shape[1]*x.shape[2],is_pos_embed=True).to(device)\n",
    "\n",
    "print(summary(model,x))\n",
    "print('input:',x.shape)\n",
    "print('output:',model(x).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Encoder] Temporal Attention (3D)  \n",
    "-> Q, K, V : D x HWC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalAttn(nn.Module):\n",
    "    def __init__(self, input_size, dim, proj_size, squeeze=8, num_heads=4, qkv_bias=False, attn_drop=0.1, proj_drop=0.1):\n",
    "        '''\n",
    "        input_size: depth (D)\n",
    "        dim: resolution * channel (H*W*C)\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.dim = dim\n",
    "        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
    "\n",
    "        # qkv are 3 linear layers (query, key, value)\n",
    "        # we use bottlenect architecture for efficient calculation!!\n",
    "        self.qkv = nn.Sequential(\n",
    "            nn.Linear(dim, squeeze, bias=qkv_bias),\n",
    "            nn.Linear(squeeze, proj_size, bias=qkv_bias),\n",
    "            nn.Linear(proj_size,squeeze, bias=qkv_bias),\n",
    "            nn.Linear(squeeze, 3*dim, bias=qkv_bias),\n",
    "        )\n",
    "\n",
    "        # projection matrices with shared weights used in attention module to project\n",
    "        self.proj_size = proj_size\n",
    "        self.proj_q = self.proj_k = nn.Linear(dim, proj_size)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop) \n",
    "\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(dim, squeeze, bias=qkv_bias),\n",
    "            nn.Linear(squeeze, proj_size, bias=qkv_bias),\n",
    "            nn.Linear(proj_size,squeeze, bias=qkv_bias),\n",
    "            nn.Linear(squeeze, dim, bias=qkv_bias),\n",
    "        )\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "    \n",
    "    def forward(self, x):        \n",
    "        '''\n",
    "        Temporal Attention\n",
    "        : Q -> Q(p), K -> K(p) [ Q(p) x K_T(p) ]\n",
    "\n",
    "        x: [B, D, HWC] \n",
    "        '''\n",
    "        B, D, HWC = x.shape \n",
    "\n",
    "        qkv = self.qkv(x).reshape(B, D, 3, HWC).permute(2,0,1,3) # B x D x 3 x HWC -> 3 x B x D x HWC\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2] # B x D x HWC\n",
    "\n",
    "        q_projected = self.proj_q(q) # B x D x P\n",
    "        k_projected = self.proj_k(k) # B x D x p\n",
    "\n",
    "        q_projected = q_projected.reshape(B, D, self.num_heads, self.proj_size // self.num_heads).permute(0,2,1,3) # B x D x h x P/h -> B x h x D x P/h\n",
    "        k_projected = k_projected.reshape(B, D, self.num_heads, self.proj_size // self.num_heads).permute(0,2,1,3) # B x D x h x P/h -> B x h x D x P/h\n",
    "        v = v.reshape(B, D, self.num_heads, self.dim // self.num_heads).permute(0,2,1,3) # B x D x h x HWC/h -> B x h x D x HWC/h\n",
    "\n",
    "        q_projected = torch.nn.functional.normalize(q_projected, dim=-2)\n",
    "        k_projected = torch.nn.functional.normalize(k_projected, dim=-2)\n",
    "        k_t_projected = k_projected.transpose(-2, -1) # K_T : B x h x P/h x D\n",
    "\n",
    "        attn_TA = (q_projected @ k_t_projected) * self.temperature  # [Q(p) x K_T(p)] B x h x D x D\n",
    "        \n",
    "        attn_TA = attn_TA.softmax(dim=-1)\n",
    "        attn_TA = self.attn_drop(attn_TA) # [Temporal Attn Map] B x h x D x D\n",
    "        \n",
    "        # [Temporal Attn Map x V(p)] B x h x D x HWC/h -> B x D x h x HWC/h -> B x D x HWC\n",
    "        x_TA = (attn_TA @ v).permute(0, 2, 1, 3).reshape(B, D, HWC) \n",
    "        \n",
    "        # linear projection for msa\n",
    "        x = self.proj(x_TA)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalAttnBlock(nn.Module):\n",
    "     def __init__(self, conv_hidden, input_size, dim, proj_size=64, squeeze=8, num_heads=4, qkv_bias=False, attn_drop=0.1, proj_drop=0.1, is_pos_embed=False):\n",
    "          '''\n",
    "          input_size: depth (D)\n",
    "          dim: resolution * channel (H*W*C)\n",
    "          '''\n",
    "          super().__init__()         \n",
    "\n",
    "          self.norm = nn.LayerNorm(dim)\n",
    "          self.is_pos_embed = is_pos_embed\n",
    "          self.pos_embed = nn.Parameter(torch.zeros(1, input_size, dim))\n",
    "          self.temporal_attn = TemporalAttn(input_size, dim, proj_size, squeeze, num_heads, qkv_bias, attn_drop, proj_drop)\n",
    "          self.dsconv = DSConv(in_channels=conv_hidden, out_channels=conv_hidden)\n",
    "\n",
    "     def forward(self,x):\n",
    "          '''\n",
    "          x: [B, C, D, H, W]\n",
    "          '''\n",
    "          B, C, D, H, W = x.shape\n",
    "          save = x\n",
    "          \n",
    "          x = rearrange(x,'b c d h w-> b d (h w c)', b=B, c=C, d=D, h=H, w=W) # [B,D,HWC]\n",
    "          if self.is_pos_embed:\n",
    "            x = x + self.pos_embed\n",
    "\n",
    "          # temporal attn -> norm\n",
    "          x = self.norm(self.temporal_attn(x))\n",
    "          x = rearrange(x,'b d (h w c)-> b c d h w', b=B, c=C, d=D, h=H, w=W) # [B,C,D,H,W]\n",
    "          x += save\n",
    "\n",
    "          # conv -> norm\n",
    "          x += self.dsconv(x)\n",
    "        \n",
    "          return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "      Layer (type)           Output Shape         Param #     Tr. Param #\n",
      "==========================================================================\n",
      "    TemporalAttn-1          [1, 4, 98304]      11,012,164      11,012,164\n",
      "       LayerNorm-2          [1, 4, 98304]         196,608         196,608\n",
      "          DSConv-3     [1, 24, 4, 64, 64]           1,272           1,272\n",
      "==========================================================================\n",
      "Total params: 11,210,044\n",
      "Trainable params: 11,210,044\n",
      "Non-trainable params: 0\n",
      "--------------------------------------------------------------------------\n",
      "input: torch.Size([1, 24, 4, 64, 64])\n",
      "output: torch.Size([1, 24, 4, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "x=torch.zeros(1,24,4,64,64).cuda() # [B,C,D,H,W] input: 64 x 64 x 4 x 24\n",
    "model=TemporalAttnBlock(conv_hidden=x.shape[1], input_size=x.shape[2], dim=x.shape[1]*x.shape[3]*x.shape[4], is_pos_embed=True).to(device)\n",
    "\n",
    "print(summary(model,x))\n",
    "print('input:',x.shape)\n",
    "print('output:',model(x).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Encoder] Spatio-Temporal Attention (3D)  \n",
    "-> Q, K, V : HWD x C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatioTemporalAttn(nn.Module):\n",
    "    def __init__(self, input_size, dim, proj_size, num_heads=4, qkv_bias=False, attn_drop=0., proj_drop=0.1):\n",
    "        super().__init__()\n",
    "        '''\n",
    "        input_size: resolution * depth (H*W*D)\n",
    "        dim: channel (C)\n",
    "        '''\n",
    "        self.num_heads = num_heads\n",
    "        self.dim = dim\n",
    "        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
    "\n",
    "        # qkv are 3 linear layers (query, key, value)\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "\n",
    "        # projection matrices with shared weights used in attention module to project\n",
    "        self.proj_k = self.proj_v = nn.Linear(input_size, proj_size)\n",
    "        self.attn_drop = nn.Dropout(attn_drop) \n",
    "\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, HWD, C = x.shape \n",
    "        \n",
    "        '''\n",
    "        Spatio-Temporal Attention\n",
    "        : K -> K(p), V -> V(p) [ Q x K_T(p) ]\n",
    "        '''\n",
    "        qkv = self.qkv(x).reshape(B, HWD, 3, self.num_heads, C // self.num_heads) # B x HWD x 3 x h x C/h\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4) # 3 x B x h x HWD x C/h\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2] # B x h x HWD x C/h\n",
    "\n",
    "        q = torch.nn.functional.normalize(q, dim=-2)\n",
    "        k = torch.nn.functional.normalize(k, dim=-2)\n",
    "\n",
    "        k_t = k.transpose(-2, -1) # B x h x C/h x HWD\n",
    "        v_t = v.transpose(-2, -1) # B x h x C/h x HWD\n",
    "\n",
    "        k_t_projected = self.proj_k(k_t) # B x h x C/h x p\n",
    "        v_t_projected = self.proj_v(v_t) # B x h x C/h x p\n",
    "\n",
    "        attn_STA = (q @ k_t_projected) * self.temperature  # [Q x K_T(p)] B x h x HWD x p\n",
    "        \n",
    "        attn_STA = attn_STA.softmax(dim=-1)\n",
    "        attn_STA = self.attn_drop(attn_STA) # [Spatial-Temporal Attn Map] B x h x HWD x p\n",
    "        \n",
    "        v_projected = v_t_projected.transpose(-2, -1) # V(p) : B x h x p x C/h\n",
    "\n",
    "        # [Spatio-Temporal Attn Map x V] B x h x HWD x C/h -> B x HWD x h x C/h -> B x HWD x C\n",
    "        x_STA = (attn_STA @ v_projected).permute(0, 2, 1, 3).reshape(B, HWD, C) \n",
    "        \n",
    "        # linear projection for msa\n",
    "        x = self.proj(x_STA)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatioTemporalAttnBlock(nn.Module):\n",
    "     def __init__(self, conv_hidden, input_size, dim, proj_size=64, num_heads=4, qkv_bias=False, attn_drop=0.1, proj_drop=0.1,is_pos_embed=False):\n",
    "          '''\n",
    "          input_size: resolution * depth (H*W*D)\n",
    "          dim: channel (C)\n",
    "          '''\n",
    "          super().__init__()\n",
    "\n",
    "          self.norm = nn.LayerNorm(dim)\n",
    "          self.is_pos_embed = is_pos_embed\n",
    "          self.pos_embed = nn.Parameter(torch.zeros(1, input_size, dim))\n",
    "          self.spatio_temporal_attn = SpatioTemporalAttn(input_size, dim, proj_size, num_heads, qkv_bias, attn_drop, proj_drop)\n",
    "          self.dsconv = DSConv(in_channels=conv_hidden, out_channels=conv_hidden)\n",
    "\n",
    "     def forward(self,x):\n",
    "          '''\n",
    "          x: [B, C, D, H, W]\n",
    "          '''\n",
    "          B, C, D, H, W = x.shape\n",
    "          save = x\n",
    "          \n",
    "          x = rearrange(x,'b c d h w-> b (h w d) c', b=B, c=C, d=D, h=H, w=W) # [B,HWD,C]\n",
    "          if self.is_pos_embed:\n",
    "               x = x + self.pos_embed\n",
    "\n",
    "          # spatio temporal attn -> norm\n",
    "          x = self.norm(self.spatio_temporal_attn(x))\n",
    "          x = rearrange(x,'b (h w d) c-> b c d h w', b=B, c=C, d=D, h=H, w=W) # [B,C,D,H,W]\n",
    "          x += save\n",
    "\n",
    "          # conv -> norm\n",
    "          x += self.dsconv(x)\n",
    "\n",
    "          return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------\n",
      "           Layer (type)           Output Shape         Param #     Tr. Param #\n",
      "===============================================================================\n",
      "   SpatioTemporalAttn-1         [2, 16384, 24]       1,050,972       1,050,972\n",
      "            LayerNorm-2         [2, 16384, 24]              48              48\n",
      "               DSConv-3     [2, 24, 4, 64, 64]           1,272           1,272\n",
      "===============================================================================\n",
      "Total params: 1,052,292\n",
      "Trainable params: 1,052,292\n",
      "Non-trainable params: 0\n",
      "-------------------------------------------------------------------------------\n",
      "input: torch.Size([2, 24, 4, 64, 64])\n",
      "output: torch.Size([2, 24, 4, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "x=torch.zeros(2,24,4,64,64).cuda() # [B,C,D,H,W] input: 64 x 64 x 4 x 24\n",
    "model=SpatioTemporalAttnBlock(conv_hidden=x.shape[1], input_size=x.shape[2]*x.shape[3]*x.shape[4], dim=x.shape[1], is_pos_embed=True).to(device)\n",
    "\n",
    "print(summary(model,x))\n",
    "print('input:',x.shape)\n",
    "print('output:',model(x).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Encoder] Channel Attention (3D)  \n",
    "-> Q, K, V : C x HWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttn(nn.Module):\n",
    "    def __init__(self, input_size, dim, proj_size, num_heads=4, qkv_bias=False, attn_drop=0.1, proj_drop=0.1):\n",
    "        '''\n",
    "        input_size: channel (C)\n",
    "        dim: resolution * Depth (H*W*D)\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.dim = dim\n",
    "        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
    "\n",
    "        # qkv are 3 linear layers (query, key, value)\n",
    "        self.qkv = nn.Sequential(\n",
    "            nn.Linear(dim, proj_size, bias=qkv_bias),\n",
    "            nn.Linear(proj_size, 3*dim, bias=qkv_bias),\n",
    "        )\n",
    "\n",
    "        # projection matrices with shared weights used in attention module to project\n",
    "        self.proj_size = proj_size\n",
    "        self.proj_q = self.proj_k = nn.Linear(dim, proj_size)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop) \n",
    "\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(dim, proj_size, bias=qkv_bias),\n",
    "            nn.Linear(proj_size, dim, bias=qkv_bias),\n",
    "        )\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "    \n",
    "    def forward(self, x):        \n",
    "        '''\n",
    "        Channel Attention\n",
    "        : Q -> Q(p), K -> K(p) [ Q(p) x K_T(p) ]\n",
    "\n",
    "        x: [B, C, HWD] \n",
    "        '''\n",
    "        B, C, HWD = x.shape \n",
    "\n",
    "        qkv = self.qkv(x).reshape(B, C, 3, HWD).permute(2,0,1,3) # B x C x 3 x HWD -> 3 x B x C x HWD\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2] # B x C x HWD\n",
    "\n",
    "        q_projected = self.proj_q(q) # B x C x P\n",
    "        k_projected = self.proj_k(k) # B x C x p\n",
    "\n",
    "        q_projected = q_projected.reshape(B, C, self.num_heads, self.proj_size // self.num_heads).permute(0,2,1,3) # B x C x h x P/h -> B x h x C x P/h\n",
    "        k_projected = k_projected.reshape(B, C, self.num_heads, self.proj_size // self.num_heads).permute(0,2,1,3) # B x C x h x P/h -> B x h x C x P/h\n",
    "        v = v.reshape(B, C, self.num_heads, self.dim // self.num_heads).permute(0,2,1,3) # B x C x h x HWD/h -> B x h x C x HWD/h\n",
    "\n",
    "        q_projected = torch.nn.functional.normalize(q_projected, dim=-2)\n",
    "        k_projected = torch.nn.functional.normalize(k_projected, dim=-2)\n",
    "        k_t_projected = k_projected.transpose(-2, -1) # K_T : B x h x P/h x C\n",
    "\n",
    "        attn_CA = (q_projected @ k_t_projected)   # [Q(p) x K_T(p)] B x h x C x C\n",
    "        attn_CA = attn_CA * self.temperature\n",
    "        \n",
    "        attn_CA = attn_CA.softmax(dim=-1)\n",
    "        attn_CA = self.attn_drop(attn_CA) # [Channel Attn Map] B x h x C x C\n",
    "\n",
    "        # [Channel Attn Map x V(p)] B x h x C x HWD/h -> B x C x h x HWD/h -> B x C x HWD\n",
    "        x_CA = (attn_CA @ v).permute(0, 2, 1, 3).reshape(B, C, HWD) \n",
    "        \n",
    "        # linear projection for msa\n",
    "        x = self.proj(x_CA)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChannelAttnBlock(nn.Module):\n",
    "     def __init__(self, conv_hidden, input_size, dim, proj_size=64, num_heads=4, qkv_bias=False, attn_drop=0.1, proj_drop=0.1,is_pos_embed=False):\n",
    "          '''\n",
    "          input_size: channel (C)\n",
    "          dim: resolution * Depth (H*W*D)\n",
    "          '''\n",
    "          super().__init__()\n",
    "        \n",
    "          self.norm = nn.LayerNorm(dim)\n",
    "          self.is_pos_embed = is_pos_embed\n",
    "          self.pos_embed = nn.Parameter(torch.zeros(1, input_size, dim))\n",
    "          self.channel_attn = ChannelAttn(input_size, dim, proj_size, num_heads, qkv_bias, attn_drop, proj_drop)\n",
    "          self.dsconv = DSConv(in_channels=conv_hidden, out_channels=conv_hidden)\n",
    "\n",
    "     def forward(self,x):\n",
    "          '''\n",
    "          x: [B, C, D, H, W]\n",
    "          '''\n",
    "          B, C, D, H, W = x.shape\n",
    "          save = x\n",
    "\n",
    "          x = rearrange(x,'b c d h w-> b c (h w d)', b=B, c=C, d=D, h=H, w=W) # [B,C,HWD]\n",
    "          if self.is_pos_embed:\n",
    "               x = x + self.pos_embed\n",
    "          \n",
    "          # channel attn -> norm\n",
    "          x = self.norm(self.channel_attn(x))\n",
    "          x = rearrange(x,'b c (h w d)-> b c d h w', b=B, c=C, d=D, h=H, w=W) # [B,C,D,H,W]\n",
    "          x += save\n",
    "\n",
    "          # conv -> norm\n",
    "          x += self.dsconv(x)\n",
    "\n",
    "          return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "      Layer (type)           Output Shape         Param #     Tr. Param #\n",
      "==========================================================================\n",
      "     ChannelAttn-1         [1, 24, 16384]       7,340,100       7,340,100\n",
      "       LayerNorm-2         [1, 24, 16384]          32,768          32,768\n",
      "          DSConv-3     [1, 24, 4, 64, 64]           1,272           1,272\n",
      "==========================================================================\n",
      "Total params: 7,374,140\n",
      "Trainable params: 7,374,140\n",
      "Non-trainable params: 0\n",
      "--------------------------------------------------------------------------\n",
      "input: torch.Size([1, 24, 4, 64, 64])\n",
      "output: torch.Size([1, 24, 4, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "x=torch.zeros(1,24,4,64,64).cuda() # [B,C,D,H,W] input: 64 x 64 x 4 x 24\n",
    "model=ChannelAttnBlock(conv_hidden=x.shape[1], input_size=x.shape[1], dim=x.shape[2]*x.shape[3]*x.shape[4], is_pos_embed=True).to(device)\n",
    "\n",
    "print(summary(model,x))\n",
    "print('input:',x.shape)\n",
    "print('output:',model(x).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Encoder] Fusion Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionFusion(nn.Module):\n",
    "    def __init__(self,in_depths,hidden_size,is_three,norm_name=\"instance\",depth=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.is_three=is_three\n",
    "\n",
    "        if is_three:\n",
    "            self.out_proj = nn.Linear(hidden_size, int(hidden_size // 3))\n",
    "            self.out_proj2 = nn.Linear(hidden_size, int(hidden_size // 3))\n",
    "            self.out_proj3 = nn.Linear(hidden_size, int(hidden_size // 3))\n",
    "        \n",
    "        else:\n",
    "            self.out_proj = nn.Linear(hidden_size, int(hidden_size // 2))\n",
    "            self.out_proj2 = nn.Linear(hidden_size, int(hidden_size // 2))\n",
    "        \n",
    "        self.conv2d = ResBlock(spatial_dims=2, in_channels=hidden_size*in_depths, out_channels=hidden_size, norm_name=norm_name, depth=depth)\n",
    "\n",
    "    def forward(self,x1,x2,x3=None):\n",
    "        if self.is_three:\n",
    "            x1 = rearrange(x1, \"b c d h w -> b d h w c\")        \n",
    "            x2 = rearrange(x2, \"b c d h w -> b d h w c\")   \n",
    "            x3 = rearrange(x3,\"b c d h w -> b d h w c\")\n",
    "\n",
    "            x1 = self.out_proj(x1)\n",
    "            x2 = self.out_proj2(x2)\n",
    "            x3 = self.out_proj3(x3)\n",
    "\n",
    "            x = torch.cat((x1,x2,x3),dim=-1)            \n",
    "\n",
    "        else:\n",
    "            x1 = rearrange(x1, \"b c d h w -> b d h w c\")        \n",
    "            x2 = rearrange(x2, \"b c d h w -> b d h w c\")   \n",
    "\n",
    "            x1 = self.out_proj(x1)\n",
    "            x2 = self.out_proj2(x2)\n",
    "            \n",
    "            x = torch.cat((x1,x2),dim=-1)\n",
    "        \n",
    "        x = rearrange(x,\"b d h w c -> b c d h w\")\n",
    "        x = rearrange(x,'b c d h w-> b (c d) h w') # [B,CD,H,W]\n",
    "        x = self.conv2d(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------\n",
      "      Layer (type)          Output Shape         Param #     Tr. Param #\n",
      "=========================================================================\n",
      "          Linear-1     [1, 4, 64, 64, 8]             200             200\n",
      "          Linear-2     [1, 4, 64, 64, 8]             200             200\n",
      "          Linear-3     [1, 4, 64, 64, 8]             200             200\n",
      "        ResBlock-4       [1, 24, 64, 64]          28,224          28,224\n",
      "=========================================================================\n",
      "Total params: 28,824\n",
      "Trainable params: 28,824\n",
      "Non-trainable params: 0\n",
      "-------------------------------------------------------------------------\n",
      "input: torch.Size([1, 24, 4, 64, 64]) torch.Size([1, 24, 4, 64, 64]) torch.Size([1, 24, 4, 64, 64])\n",
      "output: torch.Size([1, 24, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "x1=torch.zeros(1,24,4,64,64).cuda() # [B,C,D,H,W] input: 64 x 64 x 4 x 24\n",
    "x2=torch.zeros(1,24,4,64,64).cuda()\n",
    "x3=torch.zeros(1,24,4,64,64).cuda()\n",
    "\n",
    "model=AttentionFusion(in_depths=4,hidden_size=24,is_three=True).to(device)\n",
    "\n",
    "print(summary(model,x1,x2,x3))\n",
    "print('input:',x1.shape, x2.shape, x3.shape)\n",
    "print('output:',model(x1,x2,x3).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------\n",
      "      Layer (type)           Output Shape         Param #     Tr. Param #\n",
      "==========================================================================\n",
      "          Linear-1     [1, 4, 64, 64, 12]             300             300\n",
      "          Linear-2     [1, 4, 64, 64, 12]             300             300\n",
      "        ResBlock-3        [1, 24, 64, 64]          28,224          28,224\n",
      "==========================================================================\n",
      "Total params: 28,824\n",
      "Trainable params: 28,824\n",
      "Non-trainable params: 0\n",
      "--------------------------------------------------------------------------\n",
      "input: torch.Size([1, 24, 4, 64, 64]) torch.Size([1, 24, 4, 64, 64])\n",
      "output: torch.Size([1, 24, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "x1=torch.zeros(1,24,4,64,64).cuda() # [B,C,D,H,W] input: 64 x 64 x 4 x 24\n",
    "x2=torch.zeros(1,24,4,64,64).cuda()\n",
    "\n",
    "model=AttentionFusion(in_depths=4,hidden_size=24,is_three=False).to(device)\n",
    "\n",
    "print(summary(model,x1,x2))\n",
    "print('input:',x1.shape, x2.shape)\n",
    "print('output:',model(x1,x2).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Decoder] Spatial Attention (2D)  \n",
    "-> Q, K, V: HW x C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAttn2D(nn.Module):\n",
    "    def __init__(self, input_size, dim, num_heads=4, qkv_bias=False, attn_drop=0.1, proj_drop=0.1):\n",
    "        '''\n",
    "        input_size: resolution (H*W)\n",
    "        dim: channel (C)\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
    "\n",
    "        # qkv are 3 linear layers (query, key, value)\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop) \n",
    "\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "    \n",
    "    def forward(self, x):        \n",
    "        '''\n",
    "        Spatial Attention\n",
    "        : no projection \n",
    "\n",
    "        x: [B, HW, C] \n",
    "        '''\n",
    "        B, HW, C = x.shape \n",
    "\n",
    "        qkv = self.qkv(x).reshape(B, HW, 3, self.num_heads, C // self.num_heads) # B x HW x 3 x h x C/h\n",
    "        qkv = qkv.permute(2, 0, 3, 1, 4) # 3 x B x h x HW x C/h\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2] # B x h x HW x C/h\n",
    "\n",
    "        q = torch.nn.functional.normalize(q, dim=-2)\n",
    "        k = torch.nn.functional.normalize(k, dim=-2)\n",
    "        k_t = k.permute(0, 1, 3, 2) # K_T : B x h x C/h x HW\n",
    "\n",
    "        attn_SA = (q @ k_t) * self.temperature  # [Q x K_T] B x h x HW x HW\n",
    "        \n",
    "        attn_SA = attn_SA.softmax(dim=-1)\n",
    "        attn_SA = self.attn_drop(attn_SA) # [Spatial Attn Map] B x h x HW x HW\n",
    "        \n",
    "        # [Spatial Attn Map x V] B x h x HW x C/h -> B x HW x h x DC/h -> B x HW x C\n",
    "        x_SA = (attn_SA @ v).permute(0, 2, 1, 3).reshape(B, HW, C) \n",
    "        \n",
    "        # linear projection for msa\n",
    "        x = self.proj(x_SA)\n",
    "        x = self.proj_drop(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAttn2DBlock(nn.Module):\n",
    "     def __init__(self, conv_hidden, input_size, dim, num_heads=4, qkv_bias=False, attn_drop=0.1, proj_drop=0.1,is_pos_embed=False):\n",
    "          '''\n",
    "          input_size: resolution (H*W)\n",
    "          dim: channel (C)\n",
    "          '''\n",
    "          super().__init__()\n",
    "\n",
    "          self.norm = nn.LayerNorm(dim)\n",
    "          self.is_pos_embed = is_pos_embed\n",
    "          self.pos_embed = nn.Parameter(torch.zeros(1, input_size, dim))\n",
    "          self.spatial_attn_2d = SpatialAttn2D(input_size, dim, num_heads, qkv_bias, attn_drop, proj_drop)\n",
    "          self.dsconv = DSConv(in_channels=conv_hidden, out_channels=conv_hidden, spatial_dims=2)\n",
    "\n",
    "     def forward(self,x):\n",
    "          '''\n",
    "          x: [B, C, H, W]\n",
    "          '''\n",
    "          B, C, H, W = x.shape\n",
    "          save = x\n",
    "          \n",
    "          x = rearrange(x,'b c h w-> b (h w) c', b=B, c=C, h=H, w=W) # [B,HW,C]\n",
    "          if self.is_pos_embed:\n",
    "               x = x + self.pos_embed\n",
    "\n",
    "          # spatial attn -> norm\n",
    "          x = self.norm(self.spatial_attn_2d(x))\n",
    "          x = rearrange(x,'b (h w) c-> b c h w', b=B, c=C, h=H, w=W) # [B,C,H,W]\n",
    "          x += save\n",
    "\n",
    "          # conv -> norm\n",
    "          x += self.dsconv(x)\n",
    "\n",
    "          return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "      Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=======================================================================\n",
      "   SpatialAttn2D-1        [1, 256, 96]          36,964          36,964\n",
      "       LayerNorm-2        [1, 256, 96]             192             192\n",
      "          DSConv-3     [1, 96, 16, 16]          10,272          10,272\n",
      "=======================================================================\n",
      "Total params: 47,428\n",
      "Trainable params: 47,428\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n",
      "input: torch.Size([1, 96, 16, 16])\n",
      "output: torch.Size([1, 96, 16, 16])\n"
     ]
    }
   ],
   "source": [
    "x=torch.zeros(1,96,16,16).cuda() # [B,C,H,W] input: 64 x 64 x 24\n",
    "model=SpatialAttn2DBlock(conv_hidden=x.shape[1], input_size=x.shape[2]*x.shape[3], dim=x.shape[1], is_pos_embed=True).to(device)\n",
    "\n",
    "print(summary(model,x))\n",
    "print('input:',x.shape)\n",
    "print('output:',model(x).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dvaa",
   "language": "python",
   "name": "dvaa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
